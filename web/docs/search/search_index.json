{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Rhasspy Voice Assistant Rhasspy (pronounced RAH-SPEE) is an open source , fully offline set of voice assistant services for many human languages that works well with: Hermes protocol compatible services ( Snips.AI ) Home Assistant and Hass.io Node-RED Jeedom OpenHAB You specify voice commands in a template language : [LightState] states = (on | off) turn (<states>){state} [the] light and Rhasspy will produce JSON events that can trigger action in home automation software, such as a Node-RED flow : { \"text\": \"turn on the light\", \"intent\": { \"name\": \"LightState\" }, \"slots\": { \"state\": \"on\" } } Rhasspy is optimized for : Working with external services via MQTT , HTTP , and Websockets Home Assistant and Hass.IO have built-in support Pre-specified voice commands that are described well by a grammar You can also do open-ended speech recognition Voice commands with uncommon words or pronunciations New words are added phonetically with automated assistance Getting Started Ready to try Rhasspy? Follow the steps below and check out the tutorials . Make sure you have the necessary hardware Choose an installation method Access the web interface to download a profile Author your custom voice commands and train Rhasspy Connect Rhasspy to other software like Home Assistant or a Node-RED flow by: Sending and receiving Hermes MQTT messages Using Rhasspy's HTTP API Connecting a Websocket to one of Rhasspy's websocket Getting Help If you have problems, please stop by the Rhasspy community site or open a GitHub issue . Supported Languages Rhasspy supports the following languages: English ( en ) German ( de ) Spanish ( es ) French ( fr ) Italian ( it ) Dutch ( nl ) Russian ( ru ) Greek ( el ) Hindi ( hi ) Mandarin ( zh ) Vietnamese ( vi ) Portuguese ( pt ) Swedish ( sv ) Catalan ( ca ) Services As of version 2.5, Rhasspy is composed of independent services that coordinate over MQTT using a superset of the Hermes protocol . You can easily extend or replace functionality in Rhasspy by using the appropriate messages . Many of these messages can be also sent and received over the HTTP API and the Websocket API . Intended Audience Rhasspy is intended for savvy amateurs or advanced users that want to have a private voice interface to their chosen home automation software. There are many other voice assistants, but none (to my knowledge) that: Can function completely disconnected from the Internet Are entirely free/open source with a permissive license Work well with freely available home automation software If you feel comfortable sending your voice commands through the Internet for someone else to process, or are not comfortable customizing software to handle intents, I recommend taking a look at Mycroft . Contributing Community contributions are welcomed! There are many different ways to contribute: Pull requests for bug fixes, new features, or corrections to the documentation Help with any of the supported language profiles , including: Testing to make sure the acoustic models and default pronunciation dictionaries are working Translations of the sample voice commands Example WAV files of you speaking with text transcriptions for performance testing Contributing to Mozilla Common Voice Assist other Rhasspy community members Suggest or implement new features","title":"Home"},{"location":"#rhasspy-voice-assistant","text":"Rhasspy (pronounced RAH-SPEE) is an open source , fully offline set of voice assistant services for many human languages that works well with: Hermes protocol compatible services ( Snips.AI ) Home Assistant and Hass.io Node-RED Jeedom OpenHAB You specify voice commands in a template language : [LightState] states = (on | off) turn (<states>){state} [the] light and Rhasspy will produce JSON events that can trigger action in home automation software, such as a Node-RED flow : { \"text\": \"turn on the light\", \"intent\": { \"name\": \"LightState\" }, \"slots\": { \"state\": \"on\" } } Rhasspy is optimized for : Working with external services via MQTT , HTTP , and Websockets Home Assistant and Hass.IO have built-in support Pre-specified voice commands that are described well by a grammar You can also do open-ended speech recognition Voice commands with uncommon words or pronunciations New words are added phonetically with automated assistance","title":"Rhasspy Voice Assistant"},{"location":"#getting-started","text":"Ready to try Rhasspy? Follow the steps below and check out the tutorials . Make sure you have the necessary hardware Choose an installation method Access the web interface to download a profile Author your custom voice commands and train Rhasspy Connect Rhasspy to other software like Home Assistant or a Node-RED flow by: Sending and receiving Hermes MQTT messages Using Rhasspy's HTTP API Connecting a Websocket to one of Rhasspy's websocket","title":"Getting Started"},{"location":"#getting-help","text":"If you have problems, please stop by the Rhasspy community site or open a GitHub issue .","title":"Getting Help"},{"location":"#supported-languages","text":"Rhasspy supports the following languages: English ( en ) German ( de ) Spanish ( es ) French ( fr ) Italian ( it ) Dutch ( nl ) Russian ( ru ) Greek ( el ) Hindi ( hi ) Mandarin ( zh ) Vietnamese ( vi ) Portuguese ( pt ) Swedish ( sv ) Catalan ( ca )","title":"Supported Languages"},{"location":"#services","text":"As of version 2.5, Rhasspy is composed of independent services that coordinate over MQTT using a superset of the Hermes protocol . You can easily extend or replace functionality in Rhasspy by using the appropriate messages . Many of these messages can be also sent and received over the HTTP API and the Websocket API .","title":"Services"},{"location":"#intended-audience","text":"Rhasspy is intended for savvy amateurs or advanced users that want to have a private voice interface to their chosen home automation software. There are many other voice assistants, but none (to my knowledge) that: Can function completely disconnected from the Internet Are entirely free/open source with a permissive license Work well with freely available home automation software If you feel comfortable sending your voice commands through the Internet for someone else to process, or are not comfortable customizing software to handle intents, I recommend taking a look at Mycroft .","title":"Intended Audience"},{"location":"#contributing","text":"Community contributions are welcomed! There are many different ways to contribute: Pull requests for bug fixes, new features, or corrections to the documentation Help with any of the supported language profiles , including: Testing to make sure the acoustic models and default pronunciation dictionaries are working Translations of the sample voice commands Example WAV files of you speaking with text transcriptions for performance testing Contributing to Mozilla Common Voice Assist other Rhasspy community members Suggest or implement new features","title":"Contributing"},{"location":"about/","text":"About Rhasspy was created and is currently maintained by Michael Hansen . Supporting Tools The following tools/libraries help to support Rhasspy: eSpeak (text to speech) flite (text to speech) fuzzywuzzy (fuzzy string matching) Kaldi (speech to text) MaryTTS (text to speech) Montreal Forced Aligner (acoustic models) Phonetisaurus (word pronunciations) PicoTTS (text to speech) Pocketsphinx (speech to text, wake word) porcupine (wake word) PyAudio (microphone) Python 3 Quart snowboy (wake word) Sox (WAV conversion) Vue.js (web UI) webrtcvad (voice activity detection) Zamia Speech (acoustic models)","title":"About"},{"location":"about/#about","text":"Rhasspy was created and is currently maintained by Michael Hansen .","title":"About"},{"location":"about/#supporting-tools","text":"The following tools/libraries help to support Rhasspy: eSpeak (text to speech) flite (text to speech) fuzzywuzzy (fuzzy string matching) Kaldi (speech to text) MaryTTS (text to speech) Montreal Forced Aligner (acoustic models) Phonetisaurus (word pronunciations) PicoTTS (text to speech) Pocketsphinx (speech to text, wake word) porcupine (wake word) PyAudio (microphone) Python 3 Quart snowboy (wake word) Sox (WAV conversion) Vue.js (web UI) webrtcvad (voice activity detection) Zamia Speech (acoustic models)","title":"Supporting Tools"},{"location":"audio-input/","text":"Audio Input Rhasspy can listen to audio input from a local microphone or a remote audio stream. Most of the local audio testing has been done with a USB PlayStation Eye camera . MQTT/Hermes Rhasspy receives audio over MQTT using the Hermes protocol : specifically, audio chunks in the WAV format on the topic hermes/audioServer/<siteId>/audioFrame To avoid unnecessary conversion overhead, the WAV audio should be 16-bit 16Khz mono. PyAudio Streams microphone data from a PyAudio device. This is the default audio input system, and should work with both ALSA and PulseAudio . Add to your profile : \"microphone\": { \"system\": \"pyaudio\", \"pyaudio\": { \"device\": \"\", \"frames_per_buffer\": 480 } } Set microphone.pyaudio.device to a PyAudio device number or leave blank for the default device. Streams 30ms chunks of 16-bit, 16 kHz mono audio by default (480 frames). Implemented by rhasspy-microphone-pyaudio-hermes ALSA Starts an arecord process locally and reads audio data from its standard out. Works best with ALSA . Add to your profile : \"microphone\": { \"system\": \"arecord\", \"arecord\": { \"device\": \"\", \"chunk_size\": 960 } } Set microphone.arecord.device to the name of the ALSA device to use ( -D flag to arecord ) or leave blank for the default device. By default, calls arecord -t raw -r 16000 -f S16_LE -c 1 and reads 30ms (960 bytes) of audio data at a time. Implemented by rhasspy-microphone-cli-hermes GStreamer Receives audio chunks via stdout from a GStreamer pipeline. Add to your profile : \"microphone\": { \"system\": \"gstreamer\", \"gstreamer\": { \"pipeline\": \"...\", } } Set microphone.gstreamer.pipeline to your GStreamer pipeline without a sink (this will be added by Rhasspy). By default, the pipeline is: udpsrc port=12333 ! rawaudioparse use-sink-caps=false format=pcm pcm-format=s16le sample-rate=16000 num-channels=1 ! queue ! audioconvert ! audioresample which \"simply\" receives raw 16-bit 16 kHz audio chunks via UDP port 12333. You could stream microphone audio to Rhasspy from another machine by running the following terminal command: gst-launch-1.0 \\ autoaudiosrc ! \\ audioconvert ! \\ audioresample ! \\ audio/x-raw, rate=16000, channels=1, format=S16LE ! \\ udpsink host=RHASSPY_SERVER port=12333 where RHASSPY_SERVER is the hostname of your Rhasspy server (e.g., localhost ). The Rhasspy Docker images contains the \"good\" plugin set for GStreamer, which includes a wide variety of ways to stream/transform audio. TODO: Not Implemented Dummy Disables microphone recording. Add to your profile : \"microphone\": { \"system\": \"dummy\" } See rhasspy.audio_recorder.DummyAudioRecorder for details.","title":"Audio Input"},{"location":"audio-input/#audio-input","text":"Rhasspy can listen to audio input from a local microphone or a remote audio stream. Most of the local audio testing has been done with a USB PlayStation Eye camera .","title":"Audio Input"},{"location":"audio-input/#mqtthermes","text":"Rhasspy receives audio over MQTT using the Hermes protocol : specifically, audio chunks in the WAV format on the topic hermes/audioServer/<siteId>/audioFrame To avoid unnecessary conversion overhead, the WAV audio should be 16-bit 16Khz mono.","title":"MQTT/Hermes"},{"location":"audio-input/#pyaudio","text":"Streams microphone data from a PyAudio device. This is the default audio input system, and should work with both ALSA and PulseAudio . Add to your profile : \"microphone\": { \"system\": \"pyaudio\", \"pyaudio\": { \"device\": \"\", \"frames_per_buffer\": 480 } } Set microphone.pyaudio.device to a PyAudio device number or leave blank for the default device. Streams 30ms chunks of 16-bit, 16 kHz mono audio by default (480 frames). Implemented by rhasspy-microphone-pyaudio-hermes","title":"PyAudio"},{"location":"audio-input/#alsa","text":"Starts an arecord process locally and reads audio data from its standard out. Works best with ALSA . Add to your profile : \"microphone\": { \"system\": \"arecord\", \"arecord\": { \"device\": \"\", \"chunk_size\": 960 } } Set microphone.arecord.device to the name of the ALSA device to use ( -D flag to arecord ) or leave blank for the default device. By default, calls arecord -t raw -r 16000 -f S16_LE -c 1 and reads 30ms (960 bytes) of audio data at a time. Implemented by rhasspy-microphone-cli-hermes","title":"ALSA"},{"location":"audio-input/#gstreamer","text":"Receives audio chunks via stdout from a GStreamer pipeline. Add to your profile : \"microphone\": { \"system\": \"gstreamer\", \"gstreamer\": { \"pipeline\": \"...\", } } Set microphone.gstreamer.pipeline to your GStreamer pipeline without a sink (this will be added by Rhasspy). By default, the pipeline is: udpsrc port=12333 ! rawaudioparse use-sink-caps=false format=pcm pcm-format=s16le sample-rate=16000 num-channels=1 ! queue ! audioconvert ! audioresample which \"simply\" receives raw 16-bit 16 kHz audio chunks via UDP port 12333. You could stream microphone audio to Rhasspy from another machine by running the following terminal command: gst-launch-1.0 \\ autoaudiosrc ! \\ audioconvert ! \\ audioresample ! \\ audio/x-raw, rate=16000, channels=1, format=S16LE ! \\ udpsink host=RHASSPY_SERVER port=12333 where RHASSPY_SERVER is the hostname of your Rhasspy server (e.g., localhost ). The Rhasspy Docker images contains the \"good\" plugin set for GStreamer, which includes a wide variety of ways to stream/transform audio. TODO: Not Implemented","title":"GStreamer"},{"location":"audio-input/#dummy","text":"Disables microphone recording. Add to your profile : \"microphone\": { \"system\": \"dummy\" } See rhasspy.audio_recorder.DummyAudioRecorder for details.","title":"Dummy"},{"location":"audio-output/","text":"Audio Output Rhasspy provides audio feedback when waking up, processing voice commands, pronouncing custom words, and during text to speech . MQTT/Hermes Rhasspy plays WAV audio data sent with the hermes/audioServer/<siteId>/playBytes/<requestId> topic. The requestId part of the topic is simply a unique ID that will be sent back in id field of the hermes/audioServer/playFinished response. ALSA Plays WAV files on the local device by calling the aplay command. Should work with ALSA and PulseAudio. Add to your profile : \"sounds\": { \"system\": \"aplay\", \"aplay\": { \"device\": \"\" } } If provided, sounds.aplay.device is passed to aplay with the -D argument. Leave it blank to use the default device. Implemented by rhasspy-speakers-cli-hermes Remote TODO: Not Implemented Command Calls an external program to play audio. WAV audio data is sent to the program's standard in, and the program should exit once audio is finished playing. Add to your profile : \"sounds\": { \"system\": \"command\", \"command\": { \"play_program\": \"/path/to/play/program\", \"play_arguments\": [], \"list_program\": \"/path/to/list/program\", \"list_arguments\": [] } } The sounds.command.play_program is executed each time a sound is played with arguments from sounds.command.play_arguments . Rhasspy passes WAV audio to the program's standard input. If provided, the sounds.command.list_program will be executed when a rhasspy/audioServer/getDevices message is received. The program should return a listing of available audio output devices in the same format as aplay -L . Implemented by rhasspy-speakers-cli-hermes Dummy Disables audio output. Add to your profile : \"sounds\": { \"system\": \"dummy\" }","title":"Audio Output"},{"location":"audio-output/#audio-output","text":"Rhasspy provides audio feedback when waking up, processing voice commands, pronouncing custom words, and during text to speech .","title":"Audio Output"},{"location":"audio-output/#mqtthermes","text":"Rhasspy plays WAV audio data sent with the hermes/audioServer/<siteId>/playBytes/<requestId> topic. The requestId part of the topic is simply a unique ID that will be sent back in id field of the hermes/audioServer/playFinished response.","title":"MQTT/Hermes"},{"location":"audio-output/#alsa","text":"Plays WAV files on the local device by calling the aplay command. Should work with ALSA and PulseAudio. Add to your profile : \"sounds\": { \"system\": \"aplay\", \"aplay\": { \"device\": \"\" } } If provided, sounds.aplay.device is passed to aplay with the -D argument. Leave it blank to use the default device. Implemented by rhasspy-speakers-cli-hermes","title":"ALSA"},{"location":"audio-output/#remote","text":"TODO: Not Implemented","title":"Remote"},{"location":"audio-output/#command","text":"Calls an external program to play audio. WAV audio data is sent to the program's standard in, and the program should exit once audio is finished playing. Add to your profile : \"sounds\": { \"system\": \"command\", \"command\": { \"play_program\": \"/path/to/play/program\", \"play_arguments\": [], \"list_program\": \"/path/to/list/program\", \"list_arguments\": [] } } The sounds.command.play_program is executed each time a sound is played with arguments from sounds.command.play_arguments . Rhasspy passes WAV audio to the program's standard input. If provided, the sounds.command.list_program will be executed when a rhasspy/audioServer/getDevices message is received. The program should return a listing of available audio output devices in the same format as aplay -L . Implemented by rhasspy-speakers-cli-hermes","title":"Command"},{"location":"audio-output/#dummy","text":"Disables audio output. Add to your profile : \"sounds\": { \"system\": \"dummy\" }","title":"Dummy"},{"location":"command-listener/","text":"Command Listener As of Rhasspy 2.5, the speech to text system is responsible for detecting the boundaries of a voice command (the default systems use the rhasspy-silence library). Previously, this was done by a seperate \"command listener\" system. This page and its sections are here to avoid broken links and provide an explanation. MQTT/Hermes Rhasspy listens for messages according to the Hermes protocol to decide when to record voice commands. See the speech to text page for more details. WebRTCVAD The rhasspy-silence library used by Rhasspy's pocketsphinx and kaldi uses webrtcvad to detect speech and silence. TODO: Silence detection parameters Add to your profile : \"command\": { \"system\": \"webrtcvad\", \"webrtcvad\": { \"chunk_size\": 960, \"min_sec\": 2, \"sample_rate\": 16000, \"silence_sec\": 0.5, \"speech_buffers\": 5, \"throwaway_buffers\": 10, \"timeout_sec\": 30, \"vad_mode\": 0 } } This system listens for up to timeout_sec for a voice command. The first few frames of audio data are discarded ( throwaway_buffers ) to avoid clicks from the microphone being engaged. When speech is detected for some number of successive frames ( speech_buffers ), the voice command is considered to have started . After min_sec , Rhasspy will start listening for silence. If at least silence_sec goes by without any speech detected, the command is considered finished , and the recorded WAV data is sent to the speech recognition system . You may want to adjust min_sec , silence_sec , and vad_mode for your environment. These control how short a voice command can be ( min_sec ), how much silence is required before Rhasspy stops listening ( silence_sec ), and how aggressive the voice activity filter vad_mode is: this is an integer between 0 and 3. 0 is the least aggressive about filtering out non-speech, 3 is the most aggressive. OneShot Deprecated as of Rhasspy 2.5. This system previously listened for a single WAV audio chunk and processed it as a complete voice command. You can acheive the same thing now with the following steps: Send a hermes/asr/startListening message with the stopOnSilence property set to true Send one or more hermes/audioServer/<siteId>/audioFrame messages with your voice command WAV audio Send a hermes/asr/stopListening message With stopOnSilence set in startListening , the configured speech to text system should not attempt a transcription until the stopListening message is received. Command Deprecated as of Rhasspy 2.5. This system previously allowed for an external program to determine voice command boundaries. Dummy Deprecated as of Rhasspy 2.5. This system previously disabled voice command recording.","title":"Command Listener"},{"location":"command-listener/#command-listener","text":"As of Rhasspy 2.5, the speech to text system is responsible for detecting the boundaries of a voice command (the default systems use the rhasspy-silence library). Previously, this was done by a seperate \"command listener\" system. This page and its sections are here to avoid broken links and provide an explanation.","title":"Command Listener"},{"location":"command-listener/#mqtthermes","text":"Rhasspy listens for messages according to the Hermes protocol to decide when to record voice commands. See the speech to text page for more details.","title":"MQTT/Hermes"},{"location":"command-listener/#webrtcvad","text":"The rhasspy-silence library used by Rhasspy's pocketsphinx and kaldi uses webrtcvad to detect speech and silence. TODO: Silence detection parameters Add to your profile : \"command\": { \"system\": \"webrtcvad\", \"webrtcvad\": { \"chunk_size\": 960, \"min_sec\": 2, \"sample_rate\": 16000, \"silence_sec\": 0.5, \"speech_buffers\": 5, \"throwaway_buffers\": 10, \"timeout_sec\": 30, \"vad_mode\": 0 } } This system listens for up to timeout_sec for a voice command. The first few frames of audio data are discarded ( throwaway_buffers ) to avoid clicks from the microphone being engaged. When speech is detected for some number of successive frames ( speech_buffers ), the voice command is considered to have started . After min_sec , Rhasspy will start listening for silence. If at least silence_sec goes by without any speech detected, the command is considered finished , and the recorded WAV data is sent to the speech recognition system . You may want to adjust min_sec , silence_sec , and vad_mode for your environment. These control how short a voice command can be ( min_sec ), how much silence is required before Rhasspy stops listening ( silence_sec ), and how aggressive the voice activity filter vad_mode is: this is an integer between 0 and 3. 0 is the least aggressive about filtering out non-speech, 3 is the most aggressive.","title":"WebRTCVAD"},{"location":"command-listener/#oneshot","text":"Deprecated as of Rhasspy 2.5. This system previously listened for a single WAV audio chunk and processed it as a complete voice command. You can acheive the same thing now with the following steps: Send a hermes/asr/startListening message with the stopOnSilence property set to true Send one or more hermes/audioServer/<siteId>/audioFrame messages with your voice command WAV audio Send a hermes/asr/stopListening message With stopOnSilence set in startListening , the configured speech to text system should not attempt a transcription until the stopListening message is received.","title":"OneShot"},{"location":"command-listener/#command","text":"Deprecated as of Rhasspy 2.5. This system previously allowed for an external program to determine voice command boundaries.","title":"Command"},{"location":"command-listener/#dummy","text":"Deprecated as of Rhasspy 2.5. This system previously disabled voice command recording.","title":"Dummy"},{"location":"development/","text":"Development Rhasspy's code can be found on GitHub . Set up your development environment TODO: Rewrite Run the unit tests TODO: Rewrite Keeping your fork synchronized TODO: Rewrite Development practices TODO: Update links Before starting significant work, please propose it and discuss it first on the issue tracker on GitHub. Other people may have suggestions, will want to collaborate and will wish to review your code. Please work on one piece of conceptual work at a time. Keep each narrative of work in a different branch. As much as possible, have each commit solve one problem. A commit must not leave the project in a non-functional state. Run the unit tests before you create a commit. Treat code, tests and documentation as one. Create a pull request from your fork. Development workflow If you want to start working on a specific feature or bug fix, this is an example workflow: Synchronize your fork with the upstream repository. Create a new branch: git checkout -b <nameofbranch> Create your changes. Add the changed files with git add <files> . Commit your changes with git commit . Push your changes to your fork on GitHub. Create a pull request from your fork. License of contributions By submitting patches to this project, you agree to allow them to be redistributed under the project\u2019s license according to the normal forms and usages of the open source community. It is your responsibility to make sure you have all the necessary rights to contribute to the project.","title":"Development"},{"location":"development/#development","text":"Rhasspy's code can be found on GitHub .","title":"Development"},{"location":"development/#set-up-your-development-environment","text":"TODO: Rewrite","title":"Set up your development environment"},{"location":"development/#run-the-unit-tests","text":"TODO: Rewrite","title":"Run the unit tests"},{"location":"development/#keeping-your-fork-synchronized","text":"TODO: Rewrite","title":"Keeping your fork synchronized"},{"location":"development/#development-practices","text":"TODO: Update links Before starting significant work, please propose it and discuss it first on the issue tracker on GitHub. Other people may have suggestions, will want to collaborate and will wish to review your code. Please work on one piece of conceptual work at a time. Keep each narrative of work in a different branch. As much as possible, have each commit solve one problem. A commit must not leave the project in a non-functional state. Run the unit tests before you create a commit. Treat code, tests and documentation as one. Create a pull request from your fork.","title":"Development practices"},{"location":"development/#development-workflow","text":"If you want to start working on a specific feature or bug fix, this is an example workflow: Synchronize your fork with the upstream repository. Create a new branch: git checkout -b <nameofbranch> Create your changes. Add the changed files with git add <files> . Commit your changes with git commit . Push your changes to your fork on GitHub. Create a pull request from your fork.","title":"Development workflow"},{"location":"development/#license-of-contributions","text":"By submitting patches to this project, you agree to allow them to be redistributed under the project\u2019s license according to the normal forms and usages of the open source community. It is your responsibility to make sure you have all the necessary rights to contribute to the project.","title":"License of contributions"},{"location":"hardware/","text":"Hardware Rhasspy is designed to be run on different kinds of hardware, such as: Raspberry Pi 2-3 B/B+ ( armhf / aarch64 ) Desktop/laptop/server ( amd64 ) Raspberry Pi Zero ( armv6l ) The table below summarizes architecture compatibility with Rhasspy's components: Category Name amd64 armhf aarch64 Wake Word pocketsphinx \u2713 \u2713 \u2713 snowboy \u2713 \u2713 precise \u2713 \u2713 porcupine \u2713 \u2713 \u2713 Speech to Text pocketsphinx \u2713 \u2713 \u2713 kaldi \u2713 \u2713 \u2713 Intent Recognition fsticuffs \u2713 \u2713 \u2713 fuzzywuzzy \u2713 \u2713 \u2713 adapt \u2713 \u2713 \u2713 flair \u2713 rasaNLU \u2713 \u2713 \u2713 Text to Speech espeak \u2713 \u2713 \u2713 flite \u2713 \u2713 \u2713 picotts \u2713 \u2713 \u2713 marytts \u2713 \u2713 \u2713 wavenet \u2713 \u2713 \u2713 Raspberry Pi To run Rhasspy on a Raspberry Pi, you'll need at least a 4 GB SD card and a good power supply. I highly recommend the CanaKit Starter Kit , which includes a 32 GB SD card, a 2.5 A power supply, and a case. Some components of Rhasspy will not work on the Raspberry Pi 3 B+ model with a 64-bit operating system ( aarch64 ). As of the time of this writing, these are: snowboy (wake word) Mycroft Precise (wake word) Microphone Rhasspy can listen to audio input from a local microphone or from a remote audio stream . Most of the local audio testing has been done with the following microphones: PlayStation Eye camera ReSpeaker 4 Mic Array ReSpeaker 2 Mics pHAT Remote audio testing has main used the MATRIX Voice and Romkabouter's excellent MQTT Audio Streamer . For Raspberry Pi's, check out the hermes-audio-server by koenvervloesem . Some users have also reported success with the snips-audio-server package from Snips.AI . In both cases, you will need to configure Rhasspy to listen for audio data over MQTT . You may also be interested in reading this microphone benchmarking post that the Snips.AI folks did back in 2017.","title":"Hardware"},{"location":"hardware/#hardware","text":"Rhasspy is designed to be run on different kinds of hardware, such as: Raspberry Pi 2-3 B/B+ ( armhf / aarch64 ) Desktop/laptop/server ( amd64 ) Raspberry Pi Zero ( armv6l ) The table below summarizes architecture compatibility with Rhasspy's components: Category Name amd64 armhf aarch64 Wake Word pocketsphinx \u2713 \u2713 \u2713 snowboy \u2713 \u2713 precise \u2713 \u2713 porcupine \u2713 \u2713 \u2713 Speech to Text pocketsphinx \u2713 \u2713 \u2713 kaldi \u2713 \u2713 \u2713 Intent Recognition fsticuffs \u2713 \u2713 \u2713 fuzzywuzzy \u2713 \u2713 \u2713 adapt \u2713 \u2713 \u2713 flair \u2713 rasaNLU \u2713 \u2713 \u2713 Text to Speech espeak \u2713 \u2713 \u2713 flite \u2713 \u2713 \u2713 picotts \u2713 \u2713 \u2713 marytts \u2713 \u2713 \u2713 wavenet \u2713 \u2713 \u2713","title":"Hardware"},{"location":"hardware/#raspberry-pi","text":"To run Rhasspy on a Raspberry Pi, you'll need at least a 4 GB SD card and a good power supply. I highly recommend the CanaKit Starter Kit , which includes a 32 GB SD card, a 2.5 A power supply, and a case. Some components of Rhasspy will not work on the Raspberry Pi 3 B+ model with a 64-bit operating system ( aarch64 ). As of the time of this writing, these are: snowboy (wake word) Mycroft Precise (wake word)","title":"Raspberry Pi"},{"location":"hardware/#microphone","text":"Rhasspy can listen to audio input from a local microphone or from a remote audio stream . Most of the local audio testing has been done with the following microphones: PlayStation Eye camera ReSpeaker 4 Mic Array ReSpeaker 2 Mics pHAT Remote audio testing has main used the MATRIX Voice and Romkabouter's excellent MQTT Audio Streamer . For Raspberry Pi's, check out the hermes-audio-server by koenvervloesem . Some users have also reported success with the snips-audio-server package from Snips.AI . In both cases, you will need to configure Rhasspy to listen for audio data over MQTT . You may also be interested in reading this microphone benchmarking post that the Snips.AI folks did back in 2017.","title":"Microphone"},{"location":"installation/","text":"Install Docker Debian Virtual Environment From Source","title":"Installation"},{"location":"installation/#install","text":"","title":"Install"},{"location":"installation/#docker","text":"","title":"Docker"},{"location":"installation/#debian","text":"","title":"Debian"},{"location":"installation/#virtual-environment","text":"","title":"Virtual Environment"},{"location":"installation/#from-source","text":"","title":"From Source"},{"location":"intent-handling/","text":"Intent Handling After a voice command has been transcribed and your intent has been successfully recognized, Rhasspy is ready to send a JSON event to another system like Home Assistant or a remote Rhasspy server. You can also handle intents by: Listening for hermes/intents/<intentName> messages over MQTT ( details ) Connecting a websocket to /api/events/intent ( details ) Available intent handling systems are: Home Assistant Remote HTTP Server External Command Home Assistant Add to your profile : \"handle\": { \"system\": \"hass\" }, \"home_assistant\": { \"access_token\": \"\", \"api_password\": \"\", \"event_type_format\": \"rhasspy_{0}\", \"url\": \"http://hassio/homeassistant/\" } If you're running Rhasspy as an add-on inside Hass.io , the access token is automatically provided . Otherwise, you'll need to create a long-lived access token and set home_assistant.access_token manually. Implemented by rhasspy-homeassistant-hermes Events Rhasspy will send Home Assistant an event every time an intent is recognized through its REST API . The type of the event is determined by the name of the intent, and the event data comes from the tagged words in your sentences . For example, if you have an intent like: [ChangeLightColor] set the (bedroom light){name} to (red | green | blue){color} and you say something like \"set the bedroom light to blue\" , Rhasspy will POST to the /api/events/rhasspy_ChangeLightColor endpoint of your Home Assistant server with the following data: { \"name\": \"bedroom light\", \"color\": \"blue\" } In order to do something with the rhasspy_ChangeLightColor event, create an automation with an event trigger . For example, add the following to your automation.yaml file: - alias: \"Set bedroom light color (blue)\" trigger: platform: event event_type: rhasspy_ChangeLightColor event_data: name: 'bedroom light' color: 'blue' action: ... See the documentation on actions for the different things you can do with Home Assistant. MQTT Rhasspy automatically publishes intents over MQTT ( Hermes protocol ). This allows Rhasspy to send interoperate with Snips.AI compatible systems. Home Assistant can listen directly to these intents using the snips plugin. To use it, add to your Home Assistant's configuration.yaml file: snips: intent_script: ... See the intent script documentation for details on how to handle the intents. Self-Signed Certificate If your Home Assistant uses a self-signed certificate, you'll need to give Rhasspy some extra information. Add to your profile : \"home_assistant\": { ... \"pem_file\": \"/path/to/certfile\" } Set home_assistant.pem_file to the full path to your CA_BUNDLE file or a directory with certificates of trusted CAs . Use the environment variable RHASSPY_PROFILE_DIR to reference your current profile's directory. For example, $RHASSPY_PROFILE_DIR/my.pem will tell Rhasspy to use a file named my.pem in your profile directory when verifying your self-signed certificate. Remote Server Rhasspy can POST the intent JSON to a remote URL. Add to your profile : \"handle\": { \"system\": \"remote\", \"remote\": { \"url\": \"http://<address>:<port>/path/to/endpoint\" } } When an intent is recognized, Rhasspy will POST to handle.remote.url with the intent JSON. Your server should return JSON back, optionally with additional information (see below). Speech If the returned JSON contains a \"speech\" key like this: { ... \"speech\": { \"text\": \"Some text to speak.\" } } then Rhasspy will forward speech.text to the configured text to speech system using a hermes/tts/say message. TODO: Move to separate service Command Once an intent is successfully recognized, Rhasspy will send an event to Home Assistant with the details. You can call a custom program instead or in addition to this behavior. Add to your profile : \"handle\": { \"system\": \"command\", \"command\": { \"program\": \"/path/to/program\", \"arguments\": [] } } When an intent is recognized, Rhasspy will call your custom program with the intent JSON printed to standard in. You should return JSON to standard out, optionally with additional information (see below). The following environment variables are available to your program: $RHASSPY_BASE_DIR - path to the directory where Rhasspy is running from $RHASSPY_PROFILE - name of the current profile (e.g., \"en\") $RHASSPY_PROFILE_DIR - directory of the current profile (where profile.json is) See handle.sh or handle.py for example programs. Speech If the returned JSON contains a \"speech\" key like this: { ... \"speech\": { \"text\": \"Some text to speak.\" } } then Rhasspy will forward speech.text to the configured text to speech system using a hermes/tts/say message. TODO: Not implemented Dummy Disables intent handling. Add to your profile : \"handle\": { \"system\": \"dummy\" }","title":"Intent Handling"},{"location":"intent-handling/#intent-handling","text":"After a voice command has been transcribed and your intent has been successfully recognized, Rhasspy is ready to send a JSON event to another system like Home Assistant or a remote Rhasspy server. You can also handle intents by: Listening for hermes/intents/<intentName> messages over MQTT ( details ) Connecting a websocket to /api/events/intent ( details ) Available intent handling systems are: Home Assistant Remote HTTP Server External Command","title":"Intent Handling"},{"location":"intent-handling/#home-assistant","text":"Add to your profile : \"handle\": { \"system\": \"hass\" }, \"home_assistant\": { \"access_token\": \"\", \"api_password\": \"\", \"event_type_format\": \"rhasspy_{0}\", \"url\": \"http://hassio/homeassistant/\" } If you're running Rhasspy as an add-on inside Hass.io , the access token is automatically provided . Otherwise, you'll need to create a long-lived access token and set home_assistant.access_token manually. Implemented by rhasspy-homeassistant-hermes","title":"Home Assistant"},{"location":"intent-handling/#events","text":"Rhasspy will send Home Assistant an event every time an intent is recognized through its REST API . The type of the event is determined by the name of the intent, and the event data comes from the tagged words in your sentences . For example, if you have an intent like: [ChangeLightColor] set the (bedroom light){name} to (red | green | blue){color} and you say something like \"set the bedroom light to blue\" , Rhasspy will POST to the /api/events/rhasspy_ChangeLightColor endpoint of your Home Assistant server with the following data: { \"name\": \"bedroom light\", \"color\": \"blue\" } In order to do something with the rhasspy_ChangeLightColor event, create an automation with an event trigger . For example, add the following to your automation.yaml file: - alias: \"Set bedroom light color (blue)\" trigger: platform: event event_type: rhasspy_ChangeLightColor event_data: name: 'bedroom light' color: 'blue' action: ... See the documentation on actions for the different things you can do with Home Assistant.","title":"Events"},{"location":"intent-handling/#mqtt","text":"Rhasspy automatically publishes intents over MQTT ( Hermes protocol ). This allows Rhasspy to send interoperate with Snips.AI compatible systems. Home Assistant can listen directly to these intents using the snips plugin. To use it, add to your Home Assistant's configuration.yaml file: snips: intent_script: ... See the intent script documentation for details on how to handle the intents.","title":"MQTT"},{"location":"intent-handling/#self-signed-certificate","text":"If your Home Assistant uses a self-signed certificate, you'll need to give Rhasspy some extra information. Add to your profile : \"home_assistant\": { ... \"pem_file\": \"/path/to/certfile\" } Set home_assistant.pem_file to the full path to your CA_BUNDLE file or a directory with certificates of trusted CAs . Use the environment variable RHASSPY_PROFILE_DIR to reference your current profile's directory. For example, $RHASSPY_PROFILE_DIR/my.pem will tell Rhasspy to use a file named my.pem in your profile directory when verifying your self-signed certificate.","title":"Self-Signed Certificate"},{"location":"intent-handling/#remote-server","text":"Rhasspy can POST the intent JSON to a remote URL. Add to your profile : \"handle\": { \"system\": \"remote\", \"remote\": { \"url\": \"http://<address>:<port>/path/to/endpoint\" } } When an intent is recognized, Rhasspy will POST to handle.remote.url with the intent JSON. Your server should return JSON back, optionally with additional information (see below).","title":"Remote Server"},{"location":"intent-handling/#speech","text":"If the returned JSON contains a \"speech\" key like this: { ... \"speech\": { \"text\": \"Some text to speak.\" } } then Rhasspy will forward speech.text to the configured text to speech system using a hermes/tts/say message. TODO: Move to separate service","title":"Speech"},{"location":"intent-handling/#command","text":"Once an intent is successfully recognized, Rhasspy will send an event to Home Assistant with the details. You can call a custom program instead or in addition to this behavior. Add to your profile : \"handle\": { \"system\": \"command\", \"command\": { \"program\": \"/path/to/program\", \"arguments\": [] } } When an intent is recognized, Rhasspy will call your custom program with the intent JSON printed to standard in. You should return JSON to standard out, optionally with additional information (see below). The following environment variables are available to your program: $RHASSPY_BASE_DIR - path to the directory where Rhasspy is running from $RHASSPY_PROFILE - name of the current profile (e.g., \"en\") $RHASSPY_PROFILE_DIR - directory of the current profile (where profile.json is) See handle.sh or handle.py for example programs.","title":"Command"},{"location":"intent-handling/#speech_1","text":"If the returned JSON contains a \"speech\" key like this: { ... \"speech\": { \"text\": \"Some text to speak.\" } } then Rhasspy will forward speech.text to the configured text to speech system using a hermes/tts/say message. TODO: Not implemented","title":"Speech"},{"location":"intent-handling/#dummy","text":"Disables intent handling. Add to your profile : \"handle\": { \"system\": \"dummy\" }","title":"Dummy"},{"location":"intent-recognition/","text":"Intent Recognition After your voice command has been transcribed by the speech to text system, the next step is to recognize your intent. The end result is a JSON event with information about the intent. Available intent recognition systems are: Fsticuffs Fuzzywuzzy Mycroft Adapt RasaNLU Flair Remote HTTP Server External Command The following table summarizes the trade-offs of using each intent recognizer: System Ideal Sentence count Training Speed Recognition Speed Flexibility fsticuffs 1M+ very fast very fast ignores unknown words fuzzywuzzy 12-100 fast fast fuzzy string matching adapt 100-1K moderate fast ignores unknown words rasaNLU 1K-100K very slow moderate handles unseen words flair 1K-100K very slow moderate handles unseen words MQTT/Hermes Rhasspy receives intent recognition requests on the hermes/nlu/query topic. Successful recognitions are published to hermes/intent/<intentName> , and unsuccessful recognitions to hermes/nlu/intentNotRecognized The format of these messages adheres to the Hermes protocol . Fsticuffs Uses the rhasspy-nlu library to recognize only those sentences that were Rhasspy was trained on . While less flexible than the other intent recognizers, fsticuffs can be trained and perform recognition over millions of sentences in milliseconds. If you only plan to recognize voice commands from your training set (and not unseen ones via text chat), fsticuffs is the best choice. Add to your profile : \"intent\": { \"system\": \"fsticuffs\", \"fsticuffs\": { \"intent_graph\": \"intent.json\", \"ignore_unknown_words\": true, \"fuzzy\": true } } By default, fuzzy mathing is enabled ( fuzzy is true). This allows fsticuffs to be less strict when matching text, skipping over any words in the profile's stop_words.txt , and handling repeated words gracefully. Words must still appear in the correct order according to sentences.ini , but additional words will not cause a recognition failure. When ignore_unknown_words is true, any word outside of sentences.ini is silently ignored. This allows a lot more sentences to be accepted, but may cause unexpected results when used with arbitrary input from text chat. Implemented by rhasspy-nlu-hermes Fuzzywuzzy Finds the closest matching intent by using the Levenshtein distance between the text and the all of the training sentences you provided. Works best when you have a small number of sentences (dozens to hundreds) and need some resiliency to spelling errors (i.e., from text chat). Add to your profile : \"intent\": { \"system\": \"fuzzywuzzy\", \"fuzzywuzzy\": { \"examples_json\": \"intent_examples.json\" } } Implemented by rhasspy-fuzzywuzzy-hermes Mycroft Adapt Recognizes intents using Mycroft Adapt . Works best when you have a medium number of sentences (hundreds to thousands) and need to be able to recognize sentences not seen during training (no new words, though). Add to your profile : \"intent\": { \"system\": \"adapt\", \"adapt\": { \"stop_words\": \"stop_words.txt\" } } The intent.adapt.stop_words text file contains words that should be ignored (i.e., cannot be \"required\" or \"optional\"). TODO: Not implemented Flair Recognizes intents using the flair NLP framework . Works best when you have a large number of sentences (thousands to hundreds of thousands) and need to handle sentences and words not seen during training. Add to your profile : \"intent\": { \"system\": \"flair\", \"flair\": { \"data_dir\": \"flair_data\", \"max_epochs\": 25, \"do_sampling\": true, \"num_samples\": 10000 } } By default, the flair recognizer will generate 10,000 random sentences ( num_samples ) from each intent in your sentences.ini file. If you set do_sampling to false , Rhasspy will generate all possible sentences and use them as training data. This will produce the most accurate models, but may take a long time depending on the complexity of your grammars. A flair TextClassifier will be trained to classify unseen sentences by intent, and a SequenceTagger will be trained for each intent that has at least one tag . During recognition, sentences are first classified by intent and then run through the appropriate SequenceTagger model to determine slots/entities. TODO: Not implemented RasaNLU Recognizes intents remotely using a Rasa NLU server. You must install a Rasa NLU server somewhere that Rhasspy can access. Works well when you have a large number of sentences (thousands to hundreds of thousands) and need to handle sentences and words not seen during training. This needs Rasa 1.0 or higher. Add to your profile : \"intent\": { \"system\": \"rasa\", \"rasa\": { \"examples_markdown\": \"intent_examples.md\", \"project_name\": \"rhasspy\", \"url\": \"http://localhost:5005/\" } } TODO: Not implemented Remote HTTP Server Uses a remote Rhasppy server to do intent recognition. POSTs the text to an HTTP endpoint and receives an intent as JSON. An empty intent.name property of the returned JSON object indicates a recognition failure. Add to your profile : \"intent\": { \"system\": \"remote\", \"remote\": { \"url\": \"http://my-server:12101/api/text-to-intent\" } } If you want to also POST to an endpoint during training, add to your profile: \"training\": { \"system\": \"auto\", \"intent\": { \"remote\": { \"url\": \"http://my-server/intent-training-endpoint\" } } } If training.intent.remote.url is set, Rhasspy will POST the intent graph generated by rhasspy-nlu to your endpoint as JSON. No response is expected, though an HTTP error code indicates that training has failed. Implemented by rhasspy-remote-http-hermes Home Assistant Conversation Sends transcriptions from speech to text to Home Assistant's conversation API . If the response contains speech, Rhasspy can optionally speak it. Add to your profile : \"intent\": { \"system\": \"conversation\", \"conversation\": { \"handle_speech\": true } } When handle_speech is true , Rhasspy will forward the returned speech to your text to speech system. The settings from your profile's home_assistant section are automatically used (URL, access token, etc.). Because Home Assistant will already handle your intent (probably using an intent script ), Rhasspy will always generate an empty intent with this recognizer. TODO: Not implemented Command Recognizes intents from text using a custom external program. Your program should return a JSON object that describes the recognized intent; something like: { \"intent\": { \"name\": \"ChangeLightColor\", \"confidence\": 1.0 }, \"entities\": [ { \"entity\": \"name\", \"value\": \"bedroom light\" }, { \"entity\": \"color\", \"value\": \"red\" } ], \"text\": \"set the bedroom light to red\" } An empty intent.name property indicates a recognition failure. Add to your profile : \"intent\": { \"system\": \"command\", \"command\": { \"program\": \"/path/to/program\", \"arguments\": [] } } If you want to also call an external program during training, add to your profile: \"training\": { \"system\": \"auto\", \"intent\": { \"command\": { \"program\": \"/path/to/training/program\", \"arguments\": [] } } } If training.intent.command.program is set, Rhasspy will call your program with the intent graph generated by rhasspy-nlu provided as JSON on standard input. No response is expected, though a non-zero exit code indicates a training failure. The following environment variables are available to your program: $RHASSPY_BASE_DIR - path to the directory where Rhasspy is running from $RHASSPY_PROFILE - name of the current profile (e.g., \"en\") $RHASSPY_PROFILE_DIR - directory of the current profile (where profile.json is) See text2intent.sh for an example program. Implemented by rhasspy-remote-http-hermes Dummy Disables intent recognition. Add to your profile : \"intent\": { \"system\": \"dummy\" }","title":"Intent Recognition"},{"location":"intent-recognition/#intent-recognition","text":"After your voice command has been transcribed by the speech to text system, the next step is to recognize your intent. The end result is a JSON event with information about the intent. Available intent recognition systems are: Fsticuffs Fuzzywuzzy Mycroft Adapt RasaNLU Flair Remote HTTP Server External Command The following table summarizes the trade-offs of using each intent recognizer: System Ideal Sentence count Training Speed Recognition Speed Flexibility fsticuffs 1M+ very fast very fast ignores unknown words fuzzywuzzy 12-100 fast fast fuzzy string matching adapt 100-1K moderate fast ignores unknown words rasaNLU 1K-100K very slow moderate handles unseen words flair 1K-100K very slow moderate handles unseen words","title":"Intent Recognition"},{"location":"intent-recognition/#mqtthermes","text":"Rhasspy receives intent recognition requests on the hermes/nlu/query topic. Successful recognitions are published to hermes/intent/<intentName> , and unsuccessful recognitions to hermes/nlu/intentNotRecognized The format of these messages adheres to the Hermes protocol .","title":"MQTT/Hermes"},{"location":"intent-recognition/#fsticuffs","text":"Uses the rhasspy-nlu library to recognize only those sentences that were Rhasspy was trained on . While less flexible than the other intent recognizers, fsticuffs can be trained and perform recognition over millions of sentences in milliseconds. If you only plan to recognize voice commands from your training set (and not unseen ones via text chat), fsticuffs is the best choice. Add to your profile : \"intent\": { \"system\": \"fsticuffs\", \"fsticuffs\": { \"intent_graph\": \"intent.json\", \"ignore_unknown_words\": true, \"fuzzy\": true } } By default, fuzzy mathing is enabled ( fuzzy is true). This allows fsticuffs to be less strict when matching text, skipping over any words in the profile's stop_words.txt , and handling repeated words gracefully. Words must still appear in the correct order according to sentences.ini , but additional words will not cause a recognition failure. When ignore_unknown_words is true, any word outside of sentences.ini is silently ignored. This allows a lot more sentences to be accepted, but may cause unexpected results when used with arbitrary input from text chat. Implemented by rhasspy-nlu-hermes","title":"Fsticuffs"},{"location":"intent-recognition/#fuzzywuzzy","text":"Finds the closest matching intent by using the Levenshtein distance between the text and the all of the training sentences you provided. Works best when you have a small number of sentences (dozens to hundreds) and need some resiliency to spelling errors (i.e., from text chat). Add to your profile : \"intent\": { \"system\": \"fuzzywuzzy\", \"fuzzywuzzy\": { \"examples_json\": \"intent_examples.json\" } } Implemented by rhasspy-fuzzywuzzy-hermes","title":"Fuzzywuzzy"},{"location":"intent-recognition/#mycroft-adapt","text":"Recognizes intents using Mycroft Adapt . Works best when you have a medium number of sentences (hundreds to thousands) and need to be able to recognize sentences not seen during training (no new words, though). Add to your profile : \"intent\": { \"system\": \"adapt\", \"adapt\": { \"stop_words\": \"stop_words.txt\" } } The intent.adapt.stop_words text file contains words that should be ignored (i.e., cannot be \"required\" or \"optional\"). TODO: Not implemented","title":"Mycroft Adapt"},{"location":"intent-recognition/#flair","text":"Recognizes intents using the flair NLP framework . Works best when you have a large number of sentences (thousands to hundreds of thousands) and need to handle sentences and words not seen during training. Add to your profile : \"intent\": { \"system\": \"flair\", \"flair\": { \"data_dir\": \"flair_data\", \"max_epochs\": 25, \"do_sampling\": true, \"num_samples\": 10000 } } By default, the flair recognizer will generate 10,000 random sentences ( num_samples ) from each intent in your sentences.ini file. If you set do_sampling to false , Rhasspy will generate all possible sentences and use them as training data. This will produce the most accurate models, but may take a long time depending on the complexity of your grammars. A flair TextClassifier will be trained to classify unseen sentences by intent, and a SequenceTagger will be trained for each intent that has at least one tag . During recognition, sentences are first classified by intent and then run through the appropriate SequenceTagger model to determine slots/entities. TODO: Not implemented","title":"Flair"},{"location":"intent-recognition/#rasanlu","text":"Recognizes intents remotely using a Rasa NLU server. You must install a Rasa NLU server somewhere that Rhasspy can access. Works well when you have a large number of sentences (thousands to hundreds of thousands) and need to handle sentences and words not seen during training. This needs Rasa 1.0 or higher. Add to your profile : \"intent\": { \"system\": \"rasa\", \"rasa\": { \"examples_markdown\": \"intent_examples.md\", \"project_name\": \"rhasspy\", \"url\": \"http://localhost:5005/\" } } TODO: Not implemented","title":"RasaNLU"},{"location":"intent-recognition/#remote-http-server","text":"Uses a remote Rhasppy server to do intent recognition. POSTs the text to an HTTP endpoint and receives an intent as JSON. An empty intent.name property of the returned JSON object indicates a recognition failure. Add to your profile : \"intent\": { \"system\": \"remote\", \"remote\": { \"url\": \"http://my-server:12101/api/text-to-intent\" } } If you want to also POST to an endpoint during training, add to your profile: \"training\": { \"system\": \"auto\", \"intent\": { \"remote\": { \"url\": \"http://my-server/intent-training-endpoint\" } } } If training.intent.remote.url is set, Rhasspy will POST the intent graph generated by rhasspy-nlu to your endpoint as JSON. No response is expected, though an HTTP error code indicates that training has failed. Implemented by rhasspy-remote-http-hermes","title":"Remote HTTP Server"},{"location":"intent-recognition/#home-assistant-conversation","text":"Sends transcriptions from speech to text to Home Assistant's conversation API . If the response contains speech, Rhasspy can optionally speak it. Add to your profile : \"intent\": { \"system\": \"conversation\", \"conversation\": { \"handle_speech\": true } } When handle_speech is true , Rhasspy will forward the returned speech to your text to speech system. The settings from your profile's home_assistant section are automatically used (URL, access token, etc.). Because Home Assistant will already handle your intent (probably using an intent script ), Rhasspy will always generate an empty intent with this recognizer. TODO: Not implemented","title":"Home Assistant Conversation"},{"location":"intent-recognition/#command","text":"Recognizes intents from text using a custom external program. Your program should return a JSON object that describes the recognized intent; something like: { \"intent\": { \"name\": \"ChangeLightColor\", \"confidence\": 1.0 }, \"entities\": [ { \"entity\": \"name\", \"value\": \"bedroom light\" }, { \"entity\": \"color\", \"value\": \"red\" } ], \"text\": \"set the bedroom light to red\" } An empty intent.name property indicates a recognition failure. Add to your profile : \"intent\": { \"system\": \"command\", \"command\": { \"program\": \"/path/to/program\", \"arguments\": [] } } If you want to also call an external program during training, add to your profile: \"training\": { \"system\": \"auto\", \"intent\": { \"command\": { \"program\": \"/path/to/training/program\", \"arguments\": [] } } } If training.intent.command.program is set, Rhasspy will call your program with the intent graph generated by rhasspy-nlu provided as JSON on standard input. No response is expected, though a non-zero exit code indicates a training failure. The following environment variables are available to your program: $RHASSPY_BASE_DIR - path to the directory where Rhasspy is running from $RHASSPY_PROFILE - name of the current profile (e.g., \"en\") $RHASSPY_PROFILE_DIR - directory of the current profile (where profile.json is) See text2intent.sh for an example program. Implemented by rhasspy-remote-http-hermes","title":"Command"},{"location":"intent-recognition/#dummy","text":"Disables intent recognition. Add to your profile : \"intent\": { \"system\": \"dummy\" }","title":"Dummy"},{"location":"license/","text":"License Rhasspy itself is licensed under the MIT license, so feel free to do what you want with the code. Please see the individual licenses for the supporting tools as well. MIT License Copyright 2020 Michael Hansen Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"license/#license","text":"Rhasspy itself is licensed under the MIT license, so feel free to do what you want with the code. Please see the individual licenses for the supporting tools as well.","title":"License"},{"location":"license/#mit-license","text":"Copyright 2020 Michael Hansen Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"MIT License"},{"location":"profiles/","text":"Profiles A Rhasspy profile contains all of the necessary files for wake word detection, speech transcription, intent recognition, and training. Each profile is a directory contained in the top-level profiles directory. The profiles/defaults.json file contains the default configuration for all profiles. The profile.json file inside each individual profile directory (e.g., profiles/en/profile.json ) overrides settings in defaults.json . When starting Rhasspy, you must specify a profile name with --profile <NAME> where <NAME> is the name of the profile directory ( en , nl , etc.). Profile Directories Rhasspy looks for profile-related files in two directories: The system profile directory (read only) Override with --system-profiles <DIR> The user profile directory (read/write) Override with --user-profiles <DIR> Files in the user profile directory override system files, and Rhasspy will only ever write to the user profile directory. The default location for each of these directories is: TODO: Update Virtual Environment System profile location is $PWD/profiles where $PWD is Rhasspy's root directory (where run-venv.sh is located) User profile location is $HOME/.config/rhasspy/profiles Docker System profile location is either /usr/share/rhasspy/profiles (ALSA) or /home/rhasspy/profiles (PulseAudio) User profile location must be explicitly set and mapped to a volume: docker run ... -v /path/to/profiles:/profiles synesthesiam/rhasspy-server --user-profiles /profiles Example TODO: Rewrite Assume you are running Rhasspy in a virtual environment, and you add some new sentences to the en (English) profile in the web interface. When saving the sentences.ini file, Rhasspy will create $HOME/.config/rhasspy/profiles/en (if it doesn't exist), and write sentences.ini in that directory. If you adjust and save your settings, you will find them in $HOME/.config/rhasspy/profiles/en/profile.json . Downloading Profiles The first time Rhasspy loads a profile, it needs to download the required binary artifacts (acoustic model, base dictionary, etc.) from the internet . After the initial download, Rhasspy can function completely offline. If you need to install Rhasspy onto a machine that is not connected to the internet, you can simply download the artifacts yourself and place them in a download directory inside the appropriate profile directory. For example, the fr (French) profile has three artifacts : cmusphinx-fr-5.2.tar.gz fr-g2p.tar.gz fr-small.lm.gz If your user profile directory is $HOME/.config/rhasspy/profiles , then you should download/copy all three artifacts to $HOME/.config/rhasspy/profiles/fr/download on the offline machine. Now, when Rhasspy loads the fr profile and you click \"Download\", it will extract the files in the download directory without going out to the internet. Available Settings See the reference for all available profile settings.","title":"Profiles"},{"location":"profiles/#profiles","text":"A Rhasspy profile contains all of the necessary files for wake word detection, speech transcription, intent recognition, and training. Each profile is a directory contained in the top-level profiles directory. The profiles/defaults.json file contains the default configuration for all profiles. The profile.json file inside each individual profile directory (e.g., profiles/en/profile.json ) overrides settings in defaults.json . When starting Rhasspy, you must specify a profile name with --profile <NAME> where <NAME> is the name of the profile directory ( en , nl , etc.).","title":"Profiles"},{"location":"profiles/#profile-directories","text":"Rhasspy looks for profile-related files in two directories: The system profile directory (read only) Override with --system-profiles <DIR> The user profile directory (read/write) Override with --user-profiles <DIR> Files in the user profile directory override system files, and Rhasspy will only ever write to the user profile directory. The default location for each of these directories is: TODO: Update Virtual Environment System profile location is $PWD/profiles where $PWD is Rhasspy's root directory (where run-venv.sh is located) User profile location is $HOME/.config/rhasspy/profiles Docker System profile location is either /usr/share/rhasspy/profiles (ALSA) or /home/rhasspy/profiles (PulseAudio) User profile location must be explicitly set and mapped to a volume: docker run ... -v /path/to/profiles:/profiles synesthesiam/rhasspy-server --user-profiles /profiles","title":"Profile Directories"},{"location":"profiles/#example","text":"TODO: Rewrite Assume you are running Rhasspy in a virtual environment, and you add some new sentences to the en (English) profile in the web interface. When saving the sentences.ini file, Rhasspy will create $HOME/.config/rhasspy/profiles/en (if it doesn't exist), and write sentences.ini in that directory. If you adjust and save your settings, you will find them in $HOME/.config/rhasspy/profiles/en/profile.json .","title":"Example"},{"location":"profiles/#downloading-profiles","text":"The first time Rhasspy loads a profile, it needs to download the required binary artifacts (acoustic model, base dictionary, etc.) from the internet . After the initial download, Rhasspy can function completely offline. If you need to install Rhasspy onto a machine that is not connected to the internet, you can simply download the artifacts yourself and place them in a download directory inside the appropriate profile directory. For example, the fr (French) profile has three artifacts : cmusphinx-fr-5.2.tar.gz fr-g2p.tar.gz fr-small.lm.gz If your user profile directory is $HOME/.config/rhasspy/profiles , then you should download/copy all three artifacts to $HOME/.config/rhasspy/profiles/fr/download on the offline machine. Now, when Rhasspy loads the fr profile and you click \"Download\", it will extract the files in the download directory without going out to the internet.","title":"Downloading Profiles"},{"location":"profiles/#available-settings","text":"See the reference for all available profile settings.","title":"Available Settings"},{"location":"reference/","text":"Reference Supported Languages MQTT API HTTP API Websocket API Profile Settings Command Line Tools Supported Languages The table below lists which components and compatible with Rhasspy's supported languages. Category Name Offline? en de es fr it nl ru el hi zh vi pt sv ca Wake Word pocketsphinx \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 porcupine \u2713 \u2713 snowboy requires account \u2713 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 precise \u2713 \u2713 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 Speech to Text pocketsphinx \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 kaldi \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Intent Recognition fsticuffs \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 fuzzywuzzy \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 adapt \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 flair \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 rasaNLU needs extra software \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Text to Speech espeak \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 flite \u2713 \u2713 \u2713 picotts \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 marytts \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 wavenet \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2022 - yes, but requires training/customization MQTT API Rhasspy implements a superset of the Hermes protocol in rhasspy-hermes for the following components: Audio Server Automated Speech Recognition Dialogue Manager Grapheme to Phoneme Hotword Detection Intent Handling Natural Language Understanding Text to Speech Audio Server Messages for audio input and audio output . hermes/audioServer/<siteId>/audioFrame (binary) Chunk of WAV audio data wav_bytes: bytes - WAV data to play ( message payload ) siteId: string - Hermes site ID (part of topic) hermes/audioServer/<siteId>/playBytes/<requestId> (JSON) Play WAV data wav_bytes: bytes - WAV data to play (message payload) requestId: string - unique ID for request (part of topic) siteId: string - Hermes site ID (part of topic) Response(s) hermes/audioServer/<siteId>/playFinished (JSON) hermes/audioServer/<siteId>/playFinished Indicates that audio has finished playing Response to hermes/audioServer/<siteId>/playBytes/<requestId> siteId: string - Hermes site ID (part of topic) id: string = \"\" - requestId from request message Automated Speech Recognition Messages for speech to text . hermes/asr/toggleOn (JSON) Enables ASR system siteId: string = \"default\" - Hermes site ID hermes/asr/toggleOff (JSON) Disables ASR system siteId: string = \"default\" - Hermes site ID hermes/asr/startListening (JSON) Tell ASR system to start recording/transcribing siteId: string = \"default\" - Hermes site ID sessionId: string = \"\" - current session ID hermes/asr/stopListening (JSON) Tell ASR system to stop recording Emits textCaptured if silence has was not detected earlier siteId: string = \"default\" - Hermes site ID sessionId: string = \"\" - current session ID hermes/asr/textCaptured (JSON) Successful transcription, sent either when silence is detected or on stopListening text: string - transcription text likelihood: float - confidence from ASR system seconds: float - transcription time in seconds siteId: string = \"default\" - Hermes site ID sessionId: string = \"\" - current session ID hermes/error/asr (JSON) Sent when an error occurs in the ASR system error: string - description of the error context: string - system-defined context of the error siteId: string = \"default\" - Hermes site ID sessionId: string = \"\" - current session ID rhasspy/asr/<siteId>/train (JSON, Rhasspy only) Instructs the ASR system to re-train id: string - unique ID for request (copied to trainSuccess ) graph_dict: object - intent graph from rhasspy-nlu encoded as a JSON object siteId: string - Hermes site ID (part of topic) Response(s) rhasspy/asr/<siteId>/trainSuccess hermes/error/asr rhasspy/asr/<siteId>/trainSuccess (JSON, Rhasspy only) Indicates that training was successful id: string - unique ID from request (copied from train ) siteId: string - Hermes site ID (part of topic) Response to rhasspy/asr/<siteId>/train rhasspy/asr/<siteId>/<sessionId>/audioCaptured (binary, Rhasspy only) WAV audio data captured by ASR session siteId: string - Hermes site ID (part of topic) sessionId: string - current session ID (part of topic) Only sent if sendAudioCaptured = true in startListening Dialogue Manager Messages for managing dialogue sessions. These can be initiated by a hotword detected message (or /api/listen-for-command ), and manually with a startSession message (or /api/start-recording ). hermes/dialogueManager/startSession (JSON) Starts a new dialogue session (done automatically on hotword detected ) init: object - JSON object with one of two forms: Action type: string = \"action\" - required canBeEnqueued: bool - true if session can be queued if there is already one (required) text: string = \"\" - sentence to speak using text to speech intentFilter: [string] = null - valid intent names ( null means all) sendIntentNotRecognized: bool = false - send hermes/dialogueManager/intentNotRecognized if intent recognition fails Notification type: string = \"notification\" - required text: string - sentence to speak using text to speech (required) siteId: string = \"default\" - Hermes site ID customData: string = \"\" - user-defined data passed to subsequent session messages Response(s) hermes/dialogueManager/sessionStarted hermes/dialogueManager/sessionQueued hermes/dialogueManager/sessionStarted (JSON) Indicates a session has started siteId: string = \"default\" - Hermes site ID sessionId: string = \"\" - current session ID customData: string = \"\" - user-defined data (copied from startSession ) Response to [ hermes/dialogueManager/startSession ] hermes/dialogueManager/sessionQueued (JSON) Indicates a session has been queued (only when init.canBeEnqueued = true in startSession ) siteId: string = \"default\" - Hermes site ID sessionId: string = \"\" - current session ID customData: string = \"\" - user-defined data (copied from startSession ) Response to [ hermes/dialogueManager/startSession ] hermes/dialogueManager/continueSession (JSON) Requests that a session be continued after an intent has been recognized sessionId: string - current session ID (required) customData: string = \"\" - user-defined data (overrides session customData if not empty) text: string = \"\" - sentence to speak using text to speech intentFilter: [string] = null - valid intent names ( null means all) sendIntentNotRecognized: bool = false - send hermes/dialogueManager/intentNotRecognized if intent recognition fails hermes/dialogueManager/endSession (JSON) Requests that a session be terminated nominally sessionId: string - current session ID (required) customData: string = \"\" - user-defined data (overrides session customData if not empty) hermes/dialogueManager/sessionEnded (JSON) Indicates a session has terminated termination: string reason for termination (required), one of: nominal abortedByUser intentNotRecognized timeout error siteId: string = \"default\" - Hermes site ID sessionId: string = \"\" - current session ID customData: string = \"\" - user-defined data (copied from startSession ) Response to hermes/dialogueManager/endSession or other reasons for a session termination hermes/dialogueManager/intentNotRecognized (JSON) Sent when intent recognition fails during a session (only when init.sendIntentNotRecognized = true in startSession ) input: string input to NLU system (required) siteId: string = \"default\" - Hermes site ID sessionId: string = \"\" - current session ID customData: string = \"\" - user-defined data (copied from startSession ) Grapheme to Phoneme Messages for looking up word pronunciations . See also the /api/lookup HTTP endpoint. Words are usually looked up from a phonetic dictionary included with the ASR system. The current speech to text services handle these messages. rhasspy/g2p/pronounce (JSON, Rhasspy only) Requests phonetic pronunciations of words id: string = \"\" - unique ID for request (copied to phonemes ) words: [string] - words to pronounce (required) numGuesses: int = 5 - number of guesses if not in dictionary siteId: string = \"default\" - Hermes site ID sessionId: string = \"\" - current session ID Response(s) rhasspy/g2p/phonemes rhasspy/g2p/phonemes (JSON, Rhasspy only) Phonetic pronunciations of words, either from a dictionary or grapheme-to-phoneme model wordPhonemes: [object] - phonetic pronunciations (required), keyed by word, values are: phonemes: [string] - phonemes for word (key) guessed: bool - true if pronunciation came from a grapheme-to-phoneme model id: string = \"\" - unique ID for request (copied from pronounce ) siteId: string = \"default\" - Hermes site ID sessionId: string = \"\" - current session ID Response to rhasspy/g2p/pronounce rhasspy/error/g2p (JSON, Rhasspy only) Sent when an error occurs in the G2P system error: string - description of the error context: string - system-defined context of the error siteId: string = \"default\" - Hermes site ID sessionId: string = \"\" - current session ID Hotword Detection Messages for wake word detection . See also the /api/listen-for-wake HTTP endpoint and the /api/events/wake Websocket endpoint. hermes/hotword/toggleOn (JSON) Enables hotword detection siteId: string = \"default\" - Hermes site ID hermes/hotword/toggleOff (JSON) Disables hotword detection siteId: string = \"default\" - Hermes site ID hermes/hotword/<wakewordId>/detected (JSON) Indicates a hotword was successfully detected wakewordId: string - wake word ID (part of topic) modelId: string - ID of wake word model used (service specific) modelVersion: string = \"\" - version of wake word model used (service specific) modelType: string = \"personal\" - type of wake word model used (service specific) currentSensitivity: float = 1.0 - sensitivity of wake word detection (service specific) siteId: string = \"default\" - Hermes site ID sessionId: string = \"\" - current session ID (Rhasspy only) hermes/error/hotword (JSON, Rhasspy only) Sent when an error occurs in the hotword system error: string - description of the error context: string - system-defined context of the error siteId: string = \"default\" - Hermes site ID Intent Handling Messages for intent handling . rhasspy/handle/toggleOn (JSON, Rhasspy only) Enables intent handling siteId: string = \"default\" - Hermes site ID rhasspy/handle/toggleOff (JSON, Rhasspy only) Disables intent handling siteId: string = \"default\" - Hermes site ID Natural Language Understanding hermes/nlu/query (JSON) Request an intent to be recognized from text input: string - text to recognize intent from (required) intentFilter: [string] = null - valid intent names ( null means all) id: string = \"\" - unique id for request (copied to response messages) siteId: string = \"default\" - Hermes site ID sessionId: string = \"\" - current session ID Response(s) hermes/nlu/intent/<intentName> hermes/nlu/intentNotRecognized hermes/nlu/intent/<intentName> (JSON) Sent when an intent was successfully recognized input: string - text from query (required) intent: object - details of recognized intent (required) intentName: string - name of intent (required) confidenceScore: float - confidence from NLU system for this intent (required) slots: [object] = [] - details of named entities, list of: entity: string - name of entity (required) slotName: string - name of slot (required) confidence: float - confidence from NLU system for this slot (required) raw_value: string - entity value without substitutons (required) value: string - entity value with substitutons (required) range: object = null - indexes of entity value in text start: int - start index end: int - end index (exclusive) id: string = \"\" - unique id for request (copied from query ) siteId: string = \"default\" - Hermes site ID sessionId: string = \"\" - current session ID customData: string = \"\" - user-defined data (copied from startSession ) asrTokens: [string] = [] - tokens from transcription asrConfidence: float = 1.0 - confidence from ASR system for input text Response to hermes/nlu/query hermes/nlu/intentNotRecognized (JSON) Sent when intent recognition fails input: string - text from query (required) id: string = \"\" - unique id for request (copied from query ) siteId: string = \"default\" - Hermes site ID sessionId: string = \"\" - current session ID Response to hermes/nlu/query hermes/error/nlu (JSON) Sent when an error occurs in the NLU system error: string - description of the error context: string - system-defined context of the error siteId: string = \"default\" - Hermes site ID sessionId: string = \"\" - current session ID rhasspy/nlu/<siteId>/train (JSON, Rhasspy only) Instructs the NLU system to re-train id: string - unique ID for request (copied to trainSuccess ) graph_dict: object - intent graph from rhasspy-nlu encoded as a JSON object siteId: string - Hermes site ID (part of topic) Response(s) rhasspy/nlu/<siteId>/trainSuccess hermes/error/nlu rhasspy/nlu/<siteId>/trainSuccess (JSON, Rhasspy only) Indicates that training was successful id: string - unique ID from request (copied from train ) siteId: string - Hermes site ID (part of topic) Response to rhasspy/nlu/<siteId>/train Text to Speech hermes/tts/say (JSON) Generate spoken audio for a sentence using the configured text to speech system Automatically sends playBytes playBytes.requestId = say.id text: string - sentence to speak (required) lang: string = \"\" - language for TTS system id: string = \"\" - unique ID for request (copied to sayFinished ) siteId: string = \"default\" - Hermes site ID sessionId: string = \"\" - current session ID Response(s) hermes/tts/sayFinished (JSON) hermes/tts/sayFinished (JSON) Indicates that the text to speech system has finished generating audio id: string = \"\" - unique ID for request (copied from say ) siteId: string = \"default\" - Hermes site ID Response to hermes/tts/say Listen for playFinished to determine when audio is finished playing playFinished.id = sayFinished.id HTTP API Rhasspy's HTTP endpoints are documented below. You can also visit /api/ in your Rhasspy server (note the final slash) to try out each endpoint. Application authors may want to use the rhasspy-client , which provides a high-level interface to a remote Rhasspy server. Endpoints /api/custom-words GET custom word dictionary as plain text, or POST to overwrite it See custom_words.txt in your profile directory /api/download-profile Force Rhasspy to re-download profile /api/listen-for-command POST to wake Rhasspy up and start listening for a voice command Returns intent JSON when command is finished ?nohass=true - stop Rhasspy from handling the intent ?timeout=<seconds> - override default command timeout ?entity=<entity>&value=<value> - set custom entity/value in recognized intent /api/listen-for-wake POST \"on\" to have Rhasspy listen for a wake word POST \"off\" to disable wake word /api/lookup POST word as plain text to look up or guess pronunciation ?n=<number> - return at most n guessed pronunciations /api/microphones GET list of available microphones /api/phonemes GET example phonemes from speech recognizer for your profile See phoneme_examples.txt in your profile directory /api/play-wav POST to play WAV data /api/profile GET the JSON for your profile, or POST to overwrite it ?layers=profile to only see settings different from defaults.json See profile.json in your profile directory /api/restart Restart Rhasspy server /api/sentences GET voice command templates or POST to overwrite Set Accept: application/json to GET JSON with all sentence files Set Content-Type: application/json to POST JSON with sentences for multiple files See sentences.ini and intents directory in your profile /api/slots GET slot values as JSON or POST to add to/overwrite them ?overwrite_all=true to clear slots in JSON before writing /api/speakers GET list of available audio output devices /api/speech-to-intent POST a WAV file and have Rhasspy process it as a voice command Returns intent JSON when command is finished ?nohass=true - stop Rhasspy from handling the intent /api/speech-to-text POST a WAV file and have Rhasspy return the text transcription Set Accept: application/json to receive JSON with more details ?noheader=true - send raw 16-bit 16Khz mono audio without a WAV header /api/start-recording POST to have Rhasspy start recording a voice command /api/stop-recording POST to have Rhasspy stop recording and process recorded data as a voice command Returns intent JSON when command has been processed ?nohass=true - stop Rhasspy from handling the intent /api/test-microphones GET list of available microphones and if they're working /api/text-to-intent POST text and have Rhasspy process it as command Returns intent JSON when command has been processed ?nohass=true - stop Rhasspy from handling the intent /api/text-to-speech POST text and have Rhasspy speak it ?play=false - get WAV data instead of having Rhasspy speak ?voice=<voice> - override default TTS voice ?language=<language> - override default TTS language or locale ?repeat=true - have Rhasspy repeat the last sentence it spoke /api/train POST to re-train your profile /api/unknown-words GET words that Rhasspy doesn't know in your sentences See unknown_words.txt in your profile directory /api/mqtt POST JSON payload to /api/mqtt/your/full/topic Payload will be published to your/full/topic Websocket API /api/events/intent Emits JSON-encoded intents after each NLU query /api/events/text Emits JSON-encoded transcriptions after each ASR transcription /api/events/wake Emits JSON-encoded detections after each wake word detection /api/mqtt Allows you to subscribe to, receive, and publish JSON-encoded MQTT messages Profile Settings All available profile sections and settings are listed below: home_assistant - how to communicate with Home Assistant/Hass.io url - Base URL of Home Assistant server (no /api ) access_token - long-lived access token for Home Assistant (Hass.io token is used automatically) api_password - Password, if you have that enabled (deprecated) pem_file - Full path to your CA_BUNDLE file or a directory with certificates of trusted CAs event_type_format - Python format string used to create event type from intent type ( {0} ) speech_to_text - transcribing voice commands to text system - name of speech to text system ( pocketsphinx , kaldi , remote , command , remote , hermes , or dummy ) pocketsphinx - configuration for Pocketsphinx compatible - true if profile can use pocketsphinx for speech recognition acoustic_model - directory with CMU 16 kHz acoustic model base_dictionary - large text file with word pronunciations (read only) custom_words - small text file with words/pronunciations added by user dictionary - text file with all words/pronunciations needed for example sentences unknown_words - small text file with guessed word pronunciations (from phonetisaurus) language_model - text file with trigram ARPA language model built from example sentences open_transcription - true if general language model should be used (custom voices commands ignored) base_language_model - large general language model (read only) mllr_matrix - MLLR matrix from acoustic model tuning mix_weight - how much of the base language model to mix in during training (0-1) phoneme_examples - text file with examples for each acoustic model phoneme kaldi - configuration for Kaldi compatible - true if profile can use Kaldi for speech recognition kaldi_dir - absolute path to Kaldi root directory model_dir - directory where Kaldi model is stored (relative to profile directory) graph - directory where HCLG.fst is located (relative to model_dir ) base_graph - directory where large general HCLG.fst is located (relative to model_dir ) base_dictionary - large text file with word pronunciations (read only) custom_words - small text file with words/pronunciations added by user dictionary - text file with all words/pronunciations needed for example sentences open_transcription - true if general language model should be used (custom voices commands ignored) unknown_words - small text file with guessed word pronunciations (from phonetisaurus) mix_weight - how much of the base language model to mix in during training (0-1) phoneme_examples - text file with examples for each acoustic model phoneme remote - configuration for remote Rhasspy server url - URL to POST WAV data for transcription (e.g., http://your-rhasspy-server:12101/api/speech-to-text ) command - configuration for external speech-to-text program program - path to executable arguments - list of arguments to pass to program sentences_ini - Ini file with example sentences/JSGF templates grouped by intent sentences_dir - Directory with additional sentence templates (default: intents ) g2p_model - finite-state transducer for phonetisaurus to guess word pronunciations g2p_casing - casing to force for g2p model ( upper , lower , or blank) dictionary_casing - casing to force for dictionary words ( upper , lower , or blank) slots_dir - directory to look for slots lists (default: slots ) slot_programs - directory to look for slot programs (default slot_programs ) intent - transforming text commands to intents system - intent recognition system ( fsticuffs , fuzzywuzzy , rasa , remote , adapt , command , or dummy ) fsticuffs - configuration for OpenFST-based intent recognizer intent_json - path to intent graph JSON file generated by [rhasspy-nlu][https://github.com/rhasspy/rhasspy-nlu] converters_dir - directory to look for converter programs (default: converters ) ignore_unknown_words - true if words not in the FST symbol table should be ignored fuzzy - true if text is matching in a fuzzy manner, skipping words in stop_words.txt fuzzywuzzy - configuration for simplistic Levenshtein distance based intent recognizer examples_json - JSON file with intents/example sentences min_confidence - minimum confidence required for intent to be converted to a JSON event (0-1) remote - configuration for remote Rhasspy server url - URL to POST text to for intent recognition (e.g., http://your-rhasspy-server:12101/api/text-to-intent ) rasa - configuration for Rasa NLU based intent recognizer url - URL of remote Rasa NLU server (e.g., http://localhost:5005/ ) examples_markdown - Markdown file to generate with intents/example sentences project_name - name of project to generate during training adapt - configuration for Mycroft Adapt based intent recognizer stop_words - text file with words to ignore in training sentences command - configuration for external speech-to-text program program - path to executable arguments - list of arguments to pass to program replace_numbers if true, automatically replace number ranges ( N..M ) or numbers ( N ) with words text_to_speech - pronouncing words system - text to speech system ( espeak , flite , picotts , marytts , command , remote , hermes , or dummy ) espeak - configuration for eSpeak phoneme_map - text file mapping CMU phonemes to eSpeak phonemes flite - configuration for flite voice - name of voice to use (e.g., kal16 , rms , awb ) picotts - configuration for PicoTTS language - language to use (default if not present) marytts - configuration for MaryTTS url - address:port of MaryTTS server (port is usually 59125) voice - name of voice to use (e.g., cmu-slt ). Default if not present. locale - name of locale to use (e.g., en-US ). Default if not present. wavenet - configuration for Google's WaveNet cache_dir - path to directory in your profile where WAV files are cached credentials_json - path to the JSON credentials file (generated online) gender - gender of speaker ( MALE FEMALE ) language_code - language/locale e.g. en-US , sample_rate - WAV sample rate (default: 22050) url - URL of WaveNet endpoint voice - voice to use (e.g., Wavenet-C ) fallback_tts - text to speech system to use when offline or error occurs (e.g., espeak ) remote - configuration for remote text to speech server url - URL to POST sentence to and get back WAV data training - training speech/intent recognizers speech_to_text - training for speech decoder system - speech to text training system ( auto or dummy ) command - configuration for external speech-to-text training program program - path to executable arguments - list of arguments to pass to program remote - configuration for external HTTP endpoint url - URL of speech to text training endpoint intent - training for intent recognizer system - intent recognizer training system ( auto or dummy ) command - configuration for external intent recognizer training program program - path to executable arguments - list of arguments to pass to program remote - configuration for external HTTP endpoint url - URL of intent recognizer training endpoint wake - waking Rhasspy up for speech input system - wake word recognition system ( pocketsphinx , snowboy , precise , porcupine , command , hermes , or dummy ) pocketsphinx - configuration for Pocketsphinx wake word recognizer keyphrase - phrase to wake up on (3-4 syllables recommended) threshold - sensitivity of detection (recommended range 1e-50 to 1e-5) chunk_size - number of bytes per chunk to feed to Pocketsphinx (default 960) snowboy - configuration for snowboy model - path to model file(s), separated by commas (in profile directory) sensitivity - model sensitivity (0-1, default 0.5) audio_gain - audio gain (default 1) apply_frontend - true if ApplyFrontend should be set chunk_size - number of bytes per chunk to feed to snowboy (default 960) model_settings - settings for each snowboy model path (e.g., snowboy/snowboy.umdl ) <MODEL_PATH> sensitivity - model sensitivity audio_gain - audio gain apply_frontend - true if ApplyFrontend should be set precise - configuration for Mycroft Precise engine_path - path to the precise-engine binary model - path to model file (in profile directory) sensitivity - model sensitivity (0-1, default 0.5) trigger_level - number of events to trigger activation (default 3) chunk_size - number of bytes per chunk to feed to Precise (default 2048) porcupine - configuration for PicoVoice's Porcupine library_path - path to libpv_porcupine.so for your platform/architecture model_path - path to the porcupine_params.pv (lib/common) keyword_path - path to the .ppn keyword file sensitivity - model sensitivity (0-1, default 0.5) command - configuration for external speech-to-text program program - path to executable arguments - list of arguments to pass to program microphone - configuration for audio recording system - audio recording system ( pyaudio , arecord , gstreamer , or dummy`) pyaudio - configuration for PyAudio microphone device - index of device to use or empty for default device frames_per_buffer - number of frames to read at a time (default 480) arecord - configuration for ALSA microphone device - name of ALSA device (see arecord -L ) to use or empty for default device chunk_size - number of bytes to read at a time (default 960) gstreamer - configuration for GStreamer audio recorder pipeline - GStreamer pipeline (e.g., FILTER ! FILTER ! ... ) without sink sounds - configuration for audio output from Rhasspy system - which sound output system to use ( aplay , command , remote , hermes , or dummy ) wake - path to WAV file to play when Rhasspy wakes up recorded - path to WAV file to play when a command finishes recording aplay - configuration for ALSA speakers device - name of ALSA device (see aplay -L ) to use or empty for default device command - configuration for external audio output program program - path to executable arguments - list of arguments to pass to program remote - configuration for remote audio output server url - URL to POST WAV data to handle system - which intent handling system to use ( hass , command , remote , or dummy ) command - configuration for external speech-to-text program program - path to executable arguments - list of arguments to pass to program remote - configuration for remote HTTP intent handler url - URL to POST intent JSON to and receive response JSON from mqtt - configuration for MQTT enabled - true if external broker should be used (false uses internal broker on port 12183) host - external MQTT host port - external MQTT port username - external MQTT username (blank for anonymous) password - external MQTT password site_id - one or more Hermes site IDs (comma separated). First ID is used for new messages dialogue - configuration for Hermes dialogue manager system - which dialogue manager to use ( rhasspy , hermes , or dummy ) download - configuration for profile file downloading url_base - base URL to download profile artifacts (defaults to Github) conditions - profile settings that will trigger file downloads keys are profile setting paths (e.g., wake.system ) values are dictionaries whose keys are profile settings values (e.g., snowboy ) settings may have the form <=N or !X to mean \"less than or equal to N\" or \"not X\" leaf nodes are dictionaries whose keys are destination file paths and whose values reference the files dictionary files - locations, etc. of files to download keys are names of files values are dictionaries with: url - URL of file to download (appended to url_base ) bytes_expected - number of bytes file should be after decompression unzip - true if file should be decompressed with gunzip parts - list of objects representing parts of a file that should be combined with cat fragment - fragment appended to file URL bytes_expected - number of bytes for this part Command Line Tools rhasspy-nlu rhasspy-hermes rhasspy-supervisor","title":"Reference"},{"location":"reference/#reference","text":"Supported Languages MQTT API HTTP API Websocket API Profile Settings Command Line Tools","title":"Reference"},{"location":"reference/#supported-languages","text":"The table below lists which components and compatible with Rhasspy's supported languages. Category Name Offline? en de es fr it nl ru el hi zh vi pt sv ca Wake Word pocketsphinx \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 porcupine \u2713 \u2713 snowboy requires account \u2713 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 precise \u2713 \u2713 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 Speech to Text pocketsphinx \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 kaldi \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Intent Recognition fsticuffs \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 fuzzywuzzy \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 adapt \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 flair \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 rasaNLU needs extra software \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Text to Speech espeak \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 flite \u2713 \u2713 \u2713 picotts \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 marytts \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 wavenet \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2022 - yes, but requires training/customization","title":"Supported Languages"},{"location":"reference/#mqtt-api","text":"Rhasspy implements a superset of the Hermes protocol in rhasspy-hermes for the following components: Audio Server Automated Speech Recognition Dialogue Manager Grapheme to Phoneme Hotword Detection Intent Handling Natural Language Understanding Text to Speech","title":"MQTT API"},{"location":"reference/#audio-server","text":"Messages for audio input and audio output . hermes/audioServer/<siteId>/audioFrame (binary) Chunk of WAV audio data wav_bytes: bytes - WAV data to play ( message payload ) siteId: string - Hermes site ID (part of topic) hermes/audioServer/<siteId>/playBytes/<requestId> (JSON) Play WAV data wav_bytes: bytes - WAV data to play (message payload) requestId: string - unique ID for request (part of topic) siteId: string - Hermes site ID (part of topic) Response(s) hermes/audioServer/<siteId>/playFinished (JSON) hermes/audioServer/<siteId>/playFinished Indicates that audio has finished playing Response to hermes/audioServer/<siteId>/playBytes/<requestId> siteId: string - Hermes site ID (part of topic) id: string = \"\" - requestId from request message","title":"Audio Server"},{"location":"reference/#automated-speech-recognition","text":"Messages for speech to text . hermes/asr/toggleOn (JSON) Enables ASR system siteId: string = \"default\" - Hermes site ID hermes/asr/toggleOff (JSON) Disables ASR system siteId: string = \"default\" - Hermes site ID hermes/asr/startListening (JSON) Tell ASR system to start recording/transcribing siteId: string = \"default\" - Hermes site ID sessionId: string = \"\" - current session ID hermes/asr/stopListening (JSON) Tell ASR system to stop recording Emits textCaptured if silence has was not detected earlier siteId: string = \"default\" - Hermes site ID sessionId: string = \"\" - current session ID hermes/asr/textCaptured (JSON) Successful transcription, sent either when silence is detected or on stopListening text: string - transcription text likelihood: float - confidence from ASR system seconds: float - transcription time in seconds siteId: string = \"default\" - Hermes site ID sessionId: string = \"\" - current session ID hermes/error/asr (JSON) Sent when an error occurs in the ASR system error: string - description of the error context: string - system-defined context of the error siteId: string = \"default\" - Hermes site ID sessionId: string = \"\" - current session ID rhasspy/asr/<siteId>/train (JSON, Rhasspy only) Instructs the ASR system to re-train id: string - unique ID for request (copied to trainSuccess ) graph_dict: object - intent graph from rhasspy-nlu encoded as a JSON object siteId: string - Hermes site ID (part of topic) Response(s) rhasspy/asr/<siteId>/trainSuccess hermes/error/asr rhasspy/asr/<siteId>/trainSuccess (JSON, Rhasspy only) Indicates that training was successful id: string - unique ID from request (copied from train ) siteId: string - Hermes site ID (part of topic) Response to rhasspy/asr/<siteId>/train rhasspy/asr/<siteId>/<sessionId>/audioCaptured (binary, Rhasspy only) WAV audio data captured by ASR session siteId: string - Hermes site ID (part of topic) sessionId: string - current session ID (part of topic) Only sent if sendAudioCaptured = true in startListening","title":"Automated Speech Recognition"},{"location":"reference/#dialogue-manager","text":"Messages for managing dialogue sessions. These can be initiated by a hotword detected message (or /api/listen-for-command ), and manually with a startSession message (or /api/start-recording ). hermes/dialogueManager/startSession (JSON) Starts a new dialogue session (done automatically on hotword detected ) init: object - JSON object with one of two forms: Action type: string = \"action\" - required canBeEnqueued: bool - true if session can be queued if there is already one (required) text: string = \"\" - sentence to speak using text to speech intentFilter: [string] = null - valid intent names ( null means all) sendIntentNotRecognized: bool = false - send hermes/dialogueManager/intentNotRecognized if intent recognition fails Notification type: string = \"notification\" - required text: string - sentence to speak using text to speech (required) siteId: string = \"default\" - Hermes site ID customData: string = \"\" - user-defined data passed to subsequent session messages Response(s) hermes/dialogueManager/sessionStarted hermes/dialogueManager/sessionQueued hermes/dialogueManager/sessionStarted (JSON) Indicates a session has started siteId: string = \"default\" - Hermes site ID sessionId: string = \"\" - current session ID customData: string = \"\" - user-defined data (copied from startSession ) Response to [ hermes/dialogueManager/startSession ] hermes/dialogueManager/sessionQueued (JSON) Indicates a session has been queued (only when init.canBeEnqueued = true in startSession ) siteId: string = \"default\" - Hermes site ID sessionId: string = \"\" - current session ID customData: string = \"\" - user-defined data (copied from startSession ) Response to [ hermes/dialogueManager/startSession ] hermes/dialogueManager/continueSession (JSON) Requests that a session be continued after an intent has been recognized sessionId: string - current session ID (required) customData: string = \"\" - user-defined data (overrides session customData if not empty) text: string = \"\" - sentence to speak using text to speech intentFilter: [string] = null - valid intent names ( null means all) sendIntentNotRecognized: bool = false - send hermes/dialogueManager/intentNotRecognized if intent recognition fails hermes/dialogueManager/endSession (JSON) Requests that a session be terminated nominally sessionId: string - current session ID (required) customData: string = \"\" - user-defined data (overrides session customData if not empty) hermes/dialogueManager/sessionEnded (JSON) Indicates a session has terminated termination: string reason for termination (required), one of: nominal abortedByUser intentNotRecognized timeout error siteId: string = \"default\" - Hermes site ID sessionId: string = \"\" - current session ID customData: string = \"\" - user-defined data (copied from startSession ) Response to hermes/dialogueManager/endSession or other reasons for a session termination hermes/dialogueManager/intentNotRecognized (JSON) Sent when intent recognition fails during a session (only when init.sendIntentNotRecognized = true in startSession ) input: string input to NLU system (required) siteId: string = \"default\" - Hermes site ID sessionId: string = \"\" - current session ID customData: string = \"\" - user-defined data (copied from startSession )","title":"Dialogue Manager"},{"location":"reference/#grapheme-to-phoneme","text":"Messages for looking up word pronunciations . See also the /api/lookup HTTP endpoint. Words are usually looked up from a phonetic dictionary included with the ASR system. The current speech to text services handle these messages. rhasspy/g2p/pronounce (JSON, Rhasspy only) Requests phonetic pronunciations of words id: string = \"\" - unique ID for request (copied to phonemes ) words: [string] - words to pronounce (required) numGuesses: int = 5 - number of guesses if not in dictionary siteId: string = \"default\" - Hermes site ID sessionId: string = \"\" - current session ID Response(s) rhasspy/g2p/phonemes rhasspy/g2p/phonemes (JSON, Rhasspy only) Phonetic pronunciations of words, either from a dictionary or grapheme-to-phoneme model wordPhonemes: [object] - phonetic pronunciations (required), keyed by word, values are: phonemes: [string] - phonemes for word (key) guessed: bool - true if pronunciation came from a grapheme-to-phoneme model id: string = \"\" - unique ID for request (copied from pronounce ) siteId: string = \"default\" - Hermes site ID sessionId: string = \"\" - current session ID Response to rhasspy/g2p/pronounce rhasspy/error/g2p (JSON, Rhasspy only) Sent when an error occurs in the G2P system error: string - description of the error context: string - system-defined context of the error siteId: string = \"default\" - Hermes site ID sessionId: string = \"\" - current session ID","title":"Grapheme to Phoneme"},{"location":"reference/#hotword-detection","text":"Messages for wake word detection . See also the /api/listen-for-wake HTTP endpoint and the /api/events/wake Websocket endpoint. hermes/hotword/toggleOn (JSON) Enables hotword detection siteId: string = \"default\" - Hermes site ID hermes/hotword/toggleOff (JSON) Disables hotword detection siteId: string = \"default\" - Hermes site ID hermes/hotword/<wakewordId>/detected (JSON) Indicates a hotword was successfully detected wakewordId: string - wake word ID (part of topic) modelId: string - ID of wake word model used (service specific) modelVersion: string = \"\" - version of wake word model used (service specific) modelType: string = \"personal\" - type of wake word model used (service specific) currentSensitivity: float = 1.0 - sensitivity of wake word detection (service specific) siteId: string = \"default\" - Hermes site ID sessionId: string = \"\" - current session ID (Rhasspy only) hermes/error/hotword (JSON, Rhasspy only) Sent when an error occurs in the hotword system error: string - description of the error context: string - system-defined context of the error siteId: string = \"default\" - Hermes site ID","title":"Hotword Detection"},{"location":"reference/#intent-handling","text":"Messages for intent handling . rhasspy/handle/toggleOn (JSON, Rhasspy only) Enables intent handling siteId: string = \"default\" - Hermes site ID rhasspy/handle/toggleOff (JSON, Rhasspy only) Disables intent handling siteId: string = \"default\" - Hermes site ID","title":"Intent Handling"},{"location":"reference/#natural-language-understanding","text":"hermes/nlu/query (JSON) Request an intent to be recognized from text input: string - text to recognize intent from (required) intentFilter: [string] = null - valid intent names ( null means all) id: string = \"\" - unique id for request (copied to response messages) siteId: string = \"default\" - Hermes site ID sessionId: string = \"\" - current session ID Response(s) hermes/nlu/intent/<intentName> hermes/nlu/intentNotRecognized hermes/nlu/intent/<intentName> (JSON) Sent when an intent was successfully recognized input: string - text from query (required) intent: object - details of recognized intent (required) intentName: string - name of intent (required) confidenceScore: float - confidence from NLU system for this intent (required) slots: [object] = [] - details of named entities, list of: entity: string - name of entity (required) slotName: string - name of slot (required) confidence: float - confidence from NLU system for this slot (required) raw_value: string - entity value without substitutons (required) value: string - entity value with substitutons (required) range: object = null - indexes of entity value in text start: int - start index end: int - end index (exclusive) id: string = \"\" - unique id for request (copied from query ) siteId: string = \"default\" - Hermes site ID sessionId: string = \"\" - current session ID customData: string = \"\" - user-defined data (copied from startSession ) asrTokens: [string] = [] - tokens from transcription asrConfidence: float = 1.0 - confidence from ASR system for input text Response to hermes/nlu/query hermes/nlu/intentNotRecognized (JSON) Sent when intent recognition fails input: string - text from query (required) id: string = \"\" - unique id for request (copied from query ) siteId: string = \"default\" - Hermes site ID sessionId: string = \"\" - current session ID Response to hermes/nlu/query hermes/error/nlu (JSON) Sent when an error occurs in the NLU system error: string - description of the error context: string - system-defined context of the error siteId: string = \"default\" - Hermes site ID sessionId: string = \"\" - current session ID rhasspy/nlu/<siteId>/train (JSON, Rhasspy only) Instructs the NLU system to re-train id: string - unique ID for request (copied to trainSuccess ) graph_dict: object - intent graph from rhasspy-nlu encoded as a JSON object siteId: string - Hermes site ID (part of topic) Response(s) rhasspy/nlu/<siteId>/trainSuccess hermes/error/nlu rhasspy/nlu/<siteId>/trainSuccess (JSON, Rhasspy only) Indicates that training was successful id: string - unique ID from request (copied from train ) siteId: string - Hermes site ID (part of topic) Response to rhasspy/nlu/<siteId>/train","title":"Natural Language Understanding"},{"location":"reference/#text-to-speech","text":"hermes/tts/say (JSON) Generate spoken audio for a sentence using the configured text to speech system Automatically sends playBytes playBytes.requestId = say.id text: string - sentence to speak (required) lang: string = \"\" - language for TTS system id: string = \"\" - unique ID for request (copied to sayFinished ) siteId: string = \"default\" - Hermes site ID sessionId: string = \"\" - current session ID Response(s) hermes/tts/sayFinished (JSON) hermes/tts/sayFinished (JSON) Indicates that the text to speech system has finished generating audio id: string = \"\" - unique ID for request (copied from say ) siteId: string = \"default\" - Hermes site ID Response to hermes/tts/say Listen for playFinished to determine when audio is finished playing playFinished.id = sayFinished.id","title":"Text to Speech"},{"location":"reference/#http-api","text":"Rhasspy's HTTP endpoints are documented below. You can also visit /api/ in your Rhasspy server (note the final slash) to try out each endpoint. Application authors may want to use the rhasspy-client , which provides a high-level interface to a remote Rhasspy server.","title":"HTTP API"},{"location":"reference/#endpoints","text":"/api/custom-words GET custom word dictionary as plain text, or POST to overwrite it See custom_words.txt in your profile directory /api/download-profile Force Rhasspy to re-download profile /api/listen-for-command POST to wake Rhasspy up and start listening for a voice command Returns intent JSON when command is finished ?nohass=true - stop Rhasspy from handling the intent ?timeout=<seconds> - override default command timeout ?entity=<entity>&value=<value> - set custom entity/value in recognized intent /api/listen-for-wake POST \"on\" to have Rhasspy listen for a wake word POST \"off\" to disable wake word /api/lookup POST word as plain text to look up or guess pronunciation ?n=<number> - return at most n guessed pronunciations /api/microphones GET list of available microphones /api/phonemes GET example phonemes from speech recognizer for your profile See phoneme_examples.txt in your profile directory /api/play-wav POST to play WAV data /api/profile GET the JSON for your profile, or POST to overwrite it ?layers=profile to only see settings different from defaults.json See profile.json in your profile directory /api/restart Restart Rhasspy server /api/sentences GET voice command templates or POST to overwrite Set Accept: application/json to GET JSON with all sentence files Set Content-Type: application/json to POST JSON with sentences for multiple files See sentences.ini and intents directory in your profile /api/slots GET slot values as JSON or POST to add to/overwrite them ?overwrite_all=true to clear slots in JSON before writing /api/speakers GET list of available audio output devices /api/speech-to-intent POST a WAV file and have Rhasspy process it as a voice command Returns intent JSON when command is finished ?nohass=true - stop Rhasspy from handling the intent /api/speech-to-text POST a WAV file and have Rhasspy return the text transcription Set Accept: application/json to receive JSON with more details ?noheader=true - send raw 16-bit 16Khz mono audio without a WAV header /api/start-recording POST to have Rhasspy start recording a voice command /api/stop-recording POST to have Rhasspy stop recording and process recorded data as a voice command Returns intent JSON when command has been processed ?nohass=true - stop Rhasspy from handling the intent /api/test-microphones GET list of available microphones and if they're working /api/text-to-intent POST text and have Rhasspy process it as command Returns intent JSON when command has been processed ?nohass=true - stop Rhasspy from handling the intent /api/text-to-speech POST text and have Rhasspy speak it ?play=false - get WAV data instead of having Rhasspy speak ?voice=<voice> - override default TTS voice ?language=<language> - override default TTS language or locale ?repeat=true - have Rhasspy repeat the last sentence it spoke /api/train POST to re-train your profile /api/unknown-words GET words that Rhasspy doesn't know in your sentences See unknown_words.txt in your profile directory /api/mqtt POST JSON payload to /api/mqtt/your/full/topic Payload will be published to your/full/topic","title":"Endpoints"},{"location":"reference/#websocket-api","text":"/api/events/intent Emits JSON-encoded intents after each NLU query /api/events/text Emits JSON-encoded transcriptions after each ASR transcription /api/events/wake Emits JSON-encoded detections after each wake word detection /api/mqtt Allows you to subscribe to, receive, and publish JSON-encoded MQTT messages","title":"Websocket API"},{"location":"reference/#profile-settings","text":"All available profile sections and settings are listed below: home_assistant - how to communicate with Home Assistant/Hass.io url - Base URL of Home Assistant server (no /api ) access_token - long-lived access token for Home Assistant (Hass.io token is used automatically) api_password - Password, if you have that enabled (deprecated) pem_file - Full path to your CA_BUNDLE file or a directory with certificates of trusted CAs event_type_format - Python format string used to create event type from intent type ( {0} ) speech_to_text - transcribing voice commands to text system - name of speech to text system ( pocketsphinx , kaldi , remote , command , remote , hermes , or dummy ) pocketsphinx - configuration for Pocketsphinx compatible - true if profile can use pocketsphinx for speech recognition acoustic_model - directory with CMU 16 kHz acoustic model base_dictionary - large text file with word pronunciations (read only) custom_words - small text file with words/pronunciations added by user dictionary - text file with all words/pronunciations needed for example sentences unknown_words - small text file with guessed word pronunciations (from phonetisaurus) language_model - text file with trigram ARPA language model built from example sentences open_transcription - true if general language model should be used (custom voices commands ignored) base_language_model - large general language model (read only) mllr_matrix - MLLR matrix from acoustic model tuning mix_weight - how much of the base language model to mix in during training (0-1) phoneme_examples - text file with examples for each acoustic model phoneme kaldi - configuration for Kaldi compatible - true if profile can use Kaldi for speech recognition kaldi_dir - absolute path to Kaldi root directory model_dir - directory where Kaldi model is stored (relative to profile directory) graph - directory where HCLG.fst is located (relative to model_dir ) base_graph - directory where large general HCLG.fst is located (relative to model_dir ) base_dictionary - large text file with word pronunciations (read only) custom_words - small text file with words/pronunciations added by user dictionary - text file with all words/pronunciations needed for example sentences open_transcription - true if general language model should be used (custom voices commands ignored) unknown_words - small text file with guessed word pronunciations (from phonetisaurus) mix_weight - how much of the base language model to mix in during training (0-1) phoneme_examples - text file with examples for each acoustic model phoneme remote - configuration for remote Rhasspy server url - URL to POST WAV data for transcription (e.g., http://your-rhasspy-server:12101/api/speech-to-text ) command - configuration for external speech-to-text program program - path to executable arguments - list of arguments to pass to program sentences_ini - Ini file with example sentences/JSGF templates grouped by intent sentences_dir - Directory with additional sentence templates (default: intents ) g2p_model - finite-state transducer for phonetisaurus to guess word pronunciations g2p_casing - casing to force for g2p model ( upper , lower , or blank) dictionary_casing - casing to force for dictionary words ( upper , lower , or blank) slots_dir - directory to look for slots lists (default: slots ) slot_programs - directory to look for slot programs (default slot_programs ) intent - transforming text commands to intents system - intent recognition system ( fsticuffs , fuzzywuzzy , rasa , remote , adapt , command , or dummy ) fsticuffs - configuration for OpenFST-based intent recognizer intent_json - path to intent graph JSON file generated by [rhasspy-nlu][https://github.com/rhasspy/rhasspy-nlu] converters_dir - directory to look for converter programs (default: converters ) ignore_unknown_words - true if words not in the FST symbol table should be ignored fuzzy - true if text is matching in a fuzzy manner, skipping words in stop_words.txt fuzzywuzzy - configuration for simplistic Levenshtein distance based intent recognizer examples_json - JSON file with intents/example sentences min_confidence - minimum confidence required for intent to be converted to a JSON event (0-1) remote - configuration for remote Rhasspy server url - URL to POST text to for intent recognition (e.g., http://your-rhasspy-server:12101/api/text-to-intent ) rasa - configuration for Rasa NLU based intent recognizer url - URL of remote Rasa NLU server (e.g., http://localhost:5005/ ) examples_markdown - Markdown file to generate with intents/example sentences project_name - name of project to generate during training adapt - configuration for Mycroft Adapt based intent recognizer stop_words - text file with words to ignore in training sentences command - configuration for external speech-to-text program program - path to executable arguments - list of arguments to pass to program replace_numbers if true, automatically replace number ranges ( N..M ) or numbers ( N ) with words text_to_speech - pronouncing words system - text to speech system ( espeak , flite , picotts , marytts , command , remote , hermes , or dummy ) espeak - configuration for eSpeak phoneme_map - text file mapping CMU phonemes to eSpeak phonemes flite - configuration for flite voice - name of voice to use (e.g., kal16 , rms , awb ) picotts - configuration for PicoTTS language - language to use (default if not present) marytts - configuration for MaryTTS url - address:port of MaryTTS server (port is usually 59125) voice - name of voice to use (e.g., cmu-slt ). Default if not present. locale - name of locale to use (e.g., en-US ). Default if not present. wavenet - configuration for Google's WaveNet cache_dir - path to directory in your profile where WAV files are cached credentials_json - path to the JSON credentials file (generated online) gender - gender of speaker ( MALE FEMALE ) language_code - language/locale e.g. en-US , sample_rate - WAV sample rate (default: 22050) url - URL of WaveNet endpoint voice - voice to use (e.g., Wavenet-C ) fallback_tts - text to speech system to use when offline or error occurs (e.g., espeak ) remote - configuration for remote text to speech server url - URL to POST sentence to and get back WAV data training - training speech/intent recognizers speech_to_text - training for speech decoder system - speech to text training system ( auto or dummy ) command - configuration for external speech-to-text training program program - path to executable arguments - list of arguments to pass to program remote - configuration for external HTTP endpoint url - URL of speech to text training endpoint intent - training for intent recognizer system - intent recognizer training system ( auto or dummy ) command - configuration for external intent recognizer training program program - path to executable arguments - list of arguments to pass to program remote - configuration for external HTTP endpoint url - URL of intent recognizer training endpoint wake - waking Rhasspy up for speech input system - wake word recognition system ( pocketsphinx , snowboy , precise , porcupine , command , hermes , or dummy ) pocketsphinx - configuration for Pocketsphinx wake word recognizer keyphrase - phrase to wake up on (3-4 syllables recommended) threshold - sensitivity of detection (recommended range 1e-50 to 1e-5) chunk_size - number of bytes per chunk to feed to Pocketsphinx (default 960) snowboy - configuration for snowboy model - path to model file(s), separated by commas (in profile directory) sensitivity - model sensitivity (0-1, default 0.5) audio_gain - audio gain (default 1) apply_frontend - true if ApplyFrontend should be set chunk_size - number of bytes per chunk to feed to snowboy (default 960) model_settings - settings for each snowboy model path (e.g., snowboy/snowboy.umdl ) <MODEL_PATH> sensitivity - model sensitivity audio_gain - audio gain apply_frontend - true if ApplyFrontend should be set precise - configuration for Mycroft Precise engine_path - path to the precise-engine binary model - path to model file (in profile directory) sensitivity - model sensitivity (0-1, default 0.5) trigger_level - number of events to trigger activation (default 3) chunk_size - number of bytes per chunk to feed to Precise (default 2048) porcupine - configuration for PicoVoice's Porcupine library_path - path to libpv_porcupine.so for your platform/architecture model_path - path to the porcupine_params.pv (lib/common) keyword_path - path to the .ppn keyword file sensitivity - model sensitivity (0-1, default 0.5) command - configuration for external speech-to-text program program - path to executable arguments - list of arguments to pass to program microphone - configuration for audio recording system - audio recording system ( pyaudio , arecord , gstreamer , or dummy`) pyaudio - configuration for PyAudio microphone device - index of device to use or empty for default device frames_per_buffer - number of frames to read at a time (default 480) arecord - configuration for ALSA microphone device - name of ALSA device (see arecord -L ) to use or empty for default device chunk_size - number of bytes to read at a time (default 960) gstreamer - configuration for GStreamer audio recorder pipeline - GStreamer pipeline (e.g., FILTER ! FILTER ! ... ) without sink sounds - configuration for audio output from Rhasspy system - which sound output system to use ( aplay , command , remote , hermes , or dummy ) wake - path to WAV file to play when Rhasspy wakes up recorded - path to WAV file to play when a command finishes recording aplay - configuration for ALSA speakers device - name of ALSA device (see aplay -L ) to use or empty for default device command - configuration for external audio output program program - path to executable arguments - list of arguments to pass to program remote - configuration for remote audio output server url - URL to POST WAV data to handle system - which intent handling system to use ( hass , command , remote , or dummy ) command - configuration for external speech-to-text program program - path to executable arguments - list of arguments to pass to program remote - configuration for remote HTTP intent handler url - URL to POST intent JSON to and receive response JSON from mqtt - configuration for MQTT enabled - true if external broker should be used (false uses internal broker on port 12183) host - external MQTT host port - external MQTT port username - external MQTT username (blank for anonymous) password - external MQTT password site_id - one or more Hermes site IDs (comma separated). First ID is used for new messages dialogue - configuration for Hermes dialogue manager system - which dialogue manager to use ( rhasspy , hermes , or dummy ) download - configuration for profile file downloading url_base - base URL to download profile artifacts (defaults to Github) conditions - profile settings that will trigger file downloads keys are profile setting paths (e.g., wake.system ) values are dictionaries whose keys are profile settings values (e.g., snowboy ) settings may have the form <=N or !X to mean \"less than or equal to N\" or \"not X\" leaf nodes are dictionaries whose keys are destination file paths and whose values reference the files dictionary files - locations, etc. of files to download keys are names of files values are dictionaries with: url - URL of file to download (appended to url_base ) bytes_expected - number of bytes file should be after decompression unzip - true if file should be decompressed with gunzip parts - list of objects representing parts of a file that should be combined with cat fragment - fragment appended to file URL bytes_expected - number of bytes for this part","title":"Profile Settings"},{"location":"reference/#command-line-tools","text":"rhasspy-nlu rhasspy-hermes rhasspy-supervisor","title":"Command Line Tools"},{"location":"services/","text":"Services Rhasspy is composed of independent services that communicate over MQTT using a superset of the Hermes protocol for these components: Web Server Dialogue Manager Audio Input Wake Word Detection Speech to Text Intent Recognition Intent Handling Text to Speech Audio Output Message payloads are typically JSON objects , except for the following messages whose payloads are binary WAV audio : hermes/audioServer/<siteId>/audioFrame WAV chunk from microphone hermes/audioServer/<siteId>/playBytes/<requestId> WAV audio to play through speakers rhasspy/asr/<siteId>/<sessionId>/audioCaptured WAV audio recorded from session Most messages contain a string siteId property, whose default value is \"default\". Each service takes one or more --siteId <NAME> arguments that determine which site IDs the service will listen for. If not specified, the service will listen for all sites . Web Server Provides a graphical web interface for managing Rhasspy, and handles downloading language-specific profile artifacts. Available Services rhasspy-server-hermes Vue.js based web UI at http://YOUR_SERVER:12101 Implements Rhasspy's HTTP API and websocket API Dialogue Manager Manages sessions initiated by a wake word detection or a startSession . Available Services rhasspy-dialogue-hermes Input Messages hermes/dialogueManager/startSession Start a new session hermes/dialogueManager/continueSession Continue an existing session hermes/dialogueManager/endSession End an existing session Output Messages hermes/dialogueManager/sessionStarted New session has started hermes/dialogueManager/sessionQueued New session has be enqueued hermes/dialogueManager/sessionEnded Existing session has terminated hermes/dialogueManager/intentNotRecognized Voice command was not recognized in existing session Audio Input Records audio from a microphone and streams it as WAV chunks over MQTT. See Audio Input for details. Available Services rhasspy-microphone-cli-hermes Calls an external program for audio input Implements arecord rhasspy-microphone-pyaudio-hermes Records directly from a PyAudio device Implements pyaudio Input Messages rhasspy/audioServer/getDevices Requests available input devices Output Messages hermes/audioServer/<siteId>/audioFrame WAV chunk from microphone rhasspy/audioServer/devices Description of available audio input devices Wake Word Detection Listens to WAV chunks and tries to detect a wake/hotword. See Wake Word for details. Available Services rhasspy-wake-porcupine-hermes rhasspy-wake-pocketsphinx-hermes rhasspy-wake-snowboy-hermes Input Messages hermes/hotword/toggleOn Enables wake word detection hermes/hotword/toggleOff Disables wake word detection Output Messages hermes/wake/hotword/<wakewordId>/detected Wake word successfully detected Speech to Text Listens to WAV chunks and transcribes voice commands. See Speech to Text for details. Available Services rhasspy-asr-kaldi-hermes rhasspy-asr-pocketsphinx-hermes Input Messages hermes/audioServer/<siteId>/audioFrame WAV chunk from microphone hermes/asr/toggleOn Enable ASR system hermes/asr/toggleOff Disable ASR system hermes/asr/startListening Start recording a voice command hermes/asr/stopListening Stop recording a voice command rhasspy/asr/<siteId>/train Re-train ASR system rhasspy/g2p/pronounce Get phonetic pronunciations for words Output Messages hermes/asr/textCaptured Successful voice command transcription hermes/error/asr Error during transcription/training rhasspy/asr/<siteId>/trainSuccess ASR training succeeded rhasspy/asr/<siteId>/<sessionId>/audioCaptured Audio recorded from voice command rhasspy/g2p/phonemes Phonetic pronunciations of words Intent Recognition Recognizes user intents from text input. See Intent Recognition for details. Available Services rhasspy-fuzzywuzzy-hermes rhasspy-nlu-hermes Input Messages hermes/nlu/query Recognize intent from text rhasspy/nlu/<siteId>/train Retrain NLU system Output Messages hermes/nlu/intent/<intentName> Intent successfully recognized hermes/nlu/intentNotRecognized Intent was not recognized hermes/error/nlu Error during recognition/training rhasspy/nlu/<siteId>/trainSuccess NLU training succeeded Intent Handling Dispatches recognized intents to home automation software. See Intent Handling for details. Available Services rhasspy-homeassistant-hermes Input Messages hermes/nlu/intent/<intentName> Intent successfully recognized hermes/handle/toggleOn Enable intent handling hermes/handle/toggleOff Disable intent handling Output Messages hermes/tts/say Speak a sentence Text to Speech Generates spoken audio for a sentence. See Text to Speech for details. Available Services rhasspy-tts-cli-hermes Calls external program for text to speech Implements espeak , flite , picoTTS , and command rhasspy-remote-http-hermes POSTs to remote web server for text to speech Implements remote ( --tts-url ) and command ( --tts-command ) Input Messages hermes/tts/say Speak a sentence Output Messages hermes/tts/sayFinished Finished generating audio Audio Output Plays WAV audio through an audio output device (speakers). See Audio Output for details. Available Services rhasspy-speakers-cli-hermes Input Messages hermes/audioServer/<siteId>/playBytes/<requestId> WAV audio to play through speakers rhasspy/audioServer/getDevices Request audio output devices Output Messages hermes/audioServer/<siteId>/playFinished Audio has finished playing rhasspy/audioServer/devices Details of audio output devices","title":"Services"},{"location":"services/#services","text":"Rhasspy is composed of independent services that communicate over MQTT using a superset of the Hermes protocol for these components: Web Server Dialogue Manager Audio Input Wake Word Detection Speech to Text Intent Recognition Intent Handling Text to Speech Audio Output Message payloads are typically JSON objects , except for the following messages whose payloads are binary WAV audio : hermes/audioServer/<siteId>/audioFrame WAV chunk from microphone hermes/audioServer/<siteId>/playBytes/<requestId> WAV audio to play through speakers rhasspy/asr/<siteId>/<sessionId>/audioCaptured WAV audio recorded from session Most messages contain a string siteId property, whose default value is \"default\". Each service takes one or more --siteId <NAME> arguments that determine which site IDs the service will listen for. If not specified, the service will listen for all sites .","title":"Services"},{"location":"services/#web-server","text":"Provides a graphical web interface for managing Rhasspy, and handles downloading language-specific profile artifacts.","title":"Web Server"},{"location":"services/#available-services","text":"rhasspy-server-hermes Vue.js based web UI at http://YOUR_SERVER:12101 Implements Rhasspy's HTTP API and websocket API","title":"Available Services"},{"location":"services/#dialogue-manager","text":"Manages sessions initiated by a wake word detection or a startSession .","title":"Dialogue Manager"},{"location":"services/#available-services_1","text":"rhasspy-dialogue-hermes","title":"Available Services"},{"location":"services/#input-messages","text":"hermes/dialogueManager/startSession Start a new session hermes/dialogueManager/continueSession Continue an existing session hermes/dialogueManager/endSession End an existing session","title":"Input Messages"},{"location":"services/#output-messages","text":"hermes/dialogueManager/sessionStarted New session has started hermes/dialogueManager/sessionQueued New session has be enqueued hermes/dialogueManager/sessionEnded Existing session has terminated hermes/dialogueManager/intentNotRecognized Voice command was not recognized in existing session","title":"Output Messages"},{"location":"services/#audio-input","text":"Records audio from a microphone and streams it as WAV chunks over MQTT. See Audio Input for details.","title":"Audio Input"},{"location":"services/#available-services_2","text":"rhasspy-microphone-cli-hermes Calls an external program for audio input Implements arecord rhasspy-microphone-pyaudio-hermes Records directly from a PyAudio device Implements pyaudio","title":"Available Services"},{"location":"services/#input-messages_1","text":"rhasspy/audioServer/getDevices Requests available input devices","title":"Input Messages"},{"location":"services/#output-messages_1","text":"hermes/audioServer/<siteId>/audioFrame WAV chunk from microphone rhasspy/audioServer/devices Description of available audio input devices","title":"Output Messages"},{"location":"services/#wake-word-detection","text":"Listens to WAV chunks and tries to detect a wake/hotword. See Wake Word for details.","title":"Wake Word Detection"},{"location":"services/#available-services_3","text":"rhasspy-wake-porcupine-hermes rhasspy-wake-pocketsphinx-hermes rhasspy-wake-snowboy-hermes","title":"Available Services"},{"location":"services/#input-messages_2","text":"hermes/hotword/toggleOn Enables wake word detection hermes/hotword/toggleOff Disables wake word detection","title":"Input Messages"},{"location":"services/#output-messages_2","text":"hermes/wake/hotword/<wakewordId>/detected Wake word successfully detected","title":"Output Messages"},{"location":"services/#speech-to-text","text":"Listens to WAV chunks and transcribes voice commands. See Speech to Text for details.","title":"Speech to Text"},{"location":"services/#available-services_4","text":"rhasspy-asr-kaldi-hermes rhasspy-asr-pocketsphinx-hermes","title":"Available Services"},{"location":"services/#input-messages_3","text":"hermes/audioServer/<siteId>/audioFrame WAV chunk from microphone hermes/asr/toggleOn Enable ASR system hermes/asr/toggleOff Disable ASR system hermes/asr/startListening Start recording a voice command hermes/asr/stopListening Stop recording a voice command rhasspy/asr/<siteId>/train Re-train ASR system rhasspy/g2p/pronounce Get phonetic pronunciations for words","title":"Input Messages"},{"location":"services/#output-messages_3","text":"hermes/asr/textCaptured Successful voice command transcription hermes/error/asr Error during transcription/training rhasspy/asr/<siteId>/trainSuccess ASR training succeeded rhasspy/asr/<siteId>/<sessionId>/audioCaptured Audio recorded from voice command rhasspy/g2p/phonemes Phonetic pronunciations of words","title":"Output Messages"},{"location":"services/#intent-recognition","text":"Recognizes user intents from text input. See Intent Recognition for details.","title":"Intent Recognition"},{"location":"services/#available-services_5","text":"rhasspy-fuzzywuzzy-hermes rhasspy-nlu-hermes","title":"Available Services"},{"location":"services/#input-messages_4","text":"hermes/nlu/query Recognize intent from text rhasspy/nlu/<siteId>/train Retrain NLU system","title":"Input Messages"},{"location":"services/#output-messages_4","text":"hermes/nlu/intent/<intentName> Intent successfully recognized hermes/nlu/intentNotRecognized Intent was not recognized hermes/error/nlu Error during recognition/training rhasspy/nlu/<siteId>/trainSuccess NLU training succeeded","title":"Output Messages"},{"location":"services/#intent-handling","text":"Dispatches recognized intents to home automation software. See Intent Handling for details.","title":"Intent Handling"},{"location":"services/#available-services_6","text":"rhasspy-homeassistant-hermes","title":"Available Services"},{"location":"services/#input-messages_5","text":"hermes/nlu/intent/<intentName> Intent successfully recognized hermes/handle/toggleOn Enable intent handling hermes/handle/toggleOff Disable intent handling","title":"Input Messages"},{"location":"services/#output-messages_5","text":"hermes/tts/say Speak a sentence","title":"Output Messages"},{"location":"services/#text-to-speech","text":"Generates spoken audio for a sentence. See Text to Speech for details.","title":"Text to Speech"},{"location":"services/#available-services_7","text":"rhasspy-tts-cli-hermes Calls external program for text to speech Implements espeak , flite , picoTTS , and command rhasspy-remote-http-hermes POSTs to remote web server for text to speech Implements remote ( --tts-url ) and command ( --tts-command )","title":"Available Services"},{"location":"services/#input-messages_6","text":"hermes/tts/say Speak a sentence","title":"Input Messages"},{"location":"services/#output-messages_6","text":"hermes/tts/sayFinished Finished generating audio","title":"Output Messages"},{"location":"services/#audio-output","text":"Plays WAV audio through an audio output device (speakers). See Audio Output for details.","title":"Audio Output"},{"location":"services/#available-services_8","text":"rhasspy-speakers-cli-hermes","title":"Available Services"},{"location":"services/#input-messages_7","text":"hermes/audioServer/<siteId>/playBytes/<requestId> WAV audio to play through speakers rhasspy/audioServer/getDevices Request audio output devices","title":"Input Messages"},{"location":"services/#output-messages_7","text":"hermes/audioServer/<siteId>/playFinished Audio has finished playing rhasspy/audioServer/devices Details of audio output devices","title":"Output Messages"},{"location":"speech-to-text/","text":"Speech to Text Rhasspy's primary function is convert voice commands to JSON events. The first step of this process is converting speech to text (transcription). Available speech to text systems are: Pocketsphinx Kaldi Remote HTTP Server External Command The following table summarizes language support for the various speech to text systems: System en de es fr it nl ru el hi zh vi pt ca pocketsphinx \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 kaldi \u2713 \u2713 \u2713 \u2713 \u2713 MQTT/Hermes Rhasspy transcribes audio according to the Hermes protocol . The following steps are needed to get a transcription: A hermes/asr/startListening message is sent with a unique sessionId One or more hermes/audioServer/<siteId>/audioFrame messages are sent with WAV audio data If enough silence is detected, a transcription is attempted A hermes/asr/stopListening message is sent with the same sessionId . If a transcription has been sent, it will be. Either a hermes/asr/textCaptured or a hermes/error/asr message will be sent in response. Pocketsphinx Does speech recognition with CMU's pocketsphinx . This is done completely offline, on your device. If you experience performance problems (usually on a Raspberry Pi), consider running on a home server as well and have your client Rhasspy use a remote HTTP connection . Add to your profile : \"speech_to_text\": { \"system\": \"pocketsphinx\", \"pocketsphinx\": { \"acoustic_model\": \"acoustic_model\", \"base_dictionary\": \"base_dictionary.txt\", \"custom_words\": \"custom_words.txt\", \"dictionary\": \"dictionary.txt\", \"language_model\": \"language_model.txt\" } } The dictionary , language_model , and unknown_words files are written during training by the default speech to text training system . The acoustic_model and base_dictionary components for each profile were taken from a set of pre-trained models . Anyone can extend Rhasspy to new languages by training a new acoustic model . When Rhasspy starts, it creates a pocketsphinx decoder with the following attributes: hmm - speech_to_text.pocketsphinx.acoustic_model (directory) dict - speech_to_text.pocketsphinx.dictionary (file) lm - speech_to_text.pocketsphinx.language_model (file) Open Transcription If you just want to use Rhasspy for general speech to text, you can set speech_to_text.pocketsphinx.open_transcription to true in your profile. This will use the included general language model (much slower) and ignore any custom voice commands you've specified. For English, German, and Dutch, you may want to use Kaldi instead for better results. Implemented by rhasspy-asr-pocketsphinx-hermes Kaldi Does speech recognition with Kaldi . This is done completely offline, on your device. If you experience performance problems (usually on a Raspberry Pi), consider running on a home server as well and have your client Rhasspy use a remote HTTP connection . { \"speech_to_text\": { \"system\": \"kaldi\", \"kaldi\": { \"base_dictionary\": \"base_dictionary.txt\", \"compatible\": true, \"custom_words\": \"custom_words.txt\", \"dictionary\": \"dictionary.txt\", \"graph\": \"graph\", \"kaldi_dir\": \"/opt/kaldi\", \"language_model\": \"language_model.txt\", \"model_dir\": \"model\", \"unknown_words\": \"unknown_words.txt\" } } } Rhasspy currently supports nnet3 and gmm Kaldi acoustic models. This requires Kaldi to be installed, which is...challenging. The Docker image of Rhasspy contains a pre-built copy of Kaldi, which might work for you outside of Docker. Make sure to set kaldi_dir to wherever you installed Kaldi. Open Transcription If you just want to use Rhasspy for general speech to text, you can set speech_to_text.kaldi.open_transcription to true in your profile. This will use the included general language model (much slower) and ignore any custom voice commands you've specified. Implemented by rhasspy-asr-kaldi-hermes Remote HTTP Server Uses a remote HTTP server to transform speech (WAV) to text. The /api/speech-to-text endpoint from Rhasspy's HTTP API does just this, allowing you to use a remote instance of Rhasspy for speech recognition. This is typically used in a client/server set up, where Rhasspy does speech/intent recognition on a home server with decent CPU/RAM available. Add to your profile : \"speech_to_text\": { \"system\": \"remote\", \"remote\": { \"url\": \"http://my-server:12101/api/speech-to-text\" } } During speech recognition, 16-bit 16 kHz mono WAV data will be POST-ed to the endpoint with the Content-Type set to audio/wav . A text/plain response with the transcription is expected back. Implemented by rhasspy-remote-http-hermes Home Assistant STT Platform Use an STT platform on your Home Assistant server. This is the same way Ada sends speech to Home Assistant. Add to your profile : \"speech_to_text\": { \"system\": \"hass_stt\", \"hass_stt\": { \"platform\": \"...\", \"sample_rate\": 16000, \"bit_size\": 16, \"channels\": 1, \"language\": \"en-US\" } } The settings from your profile's home_assistant section are automatically used (URL, access token, etc.). Rhasspy will convert audio to the configured format before streaming it to Home Assistant. In the future, this will be auto-detected from the STT platform API. TODO: Not implemented Command Calls a custom external program to do speech recognition. WAV audio data is provided to your program's standard in, and a transcription is expected on standard out. Add to your profile : \"speech_to_text\": { \"system\": \"command\", \"command\": { \"program\": \"/path/to/program\", \"arguments\": [] } } The following environment variables are available to your program: $RHASSPY_BASE_DIR - path to the directory where Rhasspy is running from $RHASSPY_PROFILE - name of the current profile (e.g., \"en\") $RHASSPY_PROFILE_DIR - directory of the current profile (where profile.json is) See speech2text.sh for an example program. If you want to also call an external program during training, add to your profile: \"training\": { \"system\": \"auto\", \"speech_to_text\": { \"command\": { \"program\": \"/path/to/training/program\", \"arguments\": [] } } } If training.speech_to_text.command.program is set, Rhasspy will call your program with the intent graph generated by rhasspy-nlu provided as JSON on standard input. No response is expected, though a non-zero exit code indicates a training failure. Implemented by rhasspy-remote-http-hermes Dummy Disables speech to text decoding. Add to your profile : \"speech_to_text\": { \"system\": \"dummy\" }","title":"Speech to Text"},{"location":"speech-to-text/#speech-to-text","text":"Rhasspy's primary function is convert voice commands to JSON events. The first step of this process is converting speech to text (transcription). Available speech to text systems are: Pocketsphinx Kaldi Remote HTTP Server External Command The following table summarizes language support for the various speech to text systems: System en de es fr it nl ru el hi zh vi pt ca pocketsphinx \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 kaldi \u2713 \u2713 \u2713 \u2713 \u2713","title":"Speech to Text"},{"location":"speech-to-text/#mqtthermes","text":"Rhasspy transcribes audio according to the Hermes protocol . The following steps are needed to get a transcription: A hermes/asr/startListening message is sent with a unique sessionId One or more hermes/audioServer/<siteId>/audioFrame messages are sent with WAV audio data If enough silence is detected, a transcription is attempted A hermes/asr/stopListening message is sent with the same sessionId . If a transcription has been sent, it will be. Either a hermes/asr/textCaptured or a hermes/error/asr message will be sent in response.","title":"MQTT/Hermes"},{"location":"speech-to-text/#pocketsphinx","text":"Does speech recognition with CMU's pocketsphinx . This is done completely offline, on your device. If you experience performance problems (usually on a Raspberry Pi), consider running on a home server as well and have your client Rhasspy use a remote HTTP connection . Add to your profile : \"speech_to_text\": { \"system\": \"pocketsphinx\", \"pocketsphinx\": { \"acoustic_model\": \"acoustic_model\", \"base_dictionary\": \"base_dictionary.txt\", \"custom_words\": \"custom_words.txt\", \"dictionary\": \"dictionary.txt\", \"language_model\": \"language_model.txt\" } } The dictionary , language_model , and unknown_words files are written during training by the default speech to text training system . The acoustic_model and base_dictionary components for each profile were taken from a set of pre-trained models . Anyone can extend Rhasspy to new languages by training a new acoustic model . When Rhasspy starts, it creates a pocketsphinx decoder with the following attributes: hmm - speech_to_text.pocketsphinx.acoustic_model (directory) dict - speech_to_text.pocketsphinx.dictionary (file) lm - speech_to_text.pocketsphinx.language_model (file)","title":"Pocketsphinx"},{"location":"speech-to-text/#open-transcription","text":"If you just want to use Rhasspy for general speech to text, you can set speech_to_text.pocketsphinx.open_transcription to true in your profile. This will use the included general language model (much slower) and ignore any custom voice commands you've specified. For English, German, and Dutch, you may want to use Kaldi instead for better results. Implemented by rhasspy-asr-pocketsphinx-hermes","title":"Open Transcription"},{"location":"speech-to-text/#kaldi","text":"Does speech recognition with Kaldi . This is done completely offline, on your device. If you experience performance problems (usually on a Raspberry Pi), consider running on a home server as well and have your client Rhasspy use a remote HTTP connection . { \"speech_to_text\": { \"system\": \"kaldi\", \"kaldi\": { \"base_dictionary\": \"base_dictionary.txt\", \"compatible\": true, \"custom_words\": \"custom_words.txt\", \"dictionary\": \"dictionary.txt\", \"graph\": \"graph\", \"kaldi_dir\": \"/opt/kaldi\", \"language_model\": \"language_model.txt\", \"model_dir\": \"model\", \"unknown_words\": \"unknown_words.txt\" } } } Rhasspy currently supports nnet3 and gmm Kaldi acoustic models. This requires Kaldi to be installed, which is...challenging. The Docker image of Rhasspy contains a pre-built copy of Kaldi, which might work for you outside of Docker. Make sure to set kaldi_dir to wherever you installed Kaldi.","title":"Kaldi"},{"location":"speech-to-text/#open-transcription_1","text":"If you just want to use Rhasspy for general speech to text, you can set speech_to_text.kaldi.open_transcription to true in your profile. This will use the included general language model (much slower) and ignore any custom voice commands you've specified. Implemented by rhasspy-asr-kaldi-hermes","title":"Open Transcription"},{"location":"speech-to-text/#remote-http-server","text":"Uses a remote HTTP server to transform speech (WAV) to text. The /api/speech-to-text endpoint from Rhasspy's HTTP API does just this, allowing you to use a remote instance of Rhasspy for speech recognition. This is typically used in a client/server set up, where Rhasspy does speech/intent recognition on a home server with decent CPU/RAM available. Add to your profile : \"speech_to_text\": { \"system\": \"remote\", \"remote\": { \"url\": \"http://my-server:12101/api/speech-to-text\" } } During speech recognition, 16-bit 16 kHz mono WAV data will be POST-ed to the endpoint with the Content-Type set to audio/wav . A text/plain response with the transcription is expected back. Implemented by rhasspy-remote-http-hermes","title":"Remote HTTP Server"},{"location":"speech-to-text/#home-assistant-stt-platform","text":"Use an STT platform on your Home Assistant server. This is the same way Ada sends speech to Home Assistant. Add to your profile : \"speech_to_text\": { \"system\": \"hass_stt\", \"hass_stt\": { \"platform\": \"...\", \"sample_rate\": 16000, \"bit_size\": 16, \"channels\": 1, \"language\": \"en-US\" } } The settings from your profile's home_assistant section are automatically used (URL, access token, etc.). Rhasspy will convert audio to the configured format before streaming it to Home Assistant. In the future, this will be auto-detected from the STT platform API. TODO: Not implemented","title":"Home Assistant STT Platform"},{"location":"speech-to-text/#command","text":"Calls a custom external program to do speech recognition. WAV audio data is provided to your program's standard in, and a transcription is expected on standard out. Add to your profile : \"speech_to_text\": { \"system\": \"command\", \"command\": { \"program\": \"/path/to/program\", \"arguments\": [] } } The following environment variables are available to your program: $RHASSPY_BASE_DIR - path to the directory where Rhasspy is running from $RHASSPY_PROFILE - name of the current profile (e.g., \"en\") $RHASSPY_PROFILE_DIR - directory of the current profile (where profile.json is) See speech2text.sh for an example program. If you want to also call an external program during training, add to your profile: \"training\": { \"system\": \"auto\", \"speech_to_text\": { \"command\": { \"program\": \"/path/to/training/program\", \"arguments\": [] } } } If training.speech_to_text.command.program is set, Rhasspy will call your program with the intent graph generated by rhasspy-nlu provided as JSON on standard input. No response is expected, though a non-zero exit code indicates a training failure. Implemented by rhasspy-remote-http-hermes","title":"Command"},{"location":"speech-to-text/#dummy","text":"Disables speech to text decoding. Add to your profile : \"speech_to_text\": { \"system\": \"dummy\" }","title":"Dummy"},{"location":"text-to-speech/","text":"Text to Speech After you voice command has been handled , it's common to produce speech as a response back to the user. Rhasspy has support for several text to speech systems which, importantly, can be played through any of the audio output systems. The following table summarizes language support for the various text to speech systems: System en de es fr it nl ru el hi zh vi pt ca espeak \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 flite \u2713 \u2713 picotts \u2713 \u2713 \u2713 \u2713 \u2713 marytts \u2713 \u2713 \u2713 \u2713 \u2713 wavenet \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 eSpeak Uses eSpeak to speak sentences. This is the default text to speech system and, while it sounds robotic, has the widest support for different languages. Add to your profile : \"text_to_speech\": { \"system\": \"espeak\", \"espeak\": { \"voice\": \"en\" } } Remove the voice option to have espeak use your profile's language automatically. You may also pass additional arguments to the espeak command. For example, \"text_to_speech\": { \"system\": \"espeak\", \"espeak\": { \"arguments\": [\"-s\", \"80\"] } } will speak the sentence more slowly. Implemented by rhasspy-tts-cli-hermes Flite Uses FestVox's flite for speech. Sounds better than espeak in most cases, but only supports English out of the box. Add to your profile : \"text_to_speech\": { \"system\": \"flite\", \"flite\": { \"voice\": \"kal16\" } } Some other included voices are rms , slt , and awb . Implemented by rhasspy-tts-cli-hermes PicoTTS Uses SVOX's picotts for text to speech. Sounds a bit better (to me) than flite or espeak . Included languages are en-US , en-GB , de-DE , es-ES , fr-FR and it-IT . Add to your profile : \"text_to_speech\": { \"system\": \"picotts\", \"picotts\": { \"language\": \"en-US\" } } Implemented by rhasspy-tts-cli-hermes MaryTTS Uses a remote MaryTTS web server. Supported languages include German, British and American English, French, Italian, Luxembourgish, Russian, Swedish, Telugu, and Turkish. An MaryTTS Docker image is available, though only the default voice is included. Add to your profile : \"text_to_speech\": { \"system\": \"marytts\", \"marytts\": { \"url\": \"http://localhost:59125\", \"voice\": \"cmu-slt\", \"locale\": \"en-US\" } } To run the Docker image, simply execute: docker run -it -p 59125:59125 synesthesiam/marytts:5.2 and visit http://localhost:59125 after it starts. If you're using docker compose , add the following to your docker-compose.yml file: marytts: image: synesthesiam/marytts:5.2 restart: unless-stopped ports: - \"59125:59125\" When using docker-compose, set marytts.url in your profile to be http://marytts:59125 . This will allow rhasspy, from within its docker container, to resolve and connect to marytts (its sibling container). Adding Voices For more English voices, run the following commands in a Bash shell: mkdir -p marytts-5.2/download for voice in dfki-prudence dfki-poppy dfki-obadiah dfki-spike cmu-bdl cmu-rms; do wget -O marytts-5.2/download/voice-${voice}-hsmm-5.2.zip https://github.com/marytts/voice-${voice}-hsmm/releases/download/v5.2/voice-${voice}-hsmm-5.2.zip; unzip -d marytts-5.2 marytts-5.2/download/voice-${voice}-hsmm-5.2.zip; done Now run the Docker image again with the following command (in the same directory): voice=dfki-prudence docker run -it -p 59125:59125 -v \"$(pwd)/marytts-5.2/lib/voice-${voice}-hsmm-5.2.jar:/marytts/lib/voice-${voice}-hsmm-5.2.jar\" synesthesiam/marytts:5.2 Change the first line to select the voice you'd like to add. It's not recommended to link in all of the voices at once, since MaryTTS seems to load them all into memory and overwhelm the RAM of a Raspberry Pi. See rhasspy.tts.MaryTTSSentenceSpeaker for details. Audio Effects MaryTTS is capable of applying several audio effects when producing speech. See the web interface at http://localhost:59125 to experiment with this. To use these effects within Rhasspy, set text_to_speech.marytts.effects within your profile, for example: \"text_to_speech\": { \"system\": \"marytts\", \"marytts\": { \"url\": \"http://localhost:59125\", \"effects\": { \"effect_Volume_selected\": \"on\", \"effect_Volume_parameters\": \"amount=0.9;\", \"effect_TractScaler_selected\": \"on\", \"effect_TractScaler_parameters\": \"amount:1.2;\", \"effect_F0Add_selected\": \"on\", \"effect_F0Add_parameters\": \"f0Add:-50.0;\", \"effect_Robot_selected\": \"on\", \"effect_Robot_parameters\": \"amount=50.0;\" } } } You can determine the names of the parameters by examining the web interface http://localhost:59125 using your browser's Developer Tools. Implemented by rhasspy-tts-cli-hermes Google WaveNet Uses Google's WaveNet text to speech system. This requires a Google account and an internet connection to function . Rhasspy will cache WAV files for previously spoken sentences, but you will be sending Google information for every new sentence that Rhasspy speaks. Add to your profile : \"text_to_speech\": { \"system\": \"wavenet\", \"wavenet\": { \"cache_dir\": \"tts/googlewavenet/cache\", \"credentials_json\": \"tts/googlewavenet/credentials.json\", \"gender\": \"FEMALE\", \"language_code\": \"en-US\", \"sample_rate\": 22050, \"url\": \"https://texttospeech.googleapis.com/v1/text:synthesize\", \"voice\": \"Wavenet-C\", \"fallback_tts\": \"espeak\" } } Before using WaveNet, you must set up a Google cloud account and generate a JSON credentials file . Save the JSON credentials file to wherever wavenet.credentials_json points to in your profile directory. You may also need to visit your Google cloud account settings and enable the text-to-speech API. WAV files of each sentence are cached in wavenet.cache_dir in your profile directory. Sentences are cached based on their text and the gender , voice , language_code , and sample_rate of the wavenet system. Changing any of these things will require using the Google API. If there are problems using the Google API (e.g., your internet connection fails), Rhasspy will switch over to the text to speech system given in wavenet.fallback_tts . The settings for the fallback system will be loaded from your profile as expected. Contributed by Romkabouter . TODO: Unimplemented Home Assistant TTS Platform Use a TTS platform on your Home Assistant server. Add to your profile : \"text_to_speech\": { \"system\": \"hass_tts\", \"hass_tts\": { \"platform\": \"...\" } } The settings from your profile's home_assistant section are automatically used (URL, access token, etc.). TODO: Unimplemented Remote Simply POSTs the sentence to be spoken to an HTTP endpoint. Add to your profile : \"text_to_speech\": { \"system\": \"remote\", \"remote\": { \"url\": \"http://tts-server/endpoint\" } } Implemented by rhasspy-remote-http-hermes Command You can extend Rhasspy easily with your own external text to speech system. When a sentence needs to be spoken, Rhasspy will call your custom program with the text given on standard in. Your program should return the corresponding WAV data on standard out. Add to your profile : \"text_to_speech\": { \"system\": \"command\", \"command\": { \"program\": \"/path/to/program\", \"arguments\": [] } } Implemented by rhasspy-tts-cli-hermes and rhasspy-remote-http-hermes Dummy Disables text to speech. Add to your profile : \"text_to_speech\": { \"system\": \"dummy\" } See rhasspy.tts.DummySentenceSpeaker for details.","title":"Text to Speech"},{"location":"text-to-speech/#text-to-speech","text":"After you voice command has been handled , it's common to produce speech as a response back to the user. Rhasspy has support for several text to speech systems which, importantly, can be played through any of the audio output systems. The following table summarizes language support for the various text to speech systems: System en de es fr it nl ru el hi zh vi pt ca espeak \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 flite \u2713 \u2713 picotts \u2713 \u2713 \u2713 \u2713 \u2713 marytts \u2713 \u2713 \u2713 \u2713 \u2713 wavenet \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713","title":"Text to Speech"},{"location":"text-to-speech/#espeak","text":"Uses eSpeak to speak sentences. This is the default text to speech system and, while it sounds robotic, has the widest support for different languages. Add to your profile : \"text_to_speech\": { \"system\": \"espeak\", \"espeak\": { \"voice\": \"en\" } } Remove the voice option to have espeak use your profile's language automatically. You may also pass additional arguments to the espeak command. For example, \"text_to_speech\": { \"system\": \"espeak\", \"espeak\": { \"arguments\": [\"-s\", \"80\"] } } will speak the sentence more slowly. Implemented by rhasspy-tts-cli-hermes","title":"eSpeak"},{"location":"text-to-speech/#flite","text":"Uses FestVox's flite for speech. Sounds better than espeak in most cases, but only supports English out of the box. Add to your profile : \"text_to_speech\": { \"system\": \"flite\", \"flite\": { \"voice\": \"kal16\" } } Some other included voices are rms , slt , and awb . Implemented by rhasspy-tts-cli-hermes","title":"Flite"},{"location":"text-to-speech/#picotts","text":"Uses SVOX's picotts for text to speech. Sounds a bit better (to me) than flite or espeak . Included languages are en-US , en-GB , de-DE , es-ES , fr-FR and it-IT . Add to your profile : \"text_to_speech\": { \"system\": \"picotts\", \"picotts\": { \"language\": \"en-US\" } } Implemented by rhasspy-tts-cli-hermes","title":"PicoTTS"},{"location":"text-to-speech/#marytts","text":"Uses a remote MaryTTS web server. Supported languages include German, British and American English, French, Italian, Luxembourgish, Russian, Swedish, Telugu, and Turkish. An MaryTTS Docker image is available, though only the default voice is included. Add to your profile : \"text_to_speech\": { \"system\": \"marytts\", \"marytts\": { \"url\": \"http://localhost:59125\", \"voice\": \"cmu-slt\", \"locale\": \"en-US\" } } To run the Docker image, simply execute: docker run -it -p 59125:59125 synesthesiam/marytts:5.2 and visit http://localhost:59125 after it starts. If you're using docker compose , add the following to your docker-compose.yml file: marytts: image: synesthesiam/marytts:5.2 restart: unless-stopped ports: - \"59125:59125\" When using docker-compose, set marytts.url in your profile to be http://marytts:59125 . This will allow rhasspy, from within its docker container, to resolve and connect to marytts (its sibling container).","title":"MaryTTS"},{"location":"text-to-speech/#adding-voices","text":"For more English voices, run the following commands in a Bash shell: mkdir -p marytts-5.2/download for voice in dfki-prudence dfki-poppy dfki-obadiah dfki-spike cmu-bdl cmu-rms; do wget -O marytts-5.2/download/voice-${voice}-hsmm-5.2.zip https://github.com/marytts/voice-${voice}-hsmm/releases/download/v5.2/voice-${voice}-hsmm-5.2.zip; unzip -d marytts-5.2 marytts-5.2/download/voice-${voice}-hsmm-5.2.zip; done Now run the Docker image again with the following command (in the same directory): voice=dfki-prudence docker run -it -p 59125:59125 -v \"$(pwd)/marytts-5.2/lib/voice-${voice}-hsmm-5.2.jar:/marytts/lib/voice-${voice}-hsmm-5.2.jar\" synesthesiam/marytts:5.2 Change the first line to select the voice you'd like to add. It's not recommended to link in all of the voices at once, since MaryTTS seems to load them all into memory and overwhelm the RAM of a Raspberry Pi. See rhasspy.tts.MaryTTSSentenceSpeaker for details.","title":"Adding Voices"},{"location":"text-to-speech/#audio-effects","text":"MaryTTS is capable of applying several audio effects when producing speech. See the web interface at http://localhost:59125 to experiment with this. To use these effects within Rhasspy, set text_to_speech.marytts.effects within your profile, for example: \"text_to_speech\": { \"system\": \"marytts\", \"marytts\": { \"url\": \"http://localhost:59125\", \"effects\": { \"effect_Volume_selected\": \"on\", \"effect_Volume_parameters\": \"amount=0.9;\", \"effect_TractScaler_selected\": \"on\", \"effect_TractScaler_parameters\": \"amount:1.2;\", \"effect_F0Add_selected\": \"on\", \"effect_F0Add_parameters\": \"f0Add:-50.0;\", \"effect_Robot_selected\": \"on\", \"effect_Robot_parameters\": \"amount=50.0;\" } } } You can determine the names of the parameters by examining the web interface http://localhost:59125 using your browser's Developer Tools. Implemented by rhasspy-tts-cli-hermes","title":"Audio Effects"},{"location":"text-to-speech/#google-wavenet","text":"Uses Google's WaveNet text to speech system. This requires a Google account and an internet connection to function . Rhasspy will cache WAV files for previously spoken sentences, but you will be sending Google information for every new sentence that Rhasspy speaks. Add to your profile : \"text_to_speech\": { \"system\": \"wavenet\", \"wavenet\": { \"cache_dir\": \"tts/googlewavenet/cache\", \"credentials_json\": \"tts/googlewavenet/credentials.json\", \"gender\": \"FEMALE\", \"language_code\": \"en-US\", \"sample_rate\": 22050, \"url\": \"https://texttospeech.googleapis.com/v1/text:synthesize\", \"voice\": \"Wavenet-C\", \"fallback_tts\": \"espeak\" } } Before using WaveNet, you must set up a Google cloud account and generate a JSON credentials file . Save the JSON credentials file to wherever wavenet.credentials_json points to in your profile directory. You may also need to visit your Google cloud account settings and enable the text-to-speech API. WAV files of each sentence are cached in wavenet.cache_dir in your profile directory. Sentences are cached based on their text and the gender , voice , language_code , and sample_rate of the wavenet system. Changing any of these things will require using the Google API. If there are problems using the Google API (e.g., your internet connection fails), Rhasspy will switch over to the text to speech system given in wavenet.fallback_tts . The settings for the fallback system will be loaded from your profile as expected. Contributed by Romkabouter . TODO: Unimplemented","title":"Google WaveNet"},{"location":"text-to-speech/#home-assistant-tts-platform","text":"Use a TTS platform on your Home Assistant server. Add to your profile : \"text_to_speech\": { \"system\": \"hass_tts\", \"hass_tts\": { \"platform\": \"...\" } } The settings from your profile's home_assistant section are automatically used (URL, access token, etc.). TODO: Unimplemented","title":"Home Assistant TTS Platform"},{"location":"text-to-speech/#remote","text":"Simply POSTs the sentence to be spoken to an HTTP endpoint. Add to your profile : \"text_to_speech\": { \"system\": \"remote\", \"remote\": { \"url\": \"http://tts-server/endpoint\" } } Implemented by rhasspy-remote-http-hermes","title":"Remote"},{"location":"text-to-speech/#command","text":"You can extend Rhasspy easily with your own external text to speech system. When a sentence needs to be spoken, Rhasspy will call your custom program with the text given on standard in. Your program should return the corresponding WAV data on standard out. Add to your profile : \"text_to_speech\": { \"system\": \"command\", \"command\": { \"program\": \"/path/to/program\", \"arguments\": [] } } Implemented by rhasspy-tts-cli-hermes and rhasspy-remote-http-hermes","title":"Command"},{"location":"text-to-speech/#dummy","text":"Disables text to speech. Add to your profile : \"text_to_speech\": { \"system\": \"dummy\" } See rhasspy.tts.DummySentenceSpeaker for details.","title":"Dummy"},{"location":"training/","text":"Training Rhasspy is designed to recognize voice commands in a template language . These commands are categorized by intent , and may contain slots or named entities , such as the color and name of a light. Intent Recognition Basic Syntax Named Entities Number Ranges Slots Slot Synonyms Slot Programs Converters Speech Recognition Custom Words Language Model Mixing sentences.ini Voice commands stored in an ini file whose \"sections\" are intents and \"values\" are sentence templates. Basic Syntax To get started, simply list your intents (surround by brackets) and the possible ways of invoking them below: [TestIntent1] this is a sentence this is another sentence for the same intent [TestIntent2] this is a sentence for a different intent If you say \"this is a sentence\" after hitting the Train button, it will generate a TestIntent1 . Groups You can group multiple words together using (parentheses) like: turn on the (living room lamp) Groups (sometimes called sequences) can be tagged and substituted like single words. They may also contain alternatives . Optional Words Within a sentence template, you can specify optional word(s) by surrounding them [with brackets] . For example: [an] example sentence [with] some optional words will match: an example sentence with some optional words example sentence with some optional words an example sentence some optional words example sentence some optional words Alternatives A set of items where only one is matched at a time is (specified | like | this) . For N items, there will be N matched sentences (unless you nest optional words, etc.). The template: set the light to (red | green | blue) will match: set the light to red set the light to green set the light to blue Tags Named entities are marked in your sentence templates with {tags} . The name of the {entity} is between the curly braces, while the (value of the){entity} comes immediately before: [SetLightColor] set the light to (red | green | blue){color} With the {color} tag attached to (red | green | blue) , Rhasspy will match: set the light to [red](color) set the light to [green](color) set the light to [blue](color) When the SetLightColor intent is recognized, the JSON event will contain a color property whose value is either \"red\", \"green\" or \"blue\". Tag Synonyms Tag/named entity values can be (substituted](#substitutions) using the colon ( : ) inside the {curly:braces} like: turn on the (living room lamp){name:light_1} Now the name property of the intent JSON event will contain \"light_1\" instead of \"living room lamp\". Substitutions The colon ( : ) is used to put something different than what's spoken into the recognized intent JSON. The left-hand side of the : is what Rhasspy expects to hear, while the right-hand side is what gets put into the intent: turn on the (living room lamp):light_1 In this example, the spoken phrase \"living room lamp\" will be replaced by \"light_1\" in the recognized intent. Substitutions work for single words, groups , alternatives , and tags : turn on the living room lamp:light (turn | switch):switch on the living room lamp turn (on){action:activate} the living room lamp See tag synonyms for more details on tag substitution. You can leave the left-hand or right-hand side (or both!) of the : empty: these: words: will: be: dropped: :these :will :be :added When the right-hand side is empty ( dropped: ), the spoken word will not appear in the intent. An empty left-hand side ( :added ) means the word is not spoken, but will appear in the intent. Leaving both sides empty does nothing unless you attach a tag it. This allows you to embed a named entity in a voice command without matching specific words: turn on the living room lamp (:){domain:light} An intent from the example above will contain a domain entity whose value is light . Rules Rules allow you to reuse parts of your sentence templates. They're defined by rule_name = ... alongside other sentences and referenced by <rule_name> . For example: colors = (red | green | blue) set the light to <colors> which is equivalent to: set the light to (red | green | blue) You can share rules across intents by referencing them as <IntentName.rule_name> like: [SetLightColor] colors = (red | green | blue) set the light to <colors> [GetLightColor] is the light <SetLightColor.colors> The second intent ( GetLightColor ) references the colors rule from SetLightColor . Rule references without a dot must exist in the current intent. Number Ranges Rhasspy supports using number literals ( 75 ) and number ranges ( 1..10 ) directly in your sentence templates. During training, the num2words package is used to generate words that the speech recognizer can handle (\"seventy five\"). For example: [SetBrightness] set brightness to (0..100){brightness} The brightness property of the recognized SetBrightness intent will automatically be converted to an integer for you. You can optionally add a step to the integer range: evens = 0..100,2 odds = 1..100,2 Under the hood, number ranges are actually references to the rhasspy/number slot program . You can override this behavior by creating your slot_programs/rhasspy/number program or disable it entirely by setting intent.replace_numbers to false in your profile . Slots Lists Large alternatives can become unwieldy quickly. For example, say you have a list of movie names: movies = (\"Primer\" | \"Moon\" | \"Chronicle\" | \"Timecrimes\" | \"Mulholland Drive\" | ... ) Rather than keep this list in sentences.ini , you may put each movie name on a separate line in a file named slots/movies (no file extension) and reference it as $movies . Rhasspy automatically loads all files in the slots directory of your profile and makes them available as slots lists. For the example above, the file slots/movies should contain: Primer Moon Chronicle Timecrimes Mullholand Drive Now you can simply use the placeholder $movies in your sentence templates: [PlayMovie] play ($movies){movie_name} When matched, the PlayMovie intent JSON will contain movie_name property with either \"Primer\", \"Moon\", etc. Make sure to re-train Rhasspy whenever you update your slot values! Slot Directories Slot files can be put in sub-directories under slots . A list in slots/foo/bar should be referenced in sentences.ini as $foo/bar . Slot Synonyms Slot values are themselves sentence templates! So you can use all of the familiar syntax from above. Slot \"synonyms\" can be created simply using substitutions . So a file named slots/rooms may contain: [the:] (den | playroom | downstairs):den which is referenced by $rooms and will match: the den den the playroom playroom the downstairs downstairs This will always output just \"den\" because [the:] optionally matches \"the\" and then drops the word. Slot Programs Slot lists are great if your slot values always stay the same and are easily written out by hand. If you have slot values that you need to be generated each time Rhasspy is trained , you can use slot programs. Create a directory named slot_programs in your profile (e.g., $HOME/.config/rhasspy/profiles/en/slot_programs ): slot_programs=\"${HOME}/.config/rhasspy/profiles/en/slot_programs\" mkdir -p \"${slot_programs}\" Add a file in slot_programs with the name of your slot, e.g. colors . Write a program in this file, such as a bash script. Make sure to include the shebang and mark the file as executable: cat <<EOF > \"${slot_programs}/colors\" #!/usr/bin/env bash echo 'red' echo 'green' echo 'blue' EOF chmod +x \"${slot_programs}/colors\" Now, when you reference $colors in your sentences.ini , Rhasspy will run the program you wrote and collect the slot values from each line. Note that you can output all the same things as regular slots lists , including optional words, alternatives, etc. You can pass arguments to your program using the syntax $name,arg1,arg2,... in sentences.ini (no spaces). Arguments will be pass on the command-line, so arg1 and arg2 will be $1 and $2 in a bash script. Like regular slots lists, slot programs can also be put in sub-directories under slot_programs . A program in slot_programs/foo/bar should be referenced in sentences.ini as $foo/bar . Built-in Slots Rhasspy includes a few built-in slots for each language: $rhasspy/days - day names of the week $rhasspy/months - month names of the year Converters By default, all named entity values in a recognized intent's JSON are strings. If you need a different data type, such as an integer or float, or want to do some kind of complex conversion , use a converter: [SetBrightness] set brightness to (low:0 | medium:0.5 | high:1){brightness!float} The !name syntax calls a converter by name. Rhasspy includes several built-in converters: int - convert to integer float - convert to real bool - convert to boolean lower - lower-case upper - upper-case You can define your own converters by placing a file in the converters directory of your profile. Like slot programs , this file should contain a shebang and be marked as executable ( chmod +x ). A file named converters/foo/bar should be referenced as !foo/bar in sentences.ini . Your custom converter will receive the value to convert on standard in ( stdin ) encoded as JSON. You should print a converted JSON value to standard out stdout . The example below demonstrates converting a string value into an integer: #!/usr/bin/env python3 import sys import json value = json.load(sys.stdin) print(int(value)) Converters can be chained , so !foo!bar will call the foo converter and then pass the result to bar . Special Cases If one of your sentences happens to start with an optional word (e.g., [the] ), this can lead to a problem: [SomeIntent] [the] problem sentence Python's configparser will interpret [the] as a new section header, which will produce a new intent, grammar, etc. Rhasspy handles this special case by using a backslash escape sequence ( \\[ ): [SomeIntent] \\[the] problem sentence Now [the] will be properly interpreted as a sentence under [SomeIntent] . You only need to escape a [ if it's the very first character in your sentence. Motivation The combination of an ini file and JSGF is arguably an abuse of two file formats, so why do this? At a minimum, Rhasspy needs a set of sentences grouped by intent in order to train the speech and intent recognizers. A fairly pleasant way to express this in text is as follows: [Intent 1] sentence 1 sentence 2 ... [Intent 2] sentence 3 sentence 4 ... Compared to JSON, YAML, etc., there is minimal syntactic overhead for the purposes of just writing down sentences. However, its shortcomings become painfully obvious once you have more than a handful of sentences and intents: If two sentences are nearly identical, save for an optional word like \"the\" or \"a\", you have to maintain two nearly identical copies of a sentence. When speaking about collections of things, like colors or states (on/off), you need a sentence for every alternative choice . You cannot share commonly repeated phrases across sentences or intents. There is no way to tag phrases so the intent recognizer knows the values for an intent's slots (e.g., color). Each of these shortcomings are addressed by considering the space between intent headings ( [Intent 1] , etc.) as a grammar that represent many possible voice commands. The possible sentences, stripped of their tags, are used as input to opengrm to produce a standard ARPA language model for pocketsphinx or Kaldi . The tagged sentences are then used to train an intent recognizer. Custom Words Rhasspy looks for words you've defined outside of your profile's base dictionary (typically base_dictionary.txt ) in a custom words file (typically custom_words.txt ). This is just a CMU phonetic dictionary with words/pronunciations separated by newlines: hello H EH L OW world W ER L D You can use the Words tab in Rhasspy's web interface to generate this dictionary. During training, Rhasspy will merge custom_words.txt into your dictionary.txt file so the speech to text system knows the words in your voice commands are pronounced. Language Model Mixing Rhasspy is designed to only respond to the voice commands you specify in sentences.ini , but both the Pocketsphinx and Kaldi speech to text systems are capable of transcribing open ended speech. While this will never be as good as a cloud-based system, Rhasspy offers it as an option . A middle ground between open transcription and custom voice commands is language model mixing . During training, Rhasspy can mix a (large) pre-built language model with the custom-generated one. You specify a mixture weight (0-1), which controls how much of an influence the large language model has; a mixture weight of 0 makes Rhasspy sensitive only to your voice commands, which is the default. To see the effect of language model mixing, consider a simple sentences.ini file: [ChangeLightState] turn (on){state} the living room lamp This will only allow Rhasspy to understand the voice command \"turn on the living room lamp\". If we train Rhasspy and perform speech to text on a WAV file with this command, the output is no surprise: $ rhasspy-cli --profile en train OK $ rhasspy-cli --profile en wav2text < turn_on_living_room_lamp.wav turn on the living room lamp Now let's do speech to text on a variation of the command, a WAV file with the speech \"would you please turn on the living room lamp\": $ rhasspy-cli --profile en wav2text < would_you_please_turn_on_living_room_lamp.wav on the the the turn on the living room lamp The word salad here is because we're trying to recognize a voice command that was not present in sentences.ini . We could always add it, of course, and that is the preferred method for Rhasspy. There may be cases, however, where we cannot anticipate all of the variations of a voice command. For these cases, you should increase the mix_weight in your profile to something above 0: $ rhasspy-cli --profile en \\ --set 'speech_to_text.pocketsphinx.mix_weight' '0.05' \\ train OK Note that training will take significantly longer because of the size of the base language model. Now, let's test our two WAV files: $ rhasspy-cli --profile en wav2text < turn_on_living_room_lamp.wav turn on the living room lamp $ rhasspy-cli --profile en wav2text < would_you_please_turn_on_living_room_lamp.wav would you please turn on the living room lamp Great! Rhasspy was able to transcribe a sentence that it wasn't explicitly trained on. If you're trying this at home, you surely noticed that it takes a lot longer to process the WAV files too. In practice, it's not recommended to do mixed language modeling on lower-end hardware like a Raspberry Pi. If you need open ended speech recognition, try running Rhasspy in a client/server set up . The Elephant in the Room This isn't the end of the story for open ended speech recognition in Rhasspy, however, because Rhasspy also does intent recognition using the transcribed text as input. When the set of possible voice commands is known ahead of time, it's relatively easy to know what to do with each and every sentence. The flexibility gained from mixing in a base language model unfortunately places a large burden on the intent recognizer. In our ChangeLightState example above, we're fortunate that everything works as expected: $ echo 'would you please turn on the living room lamp' | \\ rhasspy-cli --profile en text2intent { \"would you please turn on the living room lamp\": { \"text\": \"turn on the living room lamp\", \"intent\": { \"name\": \"ChangeLightState\", \"confidence\": 1.0 }, \"entities\": [ { \"entity\": \"state\", \"value\": \"on\" } ], \"slots\": { \"state\": \"on\" } } } But this works only because the default intent recognizer ( fsticuffs ) ignores unknown words by default, so \"would you please\" is not interpreted. Changing \"lamp\" to \"light\" in the input sentence will reveal the problem: $ echo 'would you please turn on the living room light | \\ rhasspy-cli --profile en text2intent { \"would you please turn on the living room light\": { \"text\": \"\", \"intent\": { \"name\": \"\", \"confidence\": 0 }, \"entities\": [], \"slots\": {} } } This sentence would be impossible for the speech to text system to recognize without language model mixing, but it's quite possible now. We can band-aid over the problem a bit by switching to the fuzzywuzzy intent recognizer: $ rhasspy-cli --profile en \\ --set 'speech_to_text.pocketsphinx.mix_weight' '0.05' \\ --set 'intent.system' 'fuzzywuzzy' \\ train OK Now when we interpret the sentence with \"light\" instead of \"lamp\", we still get the expected output: $ echo 'would you please turn on the living room light' | \\ rhasspy-cli --profile en --set 'intent.system' 'fuzzywuzzy' text2intent { \"would you please turn on the living room light\": { \"text\": \"turn on the living room lamp\", \"intent\": { \"name\": \"ChangeLightState\", \"confidence\": 0.86 }, \"entities\": [ { \"entity\": \"state\", \"value\": \"on\" } ], \"slots\": { \"state\": \"on\" } } } This works well for our toy example, but will not scale well when there are thousands of voice commands represented in sentences.ini or if the words used are significantly different than in the training set (\"light\" and \"lamp\" are close enough for fuzzywuzzy ). A machine learning-based intent recognizer, like flair or Rasa , would be a better choice for open ended speech.","title":"Training"},{"location":"training/#training","text":"Rhasspy is designed to recognize voice commands in a template language . These commands are categorized by intent , and may contain slots or named entities , such as the color and name of a light. Intent Recognition Basic Syntax Named Entities Number Ranges Slots Slot Synonyms Slot Programs Converters Speech Recognition Custom Words Language Model Mixing","title":"Training"},{"location":"training/#sentencesini","text":"Voice commands stored in an ini file whose \"sections\" are intents and \"values\" are sentence templates.","title":"sentences.ini"},{"location":"training/#basic-syntax","text":"To get started, simply list your intents (surround by brackets) and the possible ways of invoking them below: [TestIntent1] this is a sentence this is another sentence for the same intent [TestIntent2] this is a sentence for a different intent If you say \"this is a sentence\" after hitting the Train button, it will generate a TestIntent1 .","title":"Basic Syntax"},{"location":"training/#groups","text":"You can group multiple words together using (parentheses) like: turn on the (living room lamp) Groups (sometimes called sequences) can be tagged and substituted like single words. They may also contain alternatives .","title":"Groups"},{"location":"training/#optional-words","text":"Within a sentence template, you can specify optional word(s) by surrounding them [with brackets] . For example: [an] example sentence [with] some optional words will match: an example sentence with some optional words example sentence with some optional words an example sentence some optional words example sentence some optional words","title":"Optional Words"},{"location":"training/#alternatives","text":"A set of items where only one is matched at a time is (specified | like | this) . For N items, there will be N matched sentences (unless you nest optional words, etc.). The template: set the light to (red | green | blue) will match: set the light to red set the light to green set the light to blue","title":"Alternatives"},{"location":"training/#tags","text":"Named entities are marked in your sentence templates with {tags} . The name of the {entity} is between the curly braces, while the (value of the){entity} comes immediately before: [SetLightColor] set the light to (red | green | blue){color} With the {color} tag attached to (red | green | blue) , Rhasspy will match: set the light to [red](color) set the light to [green](color) set the light to [blue](color) When the SetLightColor intent is recognized, the JSON event will contain a color property whose value is either \"red\", \"green\" or \"blue\".","title":"Tags"},{"location":"training/#tag-synonyms","text":"Tag/named entity values can be (substituted](#substitutions) using the colon ( : ) inside the {curly:braces} like: turn on the (living room lamp){name:light_1} Now the name property of the intent JSON event will contain \"light_1\" instead of \"living room lamp\".","title":"Tag Synonyms"},{"location":"training/#substitutions","text":"The colon ( : ) is used to put something different than what's spoken into the recognized intent JSON. The left-hand side of the : is what Rhasspy expects to hear, while the right-hand side is what gets put into the intent: turn on the (living room lamp):light_1 In this example, the spoken phrase \"living room lamp\" will be replaced by \"light_1\" in the recognized intent. Substitutions work for single words, groups , alternatives , and tags : turn on the living room lamp:light (turn | switch):switch on the living room lamp turn (on){action:activate} the living room lamp See tag synonyms for more details on tag substitution. You can leave the left-hand or right-hand side (or both!) of the : empty: these: words: will: be: dropped: :these :will :be :added When the right-hand side is empty ( dropped: ), the spoken word will not appear in the intent. An empty left-hand side ( :added ) means the word is not spoken, but will appear in the intent. Leaving both sides empty does nothing unless you attach a tag it. This allows you to embed a named entity in a voice command without matching specific words: turn on the living room lamp (:){domain:light} An intent from the example above will contain a domain entity whose value is light .","title":"Substitutions"},{"location":"training/#rules","text":"Rules allow you to reuse parts of your sentence templates. They're defined by rule_name = ... alongside other sentences and referenced by <rule_name> . For example: colors = (red | green | blue) set the light to <colors> which is equivalent to: set the light to (red | green | blue) You can share rules across intents by referencing them as <IntentName.rule_name> like: [SetLightColor] colors = (red | green | blue) set the light to <colors> [GetLightColor] is the light <SetLightColor.colors> The second intent ( GetLightColor ) references the colors rule from SetLightColor . Rule references without a dot must exist in the current intent.","title":"Rules"},{"location":"training/#number-ranges","text":"Rhasspy supports using number literals ( 75 ) and number ranges ( 1..10 ) directly in your sentence templates. During training, the num2words package is used to generate words that the speech recognizer can handle (\"seventy five\"). For example: [SetBrightness] set brightness to (0..100){brightness} The brightness property of the recognized SetBrightness intent will automatically be converted to an integer for you. You can optionally add a step to the integer range: evens = 0..100,2 odds = 1..100,2 Under the hood, number ranges are actually references to the rhasspy/number slot program . You can override this behavior by creating your slot_programs/rhasspy/number program or disable it entirely by setting intent.replace_numbers to false in your profile .","title":"Number Ranges"},{"location":"training/#slots-lists","text":"Large alternatives can become unwieldy quickly. For example, say you have a list of movie names: movies = (\"Primer\" | \"Moon\" | \"Chronicle\" | \"Timecrimes\" | \"Mulholland Drive\" | ... ) Rather than keep this list in sentences.ini , you may put each movie name on a separate line in a file named slots/movies (no file extension) and reference it as $movies . Rhasspy automatically loads all files in the slots directory of your profile and makes them available as slots lists. For the example above, the file slots/movies should contain: Primer Moon Chronicle Timecrimes Mullholand Drive Now you can simply use the placeholder $movies in your sentence templates: [PlayMovie] play ($movies){movie_name} When matched, the PlayMovie intent JSON will contain movie_name property with either \"Primer\", \"Moon\", etc. Make sure to re-train Rhasspy whenever you update your slot values!","title":"Slots Lists"},{"location":"training/#slot-directories","text":"Slot files can be put in sub-directories under slots . A list in slots/foo/bar should be referenced in sentences.ini as $foo/bar .","title":"Slot Directories"},{"location":"training/#slot-synonyms","text":"Slot values are themselves sentence templates! So you can use all of the familiar syntax from above. Slot \"synonyms\" can be created simply using substitutions . So a file named slots/rooms may contain: [the:] (den | playroom | downstairs):den which is referenced by $rooms and will match: the den den the playroom playroom the downstairs downstairs This will always output just \"den\" because [the:] optionally matches \"the\" and then drops the word.","title":"Slot Synonyms"},{"location":"training/#slot-programs","text":"Slot lists are great if your slot values always stay the same and are easily written out by hand. If you have slot values that you need to be generated each time Rhasspy is trained , you can use slot programs. Create a directory named slot_programs in your profile (e.g., $HOME/.config/rhasspy/profiles/en/slot_programs ): slot_programs=\"${HOME}/.config/rhasspy/profiles/en/slot_programs\" mkdir -p \"${slot_programs}\" Add a file in slot_programs with the name of your slot, e.g. colors . Write a program in this file, such as a bash script. Make sure to include the shebang and mark the file as executable: cat <<EOF > \"${slot_programs}/colors\" #!/usr/bin/env bash echo 'red' echo 'green' echo 'blue' EOF chmod +x \"${slot_programs}/colors\" Now, when you reference $colors in your sentences.ini , Rhasspy will run the program you wrote and collect the slot values from each line. Note that you can output all the same things as regular slots lists , including optional words, alternatives, etc. You can pass arguments to your program using the syntax $name,arg1,arg2,... in sentences.ini (no spaces). Arguments will be pass on the command-line, so arg1 and arg2 will be $1 and $2 in a bash script. Like regular slots lists, slot programs can also be put in sub-directories under slot_programs . A program in slot_programs/foo/bar should be referenced in sentences.ini as $foo/bar .","title":"Slot Programs"},{"location":"training/#built-in-slots","text":"Rhasspy includes a few built-in slots for each language: $rhasspy/days - day names of the week $rhasspy/months - month names of the year","title":"Built-in Slots"},{"location":"training/#converters","text":"By default, all named entity values in a recognized intent's JSON are strings. If you need a different data type, such as an integer or float, or want to do some kind of complex conversion , use a converter: [SetBrightness] set brightness to (low:0 | medium:0.5 | high:1){brightness!float} The !name syntax calls a converter by name. Rhasspy includes several built-in converters: int - convert to integer float - convert to real bool - convert to boolean lower - lower-case upper - upper-case You can define your own converters by placing a file in the converters directory of your profile. Like slot programs , this file should contain a shebang and be marked as executable ( chmod +x ). A file named converters/foo/bar should be referenced as !foo/bar in sentences.ini . Your custom converter will receive the value to convert on standard in ( stdin ) encoded as JSON. You should print a converted JSON value to standard out stdout . The example below demonstrates converting a string value into an integer: #!/usr/bin/env python3 import sys import json value = json.load(sys.stdin) print(int(value)) Converters can be chained , so !foo!bar will call the foo converter and then pass the result to bar .","title":"Converters"},{"location":"training/#special-cases","text":"If one of your sentences happens to start with an optional word (e.g., [the] ), this can lead to a problem: [SomeIntent] [the] problem sentence Python's configparser will interpret [the] as a new section header, which will produce a new intent, grammar, etc. Rhasspy handles this special case by using a backslash escape sequence ( \\[ ): [SomeIntent] \\[the] problem sentence Now [the] will be properly interpreted as a sentence under [SomeIntent] . You only need to escape a [ if it's the very first character in your sentence.","title":"Special Cases"},{"location":"training/#motivation","text":"The combination of an ini file and JSGF is arguably an abuse of two file formats, so why do this? At a minimum, Rhasspy needs a set of sentences grouped by intent in order to train the speech and intent recognizers. A fairly pleasant way to express this in text is as follows: [Intent 1] sentence 1 sentence 2 ... [Intent 2] sentence 3 sentence 4 ... Compared to JSON, YAML, etc., there is minimal syntactic overhead for the purposes of just writing down sentences. However, its shortcomings become painfully obvious once you have more than a handful of sentences and intents: If two sentences are nearly identical, save for an optional word like \"the\" or \"a\", you have to maintain two nearly identical copies of a sentence. When speaking about collections of things, like colors or states (on/off), you need a sentence for every alternative choice . You cannot share commonly repeated phrases across sentences or intents. There is no way to tag phrases so the intent recognizer knows the values for an intent's slots (e.g., color). Each of these shortcomings are addressed by considering the space between intent headings ( [Intent 1] , etc.) as a grammar that represent many possible voice commands. The possible sentences, stripped of their tags, are used as input to opengrm to produce a standard ARPA language model for pocketsphinx or Kaldi . The tagged sentences are then used to train an intent recognizer.","title":"Motivation"},{"location":"training/#custom-words","text":"Rhasspy looks for words you've defined outside of your profile's base dictionary (typically base_dictionary.txt ) in a custom words file (typically custom_words.txt ). This is just a CMU phonetic dictionary with words/pronunciations separated by newlines: hello H EH L OW world W ER L D You can use the Words tab in Rhasspy's web interface to generate this dictionary. During training, Rhasspy will merge custom_words.txt into your dictionary.txt file so the speech to text system knows the words in your voice commands are pronounced.","title":"Custom Words"},{"location":"training/#language-model-mixing","text":"Rhasspy is designed to only respond to the voice commands you specify in sentences.ini , but both the Pocketsphinx and Kaldi speech to text systems are capable of transcribing open ended speech. While this will never be as good as a cloud-based system, Rhasspy offers it as an option . A middle ground between open transcription and custom voice commands is language model mixing . During training, Rhasspy can mix a (large) pre-built language model with the custom-generated one. You specify a mixture weight (0-1), which controls how much of an influence the large language model has; a mixture weight of 0 makes Rhasspy sensitive only to your voice commands, which is the default. To see the effect of language model mixing, consider a simple sentences.ini file: [ChangeLightState] turn (on){state} the living room lamp This will only allow Rhasspy to understand the voice command \"turn on the living room lamp\". If we train Rhasspy and perform speech to text on a WAV file with this command, the output is no surprise: $ rhasspy-cli --profile en train OK $ rhasspy-cli --profile en wav2text < turn_on_living_room_lamp.wav turn on the living room lamp Now let's do speech to text on a variation of the command, a WAV file with the speech \"would you please turn on the living room lamp\": $ rhasspy-cli --profile en wav2text < would_you_please_turn_on_living_room_lamp.wav on the the the turn on the living room lamp The word salad here is because we're trying to recognize a voice command that was not present in sentences.ini . We could always add it, of course, and that is the preferred method for Rhasspy. There may be cases, however, where we cannot anticipate all of the variations of a voice command. For these cases, you should increase the mix_weight in your profile to something above 0: $ rhasspy-cli --profile en \\ --set 'speech_to_text.pocketsphinx.mix_weight' '0.05' \\ train OK Note that training will take significantly longer because of the size of the base language model. Now, let's test our two WAV files: $ rhasspy-cli --profile en wav2text < turn_on_living_room_lamp.wav turn on the living room lamp $ rhasspy-cli --profile en wav2text < would_you_please_turn_on_living_room_lamp.wav would you please turn on the living room lamp Great! Rhasspy was able to transcribe a sentence that it wasn't explicitly trained on. If you're trying this at home, you surely noticed that it takes a lot longer to process the WAV files too. In practice, it's not recommended to do mixed language modeling on lower-end hardware like a Raspberry Pi. If you need open ended speech recognition, try running Rhasspy in a client/server set up .","title":"Language Model Mixing"},{"location":"training/#the-elephant-in-the-room","text":"This isn't the end of the story for open ended speech recognition in Rhasspy, however, because Rhasspy also does intent recognition using the transcribed text as input. When the set of possible voice commands is known ahead of time, it's relatively easy to know what to do with each and every sentence. The flexibility gained from mixing in a base language model unfortunately places a large burden on the intent recognizer. In our ChangeLightState example above, we're fortunate that everything works as expected: $ echo 'would you please turn on the living room lamp' | \\ rhasspy-cli --profile en text2intent { \"would you please turn on the living room lamp\": { \"text\": \"turn on the living room lamp\", \"intent\": { \"name\": \"ChangeLightState\", \"confidence\": 1.0 }, \"entities\": [ { \"entity\": \"state\", \"value\": \"on\" } ], \"slots\": { \"state\": \"on\" } } } But this works only because the default intent recognizer ( fsticuffs ) ignores unknown words by default, so \"would you please\" is not interpreted. Changing \"lamp\" to \"light\" in the input sentence will reveal the problem: $ echo 'would you please turn on the living room light | \\ rhasspy-cli --profile en text2intent { \"would you please turn on the living room light\": { \"text\": \"\", \"intent\": { \"name\": \"\", \"confidence\": 0 }, \"entities\": [], \"slots\": {} } } This sentence would be impossible for the speech to text system to recognize without language model mixing, but it's quite possible now. We can band-aid over the problem a bit by switching to the fuzzywuzzy intent recognizer: $ rhasspy-cli --profile en \\ --set 'speech_to_text.pocketsphinx.mix_weight' '0.05' \\ --set 'intent.system' 'fuzzywuzzy' \\ train OK Now when we interpret the sentence with \"light\" instead of \"lamp\", we still get the expected output: $ echo 'would you please turn on the living room light' | \\ rhasspy-cli --profile en --set 'intent.system' 'fuzzywuzzy' text2intent { \"would you please turn on the living room light\": { \"text\": \"turn on the living room lamp\", \"intent\": { \"name\": \"ChangeLightState\", \"confidence\": 0.86 }, \"entities\": [ { \"entity\": \"state\", \"value\": \"on\" } ], \"slots\": { \"state\": \"on\" } } } This works well for our toy example, but will not scale well when there are thousands of voice commands represented in sentences.ini or if the words used are significantly different than in the training set (\"light\" and \"lamp\" are close enough for fuzzywuzzy ). A machine learning-based intent recognizer, like flair or Rasa , would be a better choice for open ended speech.","title":"The Elephant in the Room"},{"location":"tutorials/","text":"Tutorials","title":"Tutorials"},{"location":"tutorials/#tutorials","text":"","title":"Tutorials"},{"location":"usage/","text":"Usage You can interact with Rhasspy in more ways than your voice: Web Interface Home Assistant Node-RED with Websockets MQTT and Snips HTTP API Command Line Web Interface A browser-based interface for Rhasspy is available on port 12101 by default ( http://localhost:12101 if running locally). From this interface, you can test voice commands, add new voice commands, re-train, and edit your profile. Top Bar The top bar of the web interface lets you perform some global actions on Rhasspy, regardless of which tab you have selected. Click the Rhasspy logo to reload the page Click the version number to test the HTTP API The green Train button will re-train your profile The red Restart button forces Rhasspy to restart Speech Tab Test voice and text commands. Wake up Rhasspy and have it listen for a voice command Upload a WAV file with a voice command Enter a text command and Rhasspy recognize the intent Speak a sentence using the text to speech system Sentences Tab Add new voice commands to Rhasspy using the template syntax . Edits sentences.ini by default Create additional template files These should be prefixed by the sentences_dir in your profile . For example, intents/more-commands.ini The drop down can be used to switch editing between different template files Slots Tab Edit your slots lists . Slot values will overwrite previous ones Create new slots (files in your slots directory) Delete a slot by deleting all values and saving Words Tab Teach Rhasspy how to pronounce new words. Look up pronunciation(s) for known words (in your profile's base_dictionary.txt file) Have Rhasspy guess how to pronounce a new (unknown) word Add new words to your custom_words.txt file Settings Tab Simplified interface for editing your profile . Make sure to restart Rhasspy after saving changes. Advanced Tab Direct interface for editing your profile . Be careful! Entering invalid settings here can cause Rhasspy to not start. Log Tab Streams Rhasspy's log output over a websocket. TODO Home Assistant Rhasspy communicates with Home Assistant directly over its REST API . Specifically, Rhasspy intents are POST-ed to the events endpoint . If you have a Rhasspy intent named ChangeLightColor with name and color slots like in the RGB light example , then Home Assistant will receive an event of type rhasspy_ChangeLightColor whose event data is: { \"name\": \"bedroom\", \"color\": \"red\" } when you say \"set the bedroom to red\". You should write a custom automation with an event trigger to do something when this event arrives. Catching the example event would look like: automation: trigger: platform: event event_type: rhasspy_ChangeLightColor event_data: color: red action: ... You've now added offline, private voice commands to your Home Assistant. Happy automating! Getting the Spoken Text The Home Assistant event will contain two extra slots besides the ones you specify: _text - spoken voice command text with substitutions _raw_text - literal transcription of voice command Node-RED Rhasspy can interact directly with Node-RED directly through websockets . Simply add a websocket input and set the path to ws://<rhasspy>:12101/api/events/intent where <rhasspy> is the hostname or IP address of your Rhasspy server. Make sure to also set send/receive to \"entire message\". More example flows are available on Github . WebSocket Events Rhasspy supports multiple websocket event endpoints: /api/events/intent Intent recognized or not /api/events/wake Wake word detected /api/events/text Speech transcription WebSocket Intents Whenever a voice command is recognized, Rhasspy emits JSON events over a websocket connection available at ws://YOUR_SERVER:12101/api/events/intent (replace ws:// with wss:// if you're using secure hosting ). You can listen to these events in a Node-RED flow, and easily add offline, private voice commands to your home automation set up! For the ChangLightState intent from the RGB Light Example , Rhasspy will emit a JSON event like this over the websocket: { \"text\": \"set the bedroom light to red\", \"intent\": { \"name\": \"ChangeLightColor\", \"confidence\": 1 }, \"entities\": [ { \"entity\": \"name\", \"value\": \"bedroom\" }, { \"entity\": \"color\", \"value\": \"red\" } ], \"slots\": { \"name\": \"bedroom\", \"color\": \"red\" } } WebSocket Wake When the wake word is detected, or Rhasspy is woken up via the /api/listen-for-command HTTP endpoint, a JSON event is emitted at ws://YOUR_SERVER:12101/api/events/wake ( wss:// if using HTTPS) like: { \"wakewordId\": \"default\", \"siteId\": \"default\" } The wakewordId is set using the model or file name of your wakeword model (e.g., porcupine for porcupine.ppn ). The siteId comes from your mqtt.siteId profile setting. WebSocket Transcriptions Each time a voice command is transcribed, Rhasspy emits a JSON event at ws://YOUR_SERVER:12101/api/events/text ( wss:// if using HTTPS) like: { \"text\": \"text from voice command\", \"wakewordId\": \"default\", \"siteId\": \"default\" } The transcription is contained in the text property. wakewordId is the id of the wakeword that initiated the voice command (or default ). The siteId comes from your mqtt.siteId profile setting. Websocket MQTT Messages You can send and receive MQTT messages over a special websocket endpoint at ws://YOUR_SERVER:12101/api/mqtt ( wss:// if using HTTPS). Subscribing to Topics Send a JSON message with the following form: { \"type\": \"subscribe\", \"topic\": \"the/mqtt/topic\" } Receiving Messages When a message whose topic you have subscribed to is received, you will get a JSON message like: { \"topic\": \"/the/mqtt/topic\", \"payload\": ... } You should only subscribe to messages whose payloads can be serialized in JSON. Publishing Messages To send a message to all of Rhasspy's services, send a JSON message like: { \"type\": \"publish\", \"topic\": \"/the/mqtt/topic\", \"payload\": ... } MQTT and Snips Rhasspy is able to interoperate with Snips.AI services using the Hermes protocol over MQTT . See the description of Rhasspy's services for details. HTTP API Rhasspy features a comprehensive HTTP API available at /api/ , documented with OpenAPI 3 (Swagger). See the HTTP API reference for more details. Secure Hosting with HTTPS If you need to access Rhasspy's web interface/API through HTTPS (formally SSL), you can provide a certificate and key file via command-line parameters or the Hass.io configuration. If you're running Rhasspy via Docker or in a virtual environment, add --ssl <CERT_FILE> <KEY_FILE> to the command-line arguments where <CERT_FILE> is your SSL certificate and <KEY_FILE> is your SSL key file. You can generate a self-signed certificate with the following command: openssl req -x509 -newkey rsa:4096 -nodes -out cert.pem -keyout key.pem -days 365 After answering the series of questions, you should have cert.pem and key.pem in your current directory. Then run Rhasspy with: <RHASSPY COMMAND> --ssl cert.pem key.pem The web interface will now be available at https://localhost:12101 and the web socket events at wss://localhost:12101/api/events/intent In Hass.io, you will need to set the following options via the web interface or in your JSON configuration: ssl : true certfile : cert.pem keyfile : key.pem Command Line A command-line client for the Rhasspy HTTP API is available in the rhasspy-client library. You can install it with: pip install rhasspy-client and then run it: rhasspy-client --help","title":"Usage"},{"location":"usage/#usage","text":"You can interact with Rhasspy in more ways than your voice: Web Interface Home Assistant Node-RED with Websockets MQTT and Snips HTTP API Command Line","title":"Usage"},{"location":"usage/#web-interface","text":"A browser-based interface for Rhasspy is available on port 12101 by default ( http://localhost:12101 if running locally). From this interface, you can test voice commands, add new voice commands, re-train, and edit your profile.","title":"Web Interface"},{"location":"usage/#top-bar","text":"The top bar of the web interface lets you perform some global actions on Rhasspy, regardless of which tab you have selected. Click the Rhasspy logo to reload the page Click the version number to test the HTTP API The green Train button will re-train your profile The red Restart button forces Rhasspy to restart","title":"Top Bar"},{"location":"usage/#speech-tab","text":"Test voice and text commands. Wake up Rhasspy and have it listen for a voice command Upload a WAV file with a voice command Enter a text command and Rhasspy recognize the intent Speak a sentence using the text to speech system","title":"Speech Tab"},{"location":"usage/#sentences-tab","text":"Add new voice commands to Rhasspy using the template syntax . Edits sentences.ini by default Create additional template files These should be prefixed by the sentences_dir in your profile . For example, intents/more-commands.ini The drop down can be used to switch editing between different template files","title":"Sentences Tab"},{"location":"usage/#slots-tab","text":"Edit your slots lists . Slot values will overwrite previous ones Create new slots (files in your slots directory) Delete a slot by deleting all values and saving","title":"Slots Tab"},{"location":"usage/#words-tab","text":"Teach Rhasspy how to pronounce new words. Look up pronunciation(s) for known words (in your profile's base_dictionary.txt file) Have Rhasspy guess how to pronounce a new (unknown) word Add new words to your custom_words.txt file","title":"Words Tab"},{"location":"usage/#settings-tab","text":"Simplified interface for editing your profile . Make sure to restart Rhasspy after saving changes.","title":"Settings Tab"},{"location":"usage/#advanced-tab","text":"Direct interface for editing your profile . Be careful! Entering invalid settings here can cause Rhasspy to not start.","title":"Advanced Tab"},{"location":"usage/#log-tab","text":"Streams Rhasspy's log output over a websocket. TODO","title":"Log Tab"},{"location":"usage/#home-assistant","text":"Rhasspy communicates with Home Assistant directly over its REST API . Specifically, Rhasspy intents are POST-ed to the events endpoint . If you have a Rhasspy intent named ChangeLightColor with name and color slots like in the RGB light example , then Home Assistant will receive an event of type rhasspy_ChangeLightColor whose event data is: { \"name\": \"bedroom\", \"color\": \"red\" } when you say \"set the bedroom to red\". You should write a custom automation with an event trigger to do something when this event arrives. Catching the example event would look like: automation: trigger: platform: event event_type: rhasspy_ChangeLightColor event_data: color: red action: ... You've now added offline, private voice commands to your Home Assistant. Happy automating!","title":"Home Assistant"},{"location":"usage/#getting-the-spoken-text","text":"The Home Assistant event will contain two extra slots besides the ones you specify: _text - spoken voice command text with substitutions _raw_text - literal transcription of voice command","title":"Getting the Spoken Text"},{"location":"usage/#node-red","text":"Rhasspy can interact directly with Node-RED directly through websockets . Simply add a websocket input and set the path to ws://<rhasspy>:12101/api/events/intent where <rhasspy> is the hostname or IP address of your Rhasspy server. Make sure to also set send/receive to \"entire message\". More example flows are available on Github .","title":"Node-RED"},{"location":"usage/#websocket-events","text":"Rhasspy supports multiple websocket event endpoints: /api/events/intent Intent recognized or not /api/events/wake Wake word detected /api/events/text Speech transcription","title":"WebSocket Events"},{"location":"usage/#websocket-intents","text":"Whenever a voice command is recognized, Rhasspy emits JSON events over a websocket connection available at ws://YOUR_SERVER:12101/api/events/intent (replace ws:// with wss:// if you're using secure hosting ). You can listen to these events in a Node-RED flow, and easily add offline, private voice commands to your home automation set up! For the ChangLightState intent from the RGB Light Example , Rhasspy will emit a JSON event like this over the websocket: { \"text\": \"set the bedroom light to red\", \"intent\": { \"name\": \"ChangeLightColor\", \"confidence\": 1 }, \"entities\": [ { \"entity\": \"name\", \"value\": \"bedroom\" }, { \"entity\": \"color\", \"value\": \"red\" } ], \"slots\": { \"name\": \"bedroom\", \"color\": \"red\" } }","title":"WebSocket Intents"},{"location":"usage/#websocket-wake","text":"When the wake word is detected, or Rhasspy is woken up via the /api/listen-for-command HTTP endpoint, a JSON event is emitted at ws://YOUR_SERVER:12101/api/events/wake ( wss:// if using HTTPS) like: { \"wakewordId\": \"default\", \"siteId\": \"default\" } The wakewordId is set using the model or file name of your wakeword model (e.g., porcupine for porcupine.ppn ). The siteId comes from your mqtt.siteId profile setting.","title":"WebSocket Wake"},{"location":"usage/#websocket-transcriptions","text":"Each time a voice command is transcribed, Rhasspy emits a JSON event at ws://YOUR_SERVER:12101/api/events/text ( wss:// if using HTTPS) like: { \"text\": \"text from voice command\", \"wakewordId\": \"default\", \"siteId\": \"default\" } The transcription is contained in the text property. wakewordId is the id of the wakeword that initiated the voice command (or default ). The siteId comes from your mqtt.siteId profile setting.","title":"WebSocket Transcriptions"},{"location":"usage/#websocket-mqtt-messages","text":"You can send and receive MQTT messages over a special websocket endpoint at ws://YOUR_SERVER:12101/api/mqtt ( wss:// if using HTTPS).","title":"Websocket MQTT Messages"},{"location":"usage/#subscribing-to-topics","text":"Send a JSON message with the following form: { \"type\": \"subscribe\", \"topic\": \"the/mqtt/topic\" }","title":"Subscribing to Topics"},{"location":"usage/#receiving-messages","text":"When a message whose topic you have subscribed to is received, you will get a JSON message like: { \"topic\": \"/the/mqtt/topic\", \"payload\": ... } You should only subscribe to messages whose payloads can be serialized in JSON.","title":"Receiving Messages"},{"location":"usage/#publishing-messages","text":"To send a message to all of Rhasspy's services, send a JSON message like: { \"type\": \"publish\", \"topic\": \"/the/mqtt/topic\", \"payload\": ... }","title":"Publishing Messages"},{"location":"usage/#mqtt-and-snips","text":"Rhasspy is able to interoperate with Snips.AI services using the Hermes protocol over MQTT . See the description of Rhasspy's services for details.","title":"MQTT and Snips"},{"location":"usage/#http-api","text":"Rhasspy features a comprehensive HTTP API available at /api/ , documented with OpenAPI 3 (Swagger). See the HTTP API reference for more details.","title":"HTTP API"},{"location":"usage/#secure-hosting-with-https","text":"If you need to access Rhasspy's web interface/API through HTTPS (formally SSL), you can provide a certificate and key file via command-line parameters or the Hass.io configuration. If you're running Rhasspy via Docker or in a virtual environment, add --ssl <CERT_FILE> <KEY_FILE> to the command-line arguments where <CERT_FILE> is your SSL certificate and <KEY_FILE> is your SSL key file. You can generate a self-signed certificate with the following command: openssl req -x509 -newkey rsa:4096 -nodes -out cert.pem -keyout key.pem -days 365 After answering the series of questions, you should have cert.pem and key.pem in your current directory. Then run Rhasspy with: <RHASSPY COMMAND> --ssl cert.pem key.pem The web interface will now be available at https://localhost:12101 and the web socket events at wss://localhost:12101/api/events/intent In Hass.io, you will need to set the following options via the web interface or in your JSON configuration: ssl : true certfile : cert.pem keyfile : key.pem","title":"Secure Hosting with HTTPS"},{"location":"usage/#command-line","text":"A command-line client for the Rhasspy HTTP API is available in the rhasspy-client library. You can install it with: pip install rhasspy-client and then run it: rhasspy-client --help","title":"Command Line"},{"location":"wake-word/","text":"Wake Word The typical workflow for interacting with a voice assistant is to first activate it with a \"wake\" or \"hot\" word, then provide your voice command. Rhasspy supports listening for a wake word with one of several systems. Available wake word systems are: Porcupine Snowboy Pocketsphinx External Command You can also wake Rhasspy up using the HTTP API by POST-ing to /api/listen-for-command . Rhasspy will immediately wake up and start listening for a voice command. The following table summarizes the key characteristics of each wake word system: System Performance Training to Customize Online Sign Up porcupine excellent yes, offline no snowboy good yes, online yes pocketsphinx poor no no precise moderate yes, offline no MQTT/Hermes Rhasspy listens for hermes/hotword/<wakewordId>/detected messages to decide when to wake up. The hermes/hotword/toggleOff and hermes/hotword/toggleOff messages can be used to disable/enable wake word listening (done automatically during voice command recording and audio output). Porcupine Listens for a wake word with porcupine . This system has the best performance out of the box. If you want a custom wake word, however, you will need to re-run their optimizer tool every 30 days. Add to your profile : \"wake\": { \"system\": \"porcupine\", \"porcupine\": { \"library_path\": \"porcupine/libpv_porcupine.so\", \"model_path\": \"porcupine/porcupine_params.pv\", \"keyword_path\": \"porcupine/porcupine.ppn\", \"sensitivity\": 0.5 } }, \"rhasspy\": { \"listen_on_start\": true } There are a lot of keyword files available for download. Use the linux platform if you're on desktop/laptop ( amd64 ) and the raspberrypi platform if you're using a Raspberry Pi ( armhf / aarch64 ). The .ppn files should go in the porcupine directory inside your profile (referenced by keyword_path ). If you want to create a custom wake word, you will need to use the Picovoice Console . NOTE : the generated keyword file is only valid for 30 days, though you can always just re-run the optimizer. Implemented by rhasspy-wake-porcupine-hermes Snowboy Listens for one or more wake words with snowboy . This system has the good performance out of the box, but requires an online service to train. Add to your profile : \"wake\": { \"system\": \"snowboy\", \"hermes\": { \"wakeword_id\": \"default\" }, \"snowboy\": { \"model\": \"snowboy/snowboy.umdl\", \"audio_gain\": 1, \"sensitivity\": \"0.5\", \"apply_frontend\": false } }, \"rhasspy\": { \"listen_on_start\": true } If your hotword model has multiple embedded hotwords (such as jarvis.umdl ), the \"sensitivity\" parameter should contain sensitivities for each embedded hotword separated by commas (e.g., \"0.5,0.5\"). Visit the snowboy website to train your own wake word model (requires linking to a GitHub/Google/Facebook account). This personal model with end with .pmdl , and should go in your profile directory. Then, set wake.snowboy.model to the name of that file. You also have the option of using a pre-train universal model ( .umdl ) from Kitt.AI . Multiple Wake Words You can have snowboy listen for multiple wake words with different models, each with their own settings. You will need to download each model file to the snowboy directory in your profile. For example, to use both the snowboy.umdl and jarvis.umdl models, add this to your profile: \"wake\": { \"system\": \"snowboy\", \"snowboy\": { \"model\": \"snowboy/snowboy.umdl,snowboy/jarvis.umdl\", \"model_settings\": { \"snowboy/snowboy.umdl\": { \"sensitivity\": \"0.5\", \"audio_gain\": 1, \"apply_frontend\": false }, \"snowboy/jarvis.umdl\": { \"sensitivity\": \"0.5,0.5\", \"audio_gain\": 1, \"apply_frontend\": false } } } } Make sure to include all models you want in the model setting (separated by commas). Each model may have different settings in model_settings . If a setting is not present, the default values under snowboy will be used. Implemented by rhasspy-wake-snowboy-hermes Pocketsphinx Listens for a keyphrase using pocketsphinx . This is the most flexible wake system, but has the worst performance in terms of false positives/negatives. Add to your profile : \"wake\": { \"system\": \"pocketsphinx\", \"pocketsphinx\": { \"keyphrase\": \"okay rhasspy\", \"threshold\": 1e-30, \"chunk_size\": 960 } }, \"rhasspy\": { \"listen_on_start\": true } Set wake.pocketsphinx.keyphrase to whatever you like, though 3-4 syllables is recommended. Make sure to train and restart Rhasspy whenever you change the keyphrase. The wake.pocketsphinx.threshold should be in the range 1e-50 to 1e-5. The smaller the number, the less like the keyphrase is to be observed. At least one person has written a script to automatically tune the threshold . Implemented by rhasspy-wake-pocketsphinx-hermes Mycroft Precise Listens for a wake word with Mycroft Precise . It requires training up front, but can be done completely offline! Add to your profile : \"wake\": { \"system\": \"precise\", \"precise\": { \"model\": \"model-name-in-profile.pb\", \"sensitivity\": 0.5, \"trigger_level\": 3, \"chunk_size\": 2048 } }, \"rhasspy\": { \"listen_on_start\": true } Follow the instructions from Mycroft AI to train your own wake word model. When you're finished, place both the .pb and .pb.params files in your profile directory, and set wake.precise.model to the name of the .pb file. TODO: Not implemented Command Calls a custom external program to listen for a wake word, only waking up Rhasspy when the program exits. A wakewordId should be printed to standard out before exiting. You will receive chunks of raw audio on standard in. Add to your profile : \"wake\": { \"system\": \"command\", \"command\": { \"program\": \"/path/to/program\", \"arguments\": [], \"sample_rate\": 16000, \"sample_width\": 2, \"channels\": 1 } } When Rhasspy starts, your program will be called with the given arguments. Raw audio chunks will be written to standard in as Rhasspy receives hermes/audioServer/<siteId>/audioFrame messages. This audio is automatically converted to the format given by wake.command.sample_rate (hertz), wake.command.sample_width (bytes), and wake.command.channels . Once your program detects the wake word, it should print a wakewordId to standard out and exit. Rhasspy will call your program again when it goes back to sleep. If the empty string is printed, Rhasspy will use \"default\" for the wakewordId . The following environment variables are available to your program: $RHASSPY_BASE_DIR - path to the directory where Rhasspy is running from $RHASSPY_PROFILE - name of the current profile (e.g., \"en\") $RHASSPY_PROFILE_DIR - directory of the current profile (where profile.json is) See sleep.sh for an example program. Implemented by rhasspy-remote-http-hermes Dummy Disables wake word functionality. Add to your profile : \"wake\": { \"system\": \"dummy\" }","title":"Wake Word"},{"location":"wake-word/#wake-word","text":"The typical workflow for interacting with a voice assistant is to first activate it with a \"wake\" or \"hot\" word, then provide your voice command. Rhasspy supports listening for a wake word with one of several systems. Available wake word systems are: Porcupine Snowboy Pocketsphinx External Command You can also wake Rhasspy up using the HTTP API by POST-ing to /api/listen-for-command . Rhasspy will immediately wake up and start listening for a voice command. The following table summarizes the key characteristics of each wake word system: System Performance Training to Customize Online Sign Up porcupine excellent yes, offline no snowboy good yes, online yes pocketsphinx poor no no precise moderate yes, offline no","title":"Wake Word"},{"location":"wake-word/#mqtthermes","text":"Rhasspy listens for hermes/hotword/<wakewordId>/detected messages to decide when to wake up. The hermes/hotword/toggleOff and hermes/hotword/toggleOff messages can be used to disable/enable wake word listening (done automatically during voice command recording and audio output).","title":"MQTT/Hermes"},{"location":"wake-word/#porcupine","text":"Listens for a wake word with porcupine . This system has the best performance out of the box. If you want a custom wake word, however, you will need to re-run their optimizer tool every 30 days. Add to your profile : \"wake\": { \"system\": \"porcupine\", \"porcupine\": { \"library_path\": \"porcupine/libpv_porcupine.so\", \"model_path\": \"porcupine/porcupine_params.pv\", \"keyword_path\": \"porcupine/porcupine.ppn\", \"sensitivity\": 0.5 } }, \"rhasspy\": { \"listen_on_start\": true } There are a lot of keyword files available for download. Use the linux platform if you're on desktop/laptop ( amd64 ) and the raspberrypi platform if you're using a Raspberry Pi ( armhf / aarch64 ). The .ppn files should go in the porcupine directory inside your profile (referenced by keyword_path ). If you want to create a custom wake word, you will need to use the Picovoice Console . NOTE : the generated keyword file is only valid for 30 days, though you can always just re-run the optimizer. Implemented by rhasspy-wake-porcupine-hermes","title":"Porcupine"},{"location":"wake-word/#snowboy","text":"Listens for one or more wake words with snowboy . This system has the good performance out of the box, but requires an online service to train. Add to your profile : \"wake\": { \"system\": \"snowboy\", \"hermes\": { \"wakeword_id\": \"default\" }, \"snowboy\": { \"model\": \"snowboy/snowboy.umdl\", \"audio_gain\": 1, \"sensitivity\": \"0.5\", \"apply_frontend\": false } }, \"rhasspy\": { \"listen_on_start\": true } If your hotword model has multiple embedded hotwords (such as jarvis.umdl ), the \"sensitivity\" parameter should contain sensitivities for each embedded hotword separated by commas (e.g., \"0.5,0.5\"). Visit the snowboy website to train your own wake word model (requires linking to a GitHub/Google/Facebook account). This personal model with end with .pmdl , and should go in your profile directory. Then, set wake.snowboy.model to the name of that file. You also have the option of using a pre-train universal model ( .umdl ) from Kitt.AI .","title":"Snowboy"},{"location":"wake-word/#multiple-wake-words","text":"You can have snowboy listen for multiple wake words with different models, each with their own settings. You will need to download each model file to the snowboy directory in your profile. For example, to use both the snowboy.umdl and jarvis.umdl models, add this to your profile: \"wake\": { \"system\": \"snowboy\", \"snowboy\": { \"model\": \"snowboy/snowboy.umdl,snowboy/jarvis.umdl\", \"model_settings\": { \"snowboy/snowboy.umdl\": { \"sensitivity\": \"0.5\", \"audio_gain\": 1, \"apply_frontend\": false }, \"snowboy/jarvis.umdl\": { \"sensitivity\": \"0.5,0.5\", \"audio_gain\": 1, \"apply_frontend\": false } } } } Make sure to include all models you want in the model setting (separated by commas). Each model may have different settings in model_settings . If a setting is not present, the default values under snowboy will be used. Implemented by rhasspy-wake-snowboy-hermes","title":"Multiple Wake Words"},{"location":"wake-word/#pocketsphinx","text":"Listens for a keyphrase using pocketsphinx . This is the most flexible wake system, but has the worst performance in terms of false positives/negatives. Add to your profile : \"wake\": { \"system\": \"pocketsphinx\", \"pocketsphinx\": { \"keyphrase\": \"okay rhasspy\", \"threshold\": 1e-30, \"chunk_size\": 960 } }, \"rhasspy\": { \"listen_on_start\": true } Set wake.pocketsphinx.keyphrase to whatever you like, though 3-4 syllables is recommended. Make sure to train and restart Rhasspy whenever you change the keyphrase. The wake.pocketsphinx.threshold should be in the range 1e-50 to 1e-5. The smaller the number, the less like the keyphrase is to be observed. At least one person has written a script to automatically tune the threshold . Implemented by rhasspy-wake-pocketsphinx-hermes","title":"Pocketsphinx"},{"location":"wake-word/#mycroft-precise","text":"Listens for a wake word with Mycroft Precise . It requires training up front, but can be done completely offline! Add to your profile : \"wake\": { \"system\": \"precise\", \"precise\": { \"model\": \"model-name-in-profile.pb\", \"sensitivity\": 0.5, \"trigger_level\": 3, \"chunk_size\": 2048 } }, \"rhasspy\": { \"listen_on_start\": true } Follow the instructions from Mycroft AI to train your own wake word model. When you're finished, place both the .pb and .pb.params files in your profile directory, and set wake.precise.model to the name of the .pb file. TODO: Not implemented","title":"Mycroft Precise"},{"location":"wake-word/#command","text":"Calls a custom external program to listen for a wake word, only waking up Rhasspy when the program exits. A wakewordId should be printed to standard out before exiting. You will receive chunks of raw audio on standard in. Add to your profile : \"wake\": { \"system\": \"command\", \"command\": { \"program\": \"/path/to/program\", \"arguments\": [], \"sample_rate\": 16000, \"sample_width\": 2, \"channels\": 1 } } When Rhasspy starts, your program will be called with the given arguments. Raw audio chunks will be written to standard in as Rhasspy receives hermes/audioServer/<siteId>/audioFrame messages. This audio is automatically converted to the format given by wake.command.sample_rate (hertz), wake.command.sample_width (bytes), and wake.command.channels . Once your program detects the wake word, it should print a wakewordId to standard out and exit. Rhasspy will call your program again when it goes back to sleep. If the empty string is printed, Rhasspy will use \"default\" for the wakewordId . The following environment variables are available to your program: $RHASSPY_BASE_DIR - path to the directory where Rhasspy is running from $RHASSPY_PROFILE - name of the current profile (e.g., \"en\") $RHASSPY_PROFILE_DIR - directory of the current profile (where profile.json is) See sleep.sh for an example program. Implemented by rhasspy-remote-http-hermes","title":"Command"},{"location":"wake-word/#dummy","text":"Disables wake word functionality. Add to your profile : \"wake\": { \"system\": \"dummy\" }","title":"Dummy"}]}