<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Speech to Text - Rhasspy</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Speech to Text";
    var mkdocs_page_input_path = "speech-to-text.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> Rhasspy</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../installation/">Installation</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../services/">Services</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../tutorials/">Tutorials</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../usage/">Usage</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../profiles/">Profiles</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../training/">Training</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../hardware/">Hardware</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../audio-input/">Audio Input</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../audio-output/">Audio Output</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../wake-word/">Wake Word</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../command-listener/">Command Listener</a>
	    </li>
          
            <li class="toctree-l1 current">
		
    <a class="current" href="./">Speech to Text</a>
    <ul class="subnav">
            
    <li class="toctree-l2"><a href="#speech-to-text">Speech to Text</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#mqtthermes">MQTT/Hermes</a></li>
        
            <li><a class="toctree-l3" href="#pocketsphinx">Pocketsphinx</a></li>
        
            <li><a class="toctree-l3" href="#kaldi">Kaldi</a></li>
        
            <li><a class="toctree-l3" href="#remote-http-server">Remote HTTP Server</a></li>
        
            <li><a class="toctree-l3" href="#home-assistant-stt-platform">Home Assistant STT Platform</a></li>
        
            <li><a class="toctree-l3" href="#command">Command</a></li>
        
            <li><a class="toctree-l3" href="#dummy">Dummy</a></li>
        
        </ul>
    

    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../intent-recognition/">Intent Recognition</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../intent-handling/">Intent Handling</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../text-to-speech/">Text to Speech</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../reference/">Reference</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../development/">Development</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../license/">License</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../about/">About</a>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">Rhasspy</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Speech to Text</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="speech-to-text">Speech to Text</h1>
<p>Rhasspy's primary function is convert voice commands to JSON events. The first step of this process is converting speech to text (transcription).</p>
<p>Available speech to text systems are:</p>
<ul>
<li><a href="./#pocketsphinx">Pocketsphinx</a></li>
<li><a href="./#kaldi">Kaldi</a></li>
<li><a href="./#remote-http-server">Remote HTTP Server</a></li>
<li><a href="./#command">External Command</a></li>
</ul>
<p>The following table summarizes language support for the various speech to text systems:</p>
<table>
<thead>
<tr>
<th>System</th>
<th>en</th>
<th>de</th>
<th>es</th>
<th>fr</th>
<th>it</th>
<th>nl</th>
<th>ru</th>
<th>el</th>
<th>hi</th>
<th>zh</th>
<th>vi</th>
<th>pt</th>
<th>ca</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="./#pocketsphinx">pocketsphinx</a></td>
<td>&#x2713;</td>
<td>&#x2713;</td>
<td>&#x2713;</td>
<td>&#x2713;</td>
<td>&#x2713;</td>
<td>&#x2713;</td>
<td>&#x2713;</td>
<td>&#x2713;</td>
<td>&#x2713;</td>
<td>&#x2713;</td>
<td></td>
<td>&#x2713;</td>
<td>&#x2713;</td>
</tr>
<tr>
<td><a href="./#kaldi">kaldi</a></td>
<td>&#x2713;</td>
<td>&#x2713;</td>
<td></td>
<td>&#x2713;</td>
<td></td>
<td>&#x2713;</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>&#x2713;</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="mqtthermes">MQTT/Hermes</h2>
<p>Rhasspy transcribes audio according to the <a href="https://docs.snips.ai/reference/hermes">Hermes protocol</a>. The following steps are needed to get a transcription:</p>
<ol>
<li>A <code>hermes/asr/startListening</code> message is sent with a unique <code>sessionId</code></li>
<li>One or more <code>hermes/audioServer/&lt;siteId&gt;/audioFrame</code> messages are sent with WAV audio data</li>
<li>If enough silence is detected, a transcription is attempted</li>
<li>A <code>hermes/asr/stopListening</code> message is sent with the same <code>sessionId</code>. If a transcription has been sent, it will be.</li>
</ol>
<p>Either a <code>hermes/asr/textCaptured</code> or a <code>hermes/error/asr</code> message will be sent in response.</p>
<h2 id="pocketsphinx">Pocketsphinx</h2>
<p>Does speech recognition with <a href="https://github.com/cmusphinx/pocketsphinx">CMU's pocketsphinx</a>.
This is done completely offline, on your device. If you experience performance problems (usually on a Raspberry Pi), consider running on a home server as well and have your client Rhasspy use a <a href="./#remote-http-server">remote HTTP connection</a>.</p>
<p>Add to your <a href="../profiles/">profile</a>:</p>
<pre><code class="json">&quot;speech_to_text&quot;: {
  &quot;system&quot;: &quot;pocketsphinx&quot;,
  &quot;pocketsphinx&quot;: {
    &quot;acoustic_model&quot;: &quot;acoustic_model&quot;,
    &quot;base_dictionary&quot;: &quot;base_dictionary.txt&quot;,
    &quot;custom_words&quot;: &quot;custom_words.txt&quot;,
    &quot;dictionary&quot;: &quot;dictionary.txt&quot;,
    &quot;language_model&quot;: &quot;language_model.txt&quot;
  }
}
</code></pre>

<p>The <code>dictionary</code>, <code>language_model</code>, and <code>unknown_words</code> files are written during training by the default <a href="../training/#how-it-works">speech to text training system</a>. The <code>acoustic_model</code> and <code>base_dictionary</code> components for each profile were taken from <a href="https://sourceforge.net/projects/cmusphinx/files/Acoustic%20and%20Language%20Models/">a set of pre-trained models</a>. Anyone can extend Rhasspy to new languages by training a <a href="https://cmusphinx.github.io/wiki/tutorialam">new acoustic model</a>.</p>
<p>When Rhasspy starts, it creates a pocketsphinx decoder with the following attributes:</p>
<ul>
<li><code>hmm</code> - <code>speech_to_text.pocketsphinx.acoustic_model</code> (directory)</li>
<li><code>dict</code> - <code>speech_to_text.pocketsphinx.dictionary</code> (file)</li>
<li><code>lm</code> - <code>speech_to_text.pocketsphinx.language_model</code> (file)</li>
</ul>
<h3 id="open-transcription">Open Transcription</h3>
<p>If you just want to use Rhasspy for general speech to text, you can set <code>speech_to_text.pocketsphinx.open_transcription</code> to <code>true</code> in your profile. This will use the included general language model (much slower) and ignore any custom voice commands you've specified. For English, German, and Dutch, you may want to use <a href="#kaldi">Kaldi</a> instead for better results.</p>
<p>Implemented by <a href="https://github.com/rhasspy/rhasspy-asr-pocketsphinx-hermes">rhasspy-asr-pocketsphinx-hermes</a></p>
<h2 id="kaldi">Kaldi</h2>
<p>Does speech recognition with <a href="https://kaldi-asr.org">Kaldi</a>.
This is done completely offline, on your device. If you experience performance problems (usually on a Raspberry Pi), consider running on a home server as well and have your client Rhasspy use a <a href="./#remote-http-server">remote HTTP connection</a>.</p>
<pre><code class="json">{
  &quot;speech_to_text&quot;: {
    &quot;system&quot;: &quot;kaldi&quot;,
    &quot;kaldi&quot;: {
        &quot;base_dictionary&quot;: &quot;base_dictionary.txt&quot;,
        &quot;compatible&quot;: true,
        &quot;custom_words&quot;: &quot;custom_words.txt&quot;,
        &quot;dictionary&quot;: &quot;dictionary.txt&quot;,
        &quot;graph&quot;: &quot;graph&quot;,
        &quot;kaldi_dir&quot;: &quot;/opt/kaldi&quot;,
        &quot;language_model&quot;: &quot;language_model.txt&quot;,
        &quot;model_dir&quot;: &quot;model&quot;,
        &quot;unknown_words&quot;: &quot;unknown_words.txt&quot;
    }
  }
}
</code></pre>

<p>Rhasspy currently supports <code>nnet3</code> and <code>gmm</code> Kaldi acoustic models.</p>
<p>This requires Kaldi to be installed, which is...challenging. The <a href="https://cloud.docker.com/u/synesthesiam/repository/docker/synesthesiam/rhasspy-server">Docker image of Rhasspy</a> contains a <a href="https://github.com/synesthesiam/kaldi-docker/releases">pre-built copy</a> of Kaldi, which might work for you outside of Docker. Make sure to set <code>kaldi_dir</code> to wherever you installed Kaldi.</p>
<h3 id="open-transcription_1">Open Transcription</h3>
<p>If you just want to use Rhasspy for general speech to text, you can set <code>speech_to_text.kaldi.open_transcription</code> to <code>true</code> in your profile. This will use the included general language model (much slower) and ignore any custom voice commands you've specified.</p>
<p>Implemented by <a href="https://github.com/rhasspy/rhasspy-asr-kaldi-hermes">rhasspy-asr-kaldi-hermes</a></p>
<h2 id="remote-http-server">Remote HTTP Server</h2>
<p>Uses a remote HTTP server to transform speech (WAV) to text.
The <code>/api/speech-to-text</code> endpoint from <a href="../usage/#http-api">Rhasspy's HTTP API</a> does just this, allowing you to use a remote instance of Rhasspy for speech recognition.
This is typically used in a client/server set up, where Rhasspy does speech/intent recognition on a home server with decent CPU/RAM available.</p>
<p>Add to your <a href="../profiles/">profile</a>:</p>
<pre><code class="json">&quot;speech_to_text&quot;: {
  &quot;system&quot;: &quot;remote&quot;,
  &quot;remote&quot;: {
    &quot;url&quot;: &quot;http://my-server:12101/api/speech-to-text&quot;
  }
}
</code></pre>

<p>During speech recognition, 16-bit 16 kHz mono WAV data will be POST-ed to the endpoint with the <code>Content-Type</code> set to <code>audio/wav</code>. A <code>text/plain</code> response with the transcription is expected back.</p>
<p>Implemented by <a href="https://github.com/rhasspy/rhasspy-remote-http-hermes">rhasspy-remote-http-hermes</a></p>
<h2 id="home-assistant-stt-platform">Home Assistant STT Platform</h2>
<p>Use an <a href="https://www.home-assistant.io/integrations/stt">STT platform</a> on your Home Assistant server.
This is the same way <a href="https://github.com/home-assistant/ada">Ada</a> sends speech to Home Assistant.</p>
<p>Add to your <a href="../profiles/">profile</a>:</p>
<pre><code class="json">&quot;speech_to_text&quot;: {
  &quot;system&quot;: &quot;hass_stt&quot;,
  &quot;hass_stt&quot;: {
    &quot;platform&quot;: &quot;...&quot;,
    &quot;sample_rate&quot;: 16000,
    &quot;bit_size&quot;: 16,
    &quot;channels&quot;: 1,
    &quot;language&quot;: &quot;en-US&quot;
  }
}
</code></pre>

<p>The settings from your profile's <code>home_assistant</code> section are automatically used (URL, access token, etc.).</p>
<p>Rhasspy will convert audio to the configured format before streaming it to Home Assistant.
In the future, this will be auto-detected from the STT platform API.</p>
<p>TODO: Not implemented</p>
<h2 id="command">Command</h2>
<p>Calls a custom external program to do speech recognition. WAV audio data is provided to your program's standard in, and a transcription is expected on standard out.</p>
<p>Add to your <a href="../profiles/">profile</a>:</p>
<pre><code class="json">&quot;speech_to_text&quot;: {
  &quot;system&quot;: &quot;command&quot;,
  &quot;command&quot;: {
    &quot;program&quot;: &quot;/path/to/program&quot;,
    &quot;arguments&quot;: []
  }
}
</code></pre>

<p>The following environment variables are available to your program:</p>
<ul>
<li><code>$RHASSPY_BASE_DIR</code> - path to the directory where Rhasspy is running from</li>
<li><code>$RHASSPY_PROFILE</code> - name of the current profile (e.g., "en")</li>
<li><code>$RHASSPY_PROFILE_DIR</code> - directory of the current profile (where <code>profile.json</code> is)</li>
</ul>
<p>See <a href="https://github.com/synesthesiam/rhasspy/blob/master/bin/mock-commands/speech2text.sh">speech2text.sh</a> for an example program.</p>
<p>If you want to also call an external program during training, add to your profile:</p>
<pre><code class="json">&quot;training&quot;: {
  &quot;system&quot;: &quot;auto&quot;,
  &quot;speech_to_text&quot;: {
    &quot;command&quot;: {
      &quot;program&quot;: &quot;/path/to/training/program&quot;,
      &quot;arguments&quot;: []
    }
  }
}
</code></pre>

<p>If <code>training.speech_to_text.command.program</code> is set, Rhasspy will call your program with the intent graph generated by <a href="https://github.com/rhasspy/rhasspy-nlu">rhasspy-nlu</a> provided as JSON on standard input. No response is expected, though a non-zero exit code indicates a training failure.</p>
<p>Implemented by <a href="https://github.com/rhasspy/rhasspy-remote-http-hermes">rhasspy-remote-http-hermes</a></p>
<h2 id="dummy">Dummy</h2>
<p>Disables speech to text decoding.</p>
<p>Add to your <a href="../profiles/">profile</a>:</p>
<pre><code class="json">&quot;speech_to_text&quot;: {
  &quot;system&quot;: &quot;dummy&quot;
}
</code></pre>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../intent-recognition/" class="btn btn-neutral float-right" title="Intent Recognition">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../command-listener/" class="btn btn-neutral" title="Command Listener"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../command-listener/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../intent-recognition/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>

</body>
</html>
